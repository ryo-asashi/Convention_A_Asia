{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Surrogate Modeling with MID in Python\"\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In modern actuarial science, there is an inherent tension between predictive accuracy and model transparency.\n",
        "While ensemble tree-based models like **Gradient Boosting Machines (GBMs)** frequently outperform traditional **Generalized Linear Models (GLMs)**, their \"black-box\" nature presents significant hurdles for model governance, regulatory compliance, and price filing.\n",
        "\n",
        "This notebook demonstrates a solution using **Maximum Interpretation Decomposition (MID)** via the `{midlearn}` library in Python.\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Compatibility Notice\n",
        "This article relies on features introduced in **midlearn (>= 0.1.3)** and the underlying R package **midr (>= 0.5.3)**. Please ensure your library and package are up to date. Some arguments are not available in earlier versions.\n",
        ":::\n",
        "\n",
        "### What is MID?\n",
        "\n",
        "MID is a functional decomposition framework that acts as a high-fidelity surrogate for complex models.\n",
        "It deconstructs a black-box prediction function $f(\\mathbf{X})$ into several interpretable components: intercept $g_\\emptyset$, main effects $g_j(X_j)$, and interaction effects $g_{jk}(X_j, X_k)$.\n",
        "The prediction is represented as the following additive structure:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{X}) = g_\\emptyset + \\sum_{j} g_j(X_{j}) + \\sum_{j < k} g_{jk}(X_{j},\\;X_{k}) + \\dots + g_D(\\mathbf{X})\n",
        "$$\n",
        "\n",
        "To ensure the uniqueness and interpretability of each component, MID imposes the centering constraints:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{E}\\left[g_j(X_j)\\right] &= 0 \\\\\n",
        "\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_j = x_j\\right] &= 0 \\quad (\\forall x_j) \\\\\n",
        "\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_k = x_k\\right] &= 0 \\quad (\\forall x_k)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "By replicating a black-box model with this structured approach, we can quantify the \"uninterpreted\" variance (captured by $g_D(\\mathbf{X})$) and derive a representation that captures the superior predictive power of machine learning without sacrificing actuarial clarity.\n",
        "\n",
        "### Setting Up\n",
        "\n",
        "We begin by setting up the environment and loading the necessary libraries."
      ],
      "id": "1bd21df2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false,
        "warnigs": false
      },
      "source": [
        "#| label: setup\n",
        "# utility\n",
        "from pathlib import Path\n",
        "\n",
        "# data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# predictive modeling\n",
        "import sklearn.linear_model as lm\n",
        "import lightgbm as lgb\n",
        "\n",
        "# import loss function\n",
        "from sklearn.metrics import mean_poisson_deviance, r2_score\n",
        "\n",
        "# surrogate modeling\n",
        "import midlearn as mid\n",
        "\n",
        "# visualization\n",
        "from plotnine import *\n",
        "\n",
        "# load training and testing datasets\n",
        "PATH = Path(\"../data\")\n",
        "train = pd.read_parquet(PATH / \"train.parquet\")\n",
        "test  = pd.read_parquet(PATH / \"test.parquet\")"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: configulations\n",
        "#| include: false\n",
        "# configurations\n",
        "COLD_RUN = False\n",
        "LOAD_MID_LGB = True\n",
        "SAVE_MID_LGB = False\n",
        "\n",
        "if COLD_RUN:\n",
        "  np.random.seed(42)\n",
        "  train = train.sample(5000)\n",
        "  test  = test.sample(5000)"
      ],
      "id": "configulations",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Python workflows, we typically partition the dataset into feature matrix $X$, target vector $y$, and weight vector $w$."
      ],
      "id": "d5994671"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: split_Xy\n",
        "# training set\n",
        "X_train = train.drop(['Frequency', 'Exposure'], axis=1)\n",
        "y_train = train['Frequency']\n",
        "w_train = train['Exposure']\n",
        "# testing set\n",
        "X_test  = test.drop(['Frequency', 'Exposure'], axis=1)\n",
        "y_test  = test['Frequency']\n",
        "w_test  = test['Exposure']"
      ],
      "id": "split_Xy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A key component of our evaluation is the **Weighted Mean Poisson Deviance** defined as follows:\n",
        "\n",
        "$$\n",
        "L(\\mathbf{y}, \\hat{\\mathbf{y}}, \\mathbf{w}) = \\frac{2 \\sum_{i=1}^n w_i \\left( y_i \\log(y_i/\\hat{y}_i) - (y_i - \\hat{y}_i) \\right)}{\\sum_{i=1}^n w_i}\n",
        "$$\n",
        "\n",
        "## The Interpretable Baseline: GLM\n",
        "\n",
        "We first fit a GLM to establish a transparent benchmark.\n",
        "Since GLMs are strictly additive on the link scale, they provide a \"ground truth\" structure.\n",
        "This allows us to verify whether the MID framework can accurately recover the original coefficients and linear effects before moving to more complex black-box models."
      ],
      "id": "e2661f24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fit_glm\n",
        "# define variable encoder for PoissonRegressor\n",
        "def one_hot_encode(X):\n",
        "  cats = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "  nums = X.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
        "  ct = ColumnTransformer(\n",
        "    transformers=[\n",
        "      ('cat', OneHotEncoder(drop='first', sparse_output=False), cats),\n",
        "      ('num', 'passthrough', nums)\n",
        "    ],\n",
        "    verbose_feature_names_out=False\n",
        "  )\n",
        "  ct.set_output(transform=\"pandas\")\n",
        "  return ct.fit_transform(X)\n",
        "\n",
        "# initialize and train a Poisson GLM\n",
        "fit_glm = lm.PoissonRegressor(alpha=0, max_iter=300)\n",
        "fit_glm.fit(one_hot_encode(X_train), y_train, sample_weight=w_train)"
      ],
      "id": "fit_glm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: pred_fit_glm\n",
        "# evaluate fitted model\n",
        "y_pred_fit_glm = fit_glm.predict(one_hot_encode(X_test))\n",
        "\n",
        "deviance = mean_poisson_deviance(\n",
        "  y_true=y_test,\n",
        "  y_pred=y_pred_fit_glm,\n",
        "  sample_weight=w_test\n",
        ")\n",
        "print(\"Mean Poisson Deviance:\", round(deviance, 6))"
      ],
      "id": "pred_fit_glm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Surrogate Modeling\n",
        "\n",
        "We apply the `interpret()` function to the GLM.\n",
        "This step serves as a sanity check: if MID is effective, it should perfectly replicate the predictive behavior of the original GLM."
      ],
      "id": "4ae3c807"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: mid_glm\n",
        "# build a surrogate model\n",
        "mid_glm = mid.MIDExplainer(\n",
        "  estimator=fit_glm,\n",
        "  link=\"log\"\n",
        ")\n",
        "\n",
        "mid_glm.fit(\n",
        "  X_train,\n",
        "  y=fit_glm.predict(one_hot_encode(X_train))\n",
        ")"
      ],
      "id": "mid_glm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: pred_mid_glm\n",
        "# evaluate fitted surrogate\n",
        "print(\n",
        "  \"Uninterpreted Variation Ratio:\",\n",
        "  round(mid_glm.ratio['working'], 6)\n",
        ")\n",
        "\n",
        "y_pred_mid_glm = mid_glm.predict(X_test)\n",
        "\n",
        "deviance = mean_poisson_deviance(\n",
        "  y_true=y_test,\n",
        "  y_pred=y_pred_mid_glm,\n",
        "  sample_weight=w_test\n",
        ")\n",
        "print(\"Mean Poisson Deviance:\", round(deviance, 6))"
      ],
      "id": "pred_mid_glm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Fidelity\n",
        "\n",
        "To assess model **fidelity**, i.e., how closely the surrogate replicates the black-box, we calculate the **uninterpreted variation ratio** $\\mathbf{U}$.\n",
        "\n",
        "$$\n",
        "\\mathbf{U}(f,g) = \\frac{\\sum_{i=1}^n (f(x_i) - g(x_i))^2}{\\sum_{i=1}^n (f(x_i) - \\bar{f})^2}, \\quad \\text{where } \\bar{f} = \\frac{1}{n}\\sum_{i=1}^n f(x_i)\n",
        "$$\n",
        "\n",
        "This metric represents the proportion of the black-box model's variance that is not captured by the additive components of the MID model.\n",
        "The **R-squared score**, $\\mathbf{R}^2(f,g) = 1 - \\mathbf{U}(f,g)$, is a standard measure for this purpose. It is important to note that this $\\mathbf{R}^2$ measures the fidelity to the black-box model, not the predictive accuracy relative to the ground truth observations.\n",
        "\n",
        "In the `{midr}` package, the summary output includes this ratio calculated on the training set.\n",
        "For models with non-linear links (e.g., Poisson regression), the \"working\" ratio is computed on the scale of the link function (e.g., $\\log$ scale).\n",
        "\n",
        "To rigorously confirm the model fidelity, it is recommended to evaluate these metrics on a separate testing set.\n",
        "This ensures that the surrogate model is not just over-fitting the training predictions but has truly captured the underlying functional structure."
      ],
      "id": "a6d535ff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: r2_mid_glm\n",
        "# calculate R-squared on testing dataset\n",
        "r2_mid = r2_score(\n",
        "  y_true=np.log(fit_glm.predict(one_hot_encode(X_test))),\n",
        "  y_pred=np.log(mid_glm.predict(X_test)),\n",
        "  sample_weight=w_test\n",
        ")\n",
        "print(\"R squared:\", round(r2_mid, 10))"
      ],
      "id": "r2_mid_glm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown by the high $\\mathbf{R}^2$ score, the MID surrogate achieves near-perfect fidelity.\n",
        "This level of agreement justifies using the MID components (main effects and interactions) as a reliable lens through which to interpret the original black-box model’s behavior.\n",
        "\n",
        "### Feature Effects\n",
        "\n",
        "While the coefficients of a GLM are directly interpretable, visualizing their functional behavior across the feature space provides a more intuitive grasp of the model’s structure.\n",
        "In the Python ecosystem, standard libraries for GLMs often lack built-in \"term plots\" (common in R) to visualize partial effects on the link scale.\n",
        "\n",
        "Here, the MID surrogate serves as a visualization tool: by appropriately replicating the `PoissonRegressor`, we can directly plot the main effects $g_j(X_j)$ to verify that the linear relationships on the log scale are correctly captured."
      ],
      "id": "0aade71c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: effects_mid_glm\n",
        "# main effects of MID surrogate\n",
        "plots = []\n",
        "for feature in X_train.columns:\n",
        "  p = (\n",
        "  mid_glm.plot(feature) +\n",
        "    lims(y=[-0.8, 0.8]) +\n",
        "    labs(y=\"Main Effect\")\n",
        "  )\n",
        "  plots.append(p)\n",
        "\n",
        "display(\n",
        "  (plots[0] | plots[1] | plots[2] | plots[5]) /\n",
        "  (plots[3] | plots[4] | plots[6])\n",
        ")"
      ],
      "id": "effects_mid_glm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Effect Importance\n",
        "\n",
        "Beyond simple plots for feature effects, `{midr}` provides a suite of diagnostic tools.\n",
        "First, the **Effect Importance** of a term $j$ is defined as the mean absolute contribution of that term across the population:\n",
        "\n",
        "$$\n",
        "\\text{Importance}_j = \\mathbf{E} \\left[ | g_j(X_j) | \\right] \\approx \\frac{1}{n} \\sum_{i=1}^n | g_j(x_{ij}) |\n",
        "$$"
      ],
      "id": "5515aa58"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: imp_glm\n",
        "imp_glm = mid_glm.importance(max_nsamples=2000)\n",
        "\n",
        "p1 = (\n",
        "  imp_glm.plot(fill=\"steelblue\") +\n",
        "  labs(title=\"Mean Absolute Effect\", subtitle=\"Bar Plot\")\n",
        ")\n",
        "p2 = (\n",
        "  imp_glm.plot(style=\"sinaplot\", theme=\"mako@div\") +\n",
        "  labs(subtitle=\"Distribution of Effects\") +\n",
        "  theme(legend_position=\"none\", axis_text_y=element_blank())\n",
        ")\n",
        "display(p1 | p2)"
      ],
      "id": "imp_glm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For interaction terms, the importance is similarly calculated using $g_{jk}(X_j, X_k)$.\n",
        "This metric allows us to rank features by their average influence on the model's predictions.\n",
        "\n",
        "### Conditional Expectation\n",
        "\n",
        "Second, we can explore **Individual Conditional Expectations (ICE)**.\n",
        "In the MID framework, the ICE for a feature $j$ and a specific observation $i$ is the expected value of the prediction as $X_j$ varies, while keeping other features fixed at their observed values $\\mathbf{x}_{\\setminus j}^{(i)}$:\n",
        "\n",
        "$$\n",
        "\\text{ICE}_j^{(i)}(x) = g_\\emptyset + g_j(x) + \\sum_{k \\neq j} g_{jk}(x, x_{ik})\n",
        "$$"
      ],
      "id": "fd24c787"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: ice_glm\n",
        "ice_glm_link = mid_glm.conditional(\n",
        "  type=\"link\", variable=\"DrivAge\", data=X_train.sample(200)\n",
        ")\n",
        "ice_glm = mid_glm.conditional(\n",
        "  variable=\"DrivAge\", data=X_train.sample(200)\n",
        ")\n",
        "\n",
        "p1 = (\n",
        "  ice_glm_link.plot(theme=\"bluescale\", var_color=\"LogDensity\") +\n",
        "  theme(legend_position=\"bottom\") +\n",
        "  labs(y=\"Linear Predictor\",\n",
        "       title=\"Conditional Expectation\",\n",
        "       subtitle=\"Change in linear predictor\")\n",
        "  )\n",
        "p2 = (\n",
        "  ice_glm.plot(style=\"centered\", theme=\"bluescale\",\n",
        "               var_color=\"LogDensity\") +\n",
        "  theme(legend_position = \"bottom\") +\n",
        "  labs(y=\"Prediction\", title=\"\",\n",
        "       subtitle=\"Change in original scale\")\n",
        "  )\n",
        "display(p1 | p2)"
      ],
      "id": "ice_glm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unlike standard black-box models, MID's low-order structure allows us to compute these expectations efficiently and interpret the variation across curves (the \"thickness\" of the ICE plot) as a direct consequence of specified interaction terms $g_{jk}$.\n",
        "\n",
        "### Additive Attribution\n",
        "\n",
        "Third, we perform instance-level explanation through an Additive Breakdown of the prediction.\n",
        "For any single observation $\\mathbf{x}$, the MID surrogate's prediction $g(\\mathbf{x})$ is decomposed into the exact sum of its functional components:\n",
        "\n",
        "$$\n",
        "g(\\mathbf{x}) = \\underbrace{g_\\emptyset}_{\\text{Intercept}} + \\underbrace{\\sum_{j} g_j(x_j)}_{\\text{Main Effects}} + \\underbrace{\\sum_{j < k} g_{jk}(x_j, x_k)}_{\\text{Interactions}}\n",
        "$$"
      ],
      "id": "da75d1a7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: bd_glm\n",
        "np.random.seed(42)\n",
        "row_ids = sorted(np.random.randint(0, train.shape[0], 4).tolist())\n",
        "\n",
        "bd_plots = []\n",
        "for idx in row_ids:\n",
        "  bd = mid_glm.breakdown(row=idx)\n",
        "  label = \"Breakdown for Row \" + str(idx)\n",
        "  p = (\n",
        "    bd.plot(theme=\"shap\", format_args={'digits': 2}) +\n",
        "    labs(x=\"\", subtitle=label) +\n",
        "    theme(legend_position=\"none\")\n",
        "  )\n",
        "  bd_plots.append(p)\n",
        "\n",
        "display(\n",
        "  (bd_plots[0] | bd_plots[1]) /\n",
        "  (bd_plots[2] | bd_plots[3]) \n",
        ")"
      ],
      "id": "bd_glm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By visualizing these contributions in a waterfall plot, we can identify which specific risk factors or interaction effects drove the prediction for a particular instance, such as a high-risk policyholder.\n",
        "\n",
        "## The Black-Box: LightGBM\n",
        "\n",
        "While GLMs are transparent, GBMs such as **LightGBM** often yield superior predictive power by capturing high-order interactions.\n",
        "However, this accuracy comes at the cost of being a black box."
      ],
      "id": "b5e8429d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: fit_lgb\n",
        "# data preprocessing for LightGBM\n",
        "def str_to_cat(X):\n",
        "  cats = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "  X = X.copy()\n",
        "  X[cats] = X[cats].astype('category')\n",
        "  return X\n",
        "\n",
        "# model parameters (mirroring the R version)\n",
        "params_lgb = {\n",
        "  'objective': \"poisson\",\n",
        "  'n_estimators': 551,\n",
        "  'learning_rate': 0.01672663973358928,\n",
        "  'num_leaves': 61,\n",
        "  'max_depth': 19,\n",
        "  'min_child_samples': 10,\n",
        "  'subsample': 0.8123000876841823,\n",
        "  'colsample_bytree': 0.6507848978461632,\n",
        "  'reg_alpha': 4.234603347091384,\n",
        "  'reg_lambda': 8.790496879009705e-07,\n",
        "  'random_state': 42,\n",
        "  'n_jobs': -1,\n",
        "  'verbosity': -1,\n",
        "  'importance_type': 'gain'\n",
        "}\n",
        "\n",
        "# split datasets for validation\n",
        "valid_n = int(X_train.shape[0] * 0.2)\n",
        "train_X, valid_X = X_train.iloc[valid_n:], X_train.iloc[:valid_n]\n",
        "train_y, valid_y = y_train[valid_n:], y_train[:valid_n]\n",
        "train_w, valid_w = w_train[valid_n:], w_train[:valid_n]\n",
        "\n",
        "# initialize and train a LightGBM\n",
        "fit_lgb = lgb.LGBMRegressor(**params_lgb)\n",
        "\n",
        "fit_lgb.fit(\n",
        "  X=str_to_cat(train_X),\n",
        "  y=train_y,\n",
        "  sample_weight=train_w,\n",
        "  eval_set=[(valid_X, valid_y)],\n",
        "  eval_sample_weight=[valid_w],\n",
        "  callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        ")"
      ],
      "id": "fit_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: pred_fit_lgb\n",
        "# evaluate fitted model\n",
        "y_pred_fit_lgb = fit_lgb.predict(str_to_cat(X_test))\n",
        "\n",
        "deviance = mean_poisson_deviance(\n",
        "  y_true=y_test,\n",
        "  y_pred=y_pred_fit_lgb,\n",
        "  sample_weight=w_test\n",
        ")\n",
        "print(\"Mean Poisson Deviance:\", round(deviance, 6))"
      ],
      "id": "pred_fit_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Surrogate Modeling\n",
        "\n",
        "We use `{midr}` to replicate the LightGBM model.\n",
        "By including interaction terms in the model formula, we allow the surrogate to capture the joint relationships that the GBM has learned.\n",
        "The goal is to approximate the LightGBM function $f_{LGB}(\\mathbf{x})$ with our interpretable structure $g(\\mathbf{x})$:\n",
        "\n",
        "$$\n",
        "f_{LGB}(\\mathbf{x}) \\approx g(\\mathbf{x}) = g_\\emptyset + \\sum_{j} g_j(x_j) + \\sum_{j < k} g_{jk}(x_j, x_k)\n",
        "$$\n",
        "\n",
        "::: {.callout-warning}\n",
        "### Computational Considerations\n",
        "Including all second-order interactions using the `(...)^2` syntax results in $p(p-1)/2$ interaction terms.\n",
        "For high-dimensional data, this can be memory-intensive.\n",
        "Users should ensure sufficient RAM is available or consider limiting the formula to the most relevant features, or using a subset of the training set.\n",
        ":::"
      ],
      "id": "22be8afd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: mid_lgb\n",
        "#| eval: false\n",
        "# build a surrogate model\n",
        "mid_lgb = mid.MIDExplainer(\n",
        "  estimator=fit_lgb,\n",
        "  interactions=True,\n",
        "  link=\"log\",\n",
        "  penalty=0.01\n",
        ")\n",
        "\n",
        "mid_lgb.fit(\n",
        "  X_train, \n",
        "  y=fit_lgb.predict(X_train),\n",
        "  sample_weight=w_train\n",
        ")"
      ],
      "id": "mid_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: load_mid_lgb\n",
        "#| echo: false\n",
        "import os\n",
        "import joblib\n",
        "if LOAD_MID_LGB and os.path.exists(\"../data/mid_lgb.pkl\"):\n",
        "  mid_lgb = joblib.load(\"../data/mid_lgb.pkl\")\n",
        "else:\n",
        "  mid_lgb = mid.MIDExplainer(\n",
        "    estimator=fit_lgb,\n",
        "    interactions=True,\n",
        "    link=\"log\",\n",
        "    penalty=0.01\n",
        "  )\n",
        "  mid_lgb.fit(\n",
        "    X_train, \n",
        "    y=fit_lgb.predict(X_train),\n",
        "    sample_weight=w_train\n",
        "  )\n",
        "  if SAVE_MID_LGB:\n",
        "    joblib.dump(mid_lgb, \"../data/mid_lgb.pkl\")\n",
        "mid_lgb"
      ],
      "id": "load_mid_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: pred_mid_lgb\n",
        "# evaluate fitted surrogate\n",
        "print(\n",
        "  \"Uninterpreted Variation Ratio:\",\n",
        "  round(mid_lgb.ratio['working'], 6)\n",
        ")\n",
        "\n",
        "y_pred_mid_lgb = mid_lgb.predict(X_test)\n",
        "\n",
        "deviance = mean_poisson_deviance(\n",
        "  y_true=y_test,\n",
        "  y_pred=y_pred_mid_lgb,\n",
        "  sample_weight=w_test\n",
        ")\n",
        "print(\"Mean Poisson Deviance:\", round(deviance, 6))"
      ],
      "id": "pred_mid_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Fidelity\n",
        "\n",
        "To measure how successfully our surrogate replicates the LightGBM model, we use the R-squared score on the link scale ($\\log$ scale). "
      ],
      "id": "cbf03434"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: r2_mid_lgb\n",
        "# calculate R-squared on testing dataset\n",
        "r2_mid = r2_score(\n",
        "  y_true=np.log(fit_lgb.predict(str_to_cat(X_test))),\n",
        "  y_pred=np.log(mid_lgb.predict(X_test)),\n",
        "  sample_weight=w_test\n",
        ")\n",
        "print(\"R squared:\", round(r2_mid, 10))"
      ],
      "id": "r2_mid_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Effects"
      ],
      "id": "7af8579a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: effects_mid_lgb\n",
        "#| warning: false\n",
        "# main effects of MID surrogate\n",
        "plots = []\n",
        "for feature in X_train.columns:\n",
        "  p = (\n",
        "  mid_lgb.plot(feature) +\n",
        "    lims(y=[-1.0, 1.0]) +\n",
        "    labs(y=\"Main Effect\")\n",
        "  )\n",
        "  plots.append(p)\n",
        "\n",
        "display(\n",
        "  (plots[0] | plots[1] | plots[2] | plots[5]) /\n",
        "  (plots[3] | plots[4] | plots[6])\n",
        ")"
      ],
      "id": "effects_mid_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A key advantage of `{midr}` is its ability to isolate interaction effects $g_{jk}$ from main effects $g_j$.\n",
        "This is particularly useful to understand the joint impact of two variables (e.g., `Region` and `LogDensity`)."
      ],
      "id": "aa7b73ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: interaction\n",
        "p1 = (\n",
        "  mid_lgb.plot(\"LogDensity:Region\", style=\"data\",\n",
        "               data=train.sample(2000), theme=\"bluescale\") +\n",
        "  labs(subtitle=\"Interaction Effect\", y=\"\") +\n",
        "  theme(legend_position=\"bottom\")\n",
        ")\n",
        "p2 = (\n",
        "  mid_lgb.plot(\"LogDensity:Region\", main_effects=True) +\n",
        "  labs(subtitle=\"Total Effect\", y=\"\") +\n",
        "  theme(axis_text_y=element_blank(),\n",
        "        legend_position=\"bottom\")\n",
        ")\n",
        "display(p1 | p2)"
      ],
      "id": "interaction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Effect Importance\n",
        "\n",
        "To rank the influence of each component discovered in the LightGBM model, we calculate the Effect Importance, defined as the average absolute contribution."
      ],
      "id": "461e2420"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: imp_lgb\n",
        "imp_lgb = mid_lgb.importance(max_nsamples=2000)\n",
        "\n",
        "p1 = (\n",
        "  imp_lgb.plot(theme=\"bluescale@qual\", max_nterms=20) +\n",
        "  labs(title=\"Effect Importance\", subtitle=\"Average absolute effect\") +\n",
        "  theme(legend_position=\"none\")\n",
        ")\n",
        "p2 = (\n",
        "  imp_lgb.plot(style=\"sinaplot\", theme=\"mako@div\", max_nterms=20) +\n",
        "  labs(title=\"\", subtitle=\"Distribution of effects\") +\n",
        "  theme(axis_text_y=element_blank(), legend_position=\"none\")\n",
        ")\n",
        "display(p1 | p2)"
      ],
      "id": "imp_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conditional Expectation\n",
        "\n",
        "We further explore the model's behavior using the ICE plot.\n",
        "In the MID framework, the variation in ICE curves for a feature $j$ is explicitly governed by the interaction terms $g_{jk}$ identified from the LightGBM model."
      ],
      "id": "d24db78d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: ice_lgb\n",
        "ice_lgb_link = mid_lgb.conditional(\n",
        "  type=\"link\", variable=\"DrivAge\", data=X_train.sample(200)\n",
        ")\n",
        "ice_lgb = mid_lgb.conditional(\n",
        "  variable=\"DrivAge\", data=X_train.sample(200)\n",
        ")\n",
        "\n",
        "p1 = (\n",
        "  ice_lgb_link.plot(theme=\"bluescale\", var_color=\"LogDensity\") +\n",
        "  theme(legend_position=\"bottom\") +\n",
        "  labs(y=\"Linear Predictor\",\n",
        "       title=\"Conditional Expectation\",\n",
        "       subtitle=\"Change in linear predictor\")\n",
        "  )\n",
        "p2 = (\n",
        "  ice_lgb.plot(style=\"centered\", theme=\"bluescale\",\n",
        "               var_color=\"LogDensity\") +\n",
        "  theme(legend_position = \"bottom\") +\n",
        "  labs(y = \"Prediction\", title = \"\",\n",
        "     subtitle = \"Change in original scale\")\n",
        "  )\n",
        "display(p1 | p2)"
      ],
      "id": "ice_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additive Attribution\n",
        "\n",
        "Finally, we perform an Additive Breakdown for individual predictions.\n",
        "This provides an exact allocation of the LightGBM's prediction into the terms of our surrogate model."
      ],
      "id": "e9687863"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| label: bd_lgb\n",
        "np.random.seed(42)\n",
        "row_ids = sorted(\n",
        "  np.random.randint(0, train.shape[0], 4).tolist()\n",
        ")\n",
        "bd_plots = []\n",
        "for idx in row_ids:\n",
        "  bd = mid_lgb.breakdown(row=idx)\n",
        "  label = \"Breakdown for Row \" + str(idx)\n",
        "  p = (\n",
        "    bd.plot(theme=\"shap\", format_args={'digits': 4},\n",
        "            max_nterms=10) +\n",
        "    labs(x=\"\", subtitle=label) +\n",
        "    theme(legend_position=\"none\")\n",
        "  )\n",
        "  bd_plots.append(p)\n",
        "\n",
        "display(\n",
        "  (bd_plots[0] | bd_plots[1]) /\n",
        "  (bd_plots[2] | bd_plots[3]) \n",
        ")"
      ],
      "id": "bd_lgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we have demonstrated how **Maximum Interpretation Decomposition (MID)** bridges the gap between predictive performance and model transparency.\n",
        "By using the `{midlearn}` library, we successfully transformed a complex LightGBM model into a structured, additive representation.\n",
        "\n",
        "While the surrogate model fidelity may not always be perfect, the crucial advantage lies in our ability to quantify its limitations. \n",
        "Through the **uninterpreted variation ratio**, we can directly assess the complexity of the black-box model.\n",
        "If the fidelity is lower than expected, it serves as a diagnostic signal that the original model relies on high-order interactions or structural complexities that extend beyond second-order effects.\n",
        "\n",
        "Knowing the extent of this \"unexplained\" variance is far more valuable than operating in the dark.\n",
        "It allows actuaries to make informed decisions about whether the additional complexity of a black-box model is justified by its performance, or if a more transparent structure is preferable for regulatory and risk management purposes.\n",
        "\n",
        "As machine learning models become increasingly prevalent in insurance pricing and reserving, tools like MID will be essential for ensuring that our \"black-boxes\" remain accountable, reliable, and fundamentally understood."
      ],
      "id": "3ddac5f0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\daysb\\anaconda3\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}