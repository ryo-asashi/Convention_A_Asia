[
  {
    "objectID": "notebooks/demo_r.html",
    "href": "notebooks/demo_r.html",
    "title": "Surrogate Modeling with MID in R",
    "section": "",
    "text": "In modern actuarial science, there is often a tension between predictive accuracy and model transparency. While ensemble tree-based models like Gradient Boosting Machines (GBMs) frequently outperform traditional Generalized Linear Models (GLMs), their black-box nature presents significant hurdles for model governance, regulatory filing, and price filing.\nThis notebook demonstrates a solution using Maximum Interpretation Decomposition (MID) via the {midr} and {midnight} packages in R. MID is a functional decomposition framework that acts as a high-fidelity surrogate for complex models. It decomposes black-box predictions into main effects, the isolated impact of each individual feature on the response, and interaction effects, the joint impact of feature pairs.\nBy replicating a black-box model with this structured additive approach, we can quantify the “unexplained” variance and derive an interpretable model that captures the superior predictive power of machine learning without sacrificing clarity.\nWe begin by setting up the environment.\n\n\nCode\n# data manipulation\nlibrary(arrow)\nlibrary(dplyr)\n\n# predictive modeling\nlibrary(gam)\nlibrary(lightgbm)\n\n# surrogate modeling\nlibrary(midr)\nlibrary(midnight)\n\n# visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# load training and testin datasets\ntrain &lt;- read_parquet(\"../data/train.parquet\")\ntest  &lt;- read_parquet(\"../data/test.parquet\")\n\n\nA key component of our evaluation is the Weighted Poisson Deviance defined as follows.\n\\[\nL(\\mathbf{y}, \\hat{\\mathbf{y}}, \\mathbf{w}) = \\frac{2 \\sum_{i=1}^n w_i \\left( y_i \\log(y_i/\\hat{y}_i) - (y_i - \\hat{y}_i) \\right)}{\\sum_{i=1}^n w_i}\n\\]\n\n\nCode\n# define loss function\nmean_poisson_deviance &lt;- function(\n    y_true, y_pred, sample_weight = rep(1, length(y))\n  ) {\n  stopifnot(all(y_pred &gt; 0))\n  resid &lt;- ifelse(y_true &gt; 0, y_true * log(y_true / y_pred), 0)\n  resid &lt;- resid - y_true + y_pred\n  2 * sum(resid * sample_weight) / sum(sample_weight)\n}"
  },
  {
    "objectID": "notebooks/demo_r.html#introduction",
    "href": "notebooks/demo_r.html#introduction",
    "title": "Surrogate Modeling with MID in R",
    "section": "",
    "text": "In modern actuarial science, there is often a tension between predictive accuracy and model transparency. While ensemble tree-based models like Gradient Boosting Machines (GBMs) frequently outperform traditional Generalized Linear Models (GLMs), their black-box nature presents significant hurdles for model governance, regulatory filing, and price filing.\nThis notebook demonstrates a solution using Maximum Interpretation Decomposition (MID) via the {midr} and {midnight} packages in R. MID is a functional decomposition framework that acts as a high-fidelity surrogate for complex models. It decomposes black-box predictions into main effects, the isolated impact of each individual feature on the response, and interaction effects, the joint impact of feature pairs.\nBy replicating a black-box model with this structured additive approach, we can quantify the “unexplained” variance and derive an interpretable model that captures the superior predictive power of machine learning without sacrificing clarity.\nWe begin by setting up the environment.\n\n\nCode\n# data manipulation\nlibrary(arrow)\nlibrary(dplyr)\n\n# predictive modeling\nlibrary(gam)\nlibrary(lightgbm)\n\n# surrogate modeling\nlibrary(midr)\nlibrary(midnight)\n\n# visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# load training and testin datasets\ntrain &lt;- read_parquet(\"../data/train.parquet\")\ntest  &lt;- read_parquet(\"../data/test.parquet\")\n\n\nA key component of our evaluation is the Weighted Poisson Deviance defined as follows.\n\\[\nL(\\mathbf{y}, \\hat{\\mathbf{y}}, \\mathbf{w}) = \\frac{2 \\sum_{i=1}^n w_i \\left( y_i \\log(y_i/\\hat{y}_i) - (y_i - \\hat{y}_i) \\right)}{\\sum_{i=1}^n w_i}\n\\]\n\n\nCode\n# define loss function\nmean_poisson_deviance &lt;- function(\n    y_true, y_pred, sample_weight = rep(1, length(y))\n  ) {\n  stopifnot(all(y_pred &gt; 0))\n  resid &lt;- ifelse(y_true &gt; 0, y_true * log(y_true / y_pred), 0)\n  resid &lt;- resid - y_true + y_pred\n  2 * sum(resid * sample_weight) / sum(sample_weight)\n}"
  },
  {
    "objectID": "notebooks/demo_r.html#the-interpretable-baseline-gam",
    "href": "notebooks/demo_r.html#the-interpretable-baseline-gam",
    "title": "Surrogate Modeling with MID in R",
    "section": "The Interpretable Baseline (GAM)",
    "text": "The Interpretable Baseline (GAM)\nWe first fit a GAM to establish a transparent benchmark. Since GAMs are additive by design, they provide a “ground truth” model structure to be recovered by the functional decomposition.\n\nPredictive Modeling\n\n\nCode\nfit_gam &lt;- gam(\n  Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + s(LogDensity) +\n              VehBrand + VehGas + Region,\n  data = train,\n  weights = Exposure,\n  family = quasipoisson(link = \"log\")\n)\n\nsummary(fit_gam)\n\n\n\nCall: gam(formula = Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + \n    s(LogDensity) + VehBrand + VehGas + Region, family = quasipoisson(link = \"log\"), \n    data = train, weights = Exposure)\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-0.7606 -0.3400 -0.2556 -0.1401 10.7719 \n\n(Dispersion Parameter for quasipoisson family taken to be 1.7241)\n\n    Null Deviance: 86471.6 on 338994 degrees of freedom\nResidual Deviance: 84868.97 on 338966 degrees of freedom\nAIC: NA \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n                  Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \ns(VehPower)        1     45   45.32  26.2882 2.942e-07 ***\ns(VehAge)          1     35   34.96  20.2764 6.704e-06 ***\ns(DrivAge)         1    320  319.82 185.4987 &lt; 2.2e-16 ***\ns(LogDensity)      1    531  531.36 308.1989 &lt; 2.2e-16 ***\nVehBrand           5     91   18.14  10.5199 4.072e-10 ***\nVehGas             1     56   55.72  32.3196 1.309e-08 ***\nRegion             6     73   12.15   7.0499 1.606e-07 ***\nResiduals     338966 584406    1.72                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n              Npar Df Npar F     Pr(F)    \n(Intercept)                               \ns(VehPower)         3  1.320    0.2659    \ns(VehAge)           3 11.223 2.328e-07 ***\ns(DrivAge)          3 83.378 &lt; 2.2e-16 ***\ns(LogDensity)       3  1.052    0.3684    \nVehBrand                                  \nVehGas                                    \nRegion                                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# evaluate fitted model\npred_fit_gam &lt;- predict(fit_gam, test, type = \"response\")\n\ndeviance &lt;- mean_poisson_deviance(\n  y_true = test$Frequency,\n  y_pred = pred_fit_gam,\n  sample_weight = test$Exposure\n)\ncat(\"Mean Poisson Deviance of GAM:\", deviance)\n\n\nMean Poisson Deviance of GAM: 0.4679338\n\n\n\n\nSurrogate Modeling\nWe apply the interpret() function to the GAM. This step serves as a sanity check: if MID is effective, it should perfectly replicate the predictive behavior of the original GAM.\n\n\nCode\nmid_gam &lt;- interpret(\n  Frequency ~ VehPower + VehAge + DrivAge + LogDensity +\n              VehBrand + VehGas + Region,\n  data = train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_gam\n)\n\nsummary(mid_gam)\n\n\n\nCall:\ninterpret(formula = Frequency ~ VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region, data = train, model = fit_gam,\n weights = Exposure, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n   working   response \n3.7577e-05 2.5365e-05 \n\nWorking Residuals:\n        Min          1Q      Median          3Q         Max \n-0.01324336 -0.00069460 -0.00004683  0.00057250  0.01822450 \n\nEncoding:\n           main.effect\nVehPower     linear(9)\nVehAge      linear(18)\nDrivAge     linear(25)\nLogDensity  linear(25)\nVehBrand     factor(6)\nVehGas       factor(2)\nRegion       factor(7)\n\n\n\n\nCode\n# evaluate fitted surrogate\npred_mid_gam = predict(mid_gam, test, type = \"response\")\n\ndeviance &lt;- mean_poisson_deviance(\n  y_true = test$Frequency,\n  y_pred = pred_mid_gam,\n  sample_weight = test$Exposure\n)\ncat(\"Mean Poisson Deviance of Surrogate(GAM):\", deviance)\n\n\nMean Poisson Deviance of Surrogate(GAM): 0.4679292\n\n\n\n\nModel Fidelity\nOne way to measure the surrogate model fidelity, i.e., how closely the surrogate model replicates the original model, is to calculate the \\(R^2\\) measure between the original model’s predictions \\(\\mathbf{y}\\) and the surrogate’s predictions \\(\\hat{\\mathbf{y}}\\).\n\\[\nR^2(\\mathbf{y},\\hat{\\mathbf{y}}) = 1 - \\frac{\\sum_{i=1}^n ({y_i}-\\hat{y}_i)^2} {\\sum_{i=1}^n (y_i-\\bar{y}_i)^2}\n\\]\nThe Uninterpreted Variation Ratio\nAs shown by the \\(R^2\\) score on the linear predictor (log) scale, the MID surrogate achieves near-perfect fidelity here.\n\n\nCode\nset.seed(42)\n\n# calculate R-squared on testing dataset\nR2_mid &lt;- weighted.loss(\n  x = log(pred_fit_gam),\n  y = log(pred_mid_gam),\n  w = test$Exposure,\n  method = \"r2\"\n)\n\ncat(sprintf(\"R-squared: %.6f\", R2_mid))\n\n\nR-squared: 0.999963\n\n\n\n\nCode\n# feature effects of GAM\npar.midr(mfrow = c(2, 4))\ntermplot(fit_gam)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# main effects of MID surrogate\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_gam, engine = \"graphics\", ylab = \"Main Effect\")\n\n\n\n\n\n\n\n\n\nWe can also visualize prediction surface with the S3 method of the persp() function for MID models.\n\n\nCode\npar.midr(mar = c(1, 0, 1, 0), mfrow = c(1, 2))\npersp(mid_gam, \"LogDensity:DrivAge\", theta = 225, phi = 40, shade = .5)\npersp(mid_gam, \"LogDensity:Region\", theta = 225, phi = 40, shade = .5)\n\n\n\n\n\n\n\n\n\n\n\nEffect Importance\nBeyond simple plots for feature effects, {midr} provides a suite of diagnostic tools. First, we can quantify feature importance measured as follows:\n\\[\n\\text{Importance}_j=\\mathbf{E}\\left[\\left| f_j(\\mathbf{X}) \\right|\\right]\n\\]\n\n\nCode\nimp_gam &lt;- mid.importance(mid_gam, data = train, max.nrow = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(5, 4),\n  ggmid(imp_gam, fill = \"steelblue\") +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\"),\n  ggmid(imp_gam, type = \"beeswarm\", theme = \"mako@div\") +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n\n\n\n\n\n\n\n\n\n\n\nConditional Expectation\nSecond, we can explore individual conditional expectations (ICE) of the fitted MID model.\n\n\nCode\nice_gam_link &lt;- mid.conditional(mid_gam, type = \"link\", variable = \"DrivAge\")\nice_gam &lt;- mid.conditional(mid_gam, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_gam_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_gam, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAdditive Attribution\nThird, we can perform instance-level attribution through additive breakdown plots, which are directly derived from the functional decomposition.\n\n\nCode\nset.seed(42)\nrow_ids &lt;- sort(sample(nrow(train), 4))\nbd_list &lt;- lapply(\n  row_ids,\n  function(x) {\n    res &lt;- mid.breakdown(mid_gam, train, row = x, format = \"%.6s\")\n    structure(res, row_id = x)\n  }\n)\nbd_plots &lt;- lapply(\n  bd_list, function(x) {\n    label &lt;- paste0(\"Breakdown of Row \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\") +\n      labs(x = NULL, subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\ngrid.arrange(grobs = bd_plots)"
  },
  {
    "objectID": "notebooks/demo_r.html#black-box-gbm",
    "href": "notebooks/demo_r.html#black-box-gbm",
    "title": "Surrogate Modeling with MID in R",
    "section": "Black-Box GBM",
    "text": "Black-Box GBM\nWhile GAMs are transparent, GBMs such as LightGBM often yields superior predictive power by capturing high-order interactions. However, this accuracy comes at the cost of being a black box. Here, we train a LightGBM model using optimized hyperparameters to achieve high predictive performance.\n\n\nCode\n# hold out validation dataset\nvalid_idx &lt;- seq_len(floor(nrow(train) * 0.2))\n\n# create datasets for training\ndtrain &lt;- lgb.Dataset(\n  data.matrix(select(train[-valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[-valid_idx],\n  weight = train$Exposure[-valid_idx],\n  categorical_feature = c(\"VehBrand\", \"VehGas\", \"Region\")\n)\n\ndvalid &lt;- lgb.Dataset.create.valid(\n  dtrain,\n  data.matrix(select(train[ valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[ valid_idx],\n  weight = train$Exposure[ valid_idx]\n)\n\n# model parameters\nparams_lgb &lt;- list(\n  objective = \"poisson\",\n  learning_rate = 0.03188002,\n  num_leaves = 30,\n  reg_lambda = 0.004201069,\n  reg_alpha = 0.2523909,\n  colsample_bynode = 0.5552524,\n  subsample = 0.5938199,\n  min_child_samples = 9,\n  min_split_gain = 0.3920509,\n  poisson_max_delta_step = 0.8039541\n)\n\nset.seed(42)\nfit_lgb &lt;- lgb.train(\n  params = params_lgb,\n  data = dtrain,\n  nrounds = 1000L,\n  valids = list(eval = dvalid),\n  early_stopping_round = 50L,\n  verbose = 0L\n)\n\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n\nCode\nsummary(fit_lgb)\n\n\nLightGBM Model (402 trees)\nObjective: poisson\nFitted to dataset with 7 columns\n\n\nThe weighted mean Poisson deviance of the fitted LightGBM is around 0.4655023, which is better than the GAM’s performance.\n\n\nCode\n# evaluate model\npred_fit_lgb &lt;- predict(\n  fit_lgb, data.matrix(select(test, -Frequency, -Exposure))\n)\n\ncat(\"Mean Poisson Deviance:\",\n    mean_poisson_deviance(test$Frequency, pred_fit_lgb, test$Exposure))\n\n\nMean Poisson Deviance: 0.4655023\n\n\n\nSurrogate Modeling\nWe now use {midr} to replicate the LightGBM model. By including interaction terms in the model formula, we allow the surrogate to capture two-way joint relationships of features that the GBM might have learned from the data.\n\n\nCode\nmid_lgb &lt;- interpret(\n  Frequency ~ (VehPower + VehAge + DrivAge + LogDensity +\n               VehBrand + VehGas + Region)^2,\n  data = train,\n  lambda = 0.01,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_lgb,\n  pred.fun = function(model, data) {\n    newdata &lt;- data.matrix(select(data, -Frequency, -Exposure))\n    predict(model, newdata)\n  }\n)\n\nsummary(mid_lgb)\n\n\n\nCall:\ninterpret(formula = Frequency ~ (VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region)^2, data = train,\n model = fit_lgb, pred.fun = function(model, data) {\n newdata &lt;- data.matrix(select(data, -Frequency, -Exposure))\n predict(model, newdata)\n }, weights = Exposure, lambda = 0.01, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n working response \n0.071642 0.181512 \n\nWorking Residuals:\n     Min       1Q   Median       3Q      Max \n-0.51124 -0.04431 -0.00326  0.03899  3.63211 \n\nEncoding:\n           main.effect interaction\nVehPower     linear(9)   linear(5)\nVehAge      linear(18)   linear(5)\nDrivAge     linear(25)   linear(5)\nLogDensity  linear(25)   linear(5)\nVehBrand     factor(6)   factor(6)\nVehGas       factor(2)   factor(2)\nRegion       factor(7)   factor(7)\n\n\n\n\nCode\npred_mid_lgb = predict(mid_lgb, test, type = \"response\")\n\ncat(\"Mean Poisson Deviance:\",\n    mean_poisson_deviance(test$Frequency, pred_mid_lgb, test$Exposure))\n\n\nMean Poisson Deviance: 0.4670904\n\n\n\n\nModel Fidelity\n\n\nCode\ntheme_set(theme_midr(\"xy\"))\n\n# calculate R-squared on testing dataset\nR2_mid &lt;- weighted.loss(\n  x = log(pred_fit_lgb),\n  y = log(pred_mid_lgb),\n  w = test$Exposure,\n  method = \"r2\"\n)\n\ncat(sprintf(\"R-squared: %.4f\", R2_mid))\n\n\nR-squared: 0.9295\n\n\n\n\nMain Effect\n\n\nCode\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_lgb, engine = \"graphics\", ylab = \"Partial Effect\")\n\n\n\n\n\n\n\n\n\n\n\nInteraction Effect\nOne of the important features of {midr} is its ability to isolate interaction effects. In this section, we visualize how geographical Region interacts with LogDensity. These insights are often hidden in GBMs but are critical in actuarial practice.\n\n\nCode\ngrid.arrange(\n  nrow = 1, widths = c(3, 2),\n  ggmid(mid_lgb, \"LogDensity:Region\", type = \"data\",\n        data = train[1:1e4, ]) +\n    labs(y = NULL, subtitle = \"Interaction Effect\") +\n    theme(legend.position = \"bottom\"),\n  ggmid(mid_lgb, \"LogDensity:Region\", main.effects = TRUE) +\n    labs(y = NULL, subtitle = \"Total Effect\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\npar.midr(mar = c(1, 0, 1, 0), mfrow = c(1, 2))\npersp(mid_lgb, \"LogDensity:DrivAge\", theta = 225, phi = 40, shade = .5)\npersp(mid_lgb, \"LogDensity:Region\", theta = 225, phi = 40, shade = .5)\n\n\n\n\n\n\n\n\n\n\n\nEffect Importance\n\n\nCode\nimp_lgb &lt;- mid.importance(mid_lgb, data = train, max.nrow = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(4, 3),\n  ggmid(imp_lgb, theme = \"bluescale@qual\", max.nterms = 20) +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\") +\n    theme(legend.position = \"none\"),\n  ggmid(imp_lgb, type = \"beeswarm\", theme = \"mako@div\", max.nterms = 20) +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n\n\n\n\n\n\n\n\n\n\n\nConditional Expectation\n\n\nCode\nice_lgb_link &lt;- mid.conditional(mid_lgb, type = \"link\", variable = \"DrivAge\")\nice_lgb &lt;- mid.conditional(mid_lgb, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_lgb_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_lgb, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAdditive Attribution\n\n\nCode\nset.seed(42)\nrow_ids &lt;- sort(sample(nrow(train), 4))\n\nbd_list &lt;- lapply(\n  row_ids,\n  function(x) {\n    res &lt;- mid.breakdown(mid_lgb, train, row = x,\n                         format = c(\"%.6s\", \"%.3s, %.3s\"))\n    structure(res, row_id = x)\n  }\n)\n\nbd_plots &lt;- lapply(\n  bd_list, function(x) {\n    label &lt;- paste0(\"Breakdown of Row \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\", max.nterms = 10) +\n      labs(x = \"Linear Predictor\", subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\n\ngrid.arrange(grobs = bd_plots)"
  },
  {
    "objectID": "notebooks/demo_r.html#conclusion",
    "href": "notebooks/demo_r.html#conclusion",
    "title": "Surrogate Modeling with MID in R",
    "section": "Conclusion",
    "text": "Conclusion\nBy using {midr}, we have transformed a complex LightGBM model into a set of interpretable charts and attributions."
  },
  {
    "objectID": "notebooks/dataset.html",
    "href": "notebooks/dataset.html",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#overview",
    "href": "notebooks/dataset.html#overview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#data-preprocessing",
    "href": "notebooks/dataset.html#data-preprocessing",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThe following transformations are applied to handle outliers, improve model stability, and align with actuarial pricing practices:\n\nTarget Variable: We define Frequency as the number of claims per unit of exposure. \\[\\text{Frequency} = \\frac{\\text{ClaimNb}}{\\text{Exposure}} \\]\nCapping: VehAge (vehicle age) is capped at 25 years to mitigate the influence of extreme outliers (vintage cars) which follow different risk profiles.\nLog-Transformation: Density (population density) is log-transformed to stabilize the variance and simplify its relationship with claim frequency.\nCategorical Lumping: For VehBrand and Region, infrequent levels are collapsed into an “Other” category. This prevents overfitting in low-exposure segments.\n\n\n\nCode\n# load dataset from CASdatasets\ndata(freMTPL2freq, package = \"CASdatasets\")\n\n# preprocess dataset\ndf_all &lt;- freMTPL2freq |&gt;\n  dplyr::mutate(\n    Frequency  = ClaimNb / Exposure,\n    VehAge = pmin(VehAge, 25),\n    VehBrand = forcats::fct_lump(VehBrand, 5),\n    LogDensity = log(Density),\n    Region = forcats::fct_lump(Region, 6)\n  ) |&gt;\n  dplyr::select(\n    Frequency, Exposure, VehPower, VehAge,\n    DrivAge, VehBrand, VehGas, LogDensity, Region\n  ) |&gt;\n  dplyr::as_tibble()"
  },
  {
    "objectID": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "href": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Split for Hold-Out Validation",
    "text": "Data Split for Hold-Out Validation\nTo ensure a robust evaluation of our models, we split the dataset into training and testing sets.\nWe store these datasets in Parquet format. Unlike standard CSV files, Parquet preserves schema information (e.g., categorical types) and provides high-performance I/O.\nBy locking the data into a binary format after the initial split, we guarantee that all subsequent modeling steps across different environments, R and Python, remain consistent.\n\n\nCode\n# split dataset\nset.seed(42)\ndf_split &lt;- rsample::initial_split(df_all, prop = 1/2)\ndf_train &lt;- rsample::training(df_split)\ndf_test  &lt;- rsample::testing(df_split)\n\n# write dataset as parquets\narrow::write_parquet(df_train, \"../data/train.parquet\")\narrow::write_parquet(df_test, \"../data/test.parquet\")"
  },
  {
    "objectID": "notebooks/dataset.html#data-preview",
    "href": "notebooks/dataset.html#data-preview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preview",
    "text": "Data Preview\nBelow is a summary of the first 1000 rows of the processed training dataset. The Exposure column will be utilized as an offset in our subsequent modeling to account for varying policy durations."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Overview",
    "text": "Overview\nWelcome to the supplementary repository for our presentation Maximizing the Interpretation of Black-Box Models at CONVENTION A | ASIA.\nThis project demonstrates the application of Maximum Interpretation Decomposition (MID), a novel approach in Interpretable Machine Learning (IML) designed to bridge the gap between high-performance “black-box” models and the transparency requirements of actuarial practice.\n\nKey Topics of the Presentation\n\nTheoretical Foundation: Understanding the decomposition logic of complex models into interpretable parts: intercept, first-order main effects, and second-order interaction effects by MID.\nActuarial Application: Benchmarking MID using the industry-standard freMTPL2freq dataset using open-source software: {midr}, {midnight} (R), and {midlearn} (Python)."
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Project Structure",
    "text": "Project Structure\nThis Quarto website contains the following sections:\n\nDataset: Data cleaning and feature engineering of the French Motor Third-Party Liability dataset.\nR Demo: A demonstration of the {midr} and {midlearn} packages in R.\nPython Demo: A demonstration of the {midlearn} library, providing a seamless interface for Python users."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Software",
    "text": "Software\n\n{midr}: GitHub / CRAN\nR package for Maximum Interpretation Decomposition in R\n{midnight}: GitHub\nR package to integrate {midr} to the {tidymodels} ecosystem\n{midlearn}: GitHub / PyPI\nPython library to integrate {midr} to the {scikit-learn} ecosystem"
  },
  {
    "objectID": "index.html#acknowledgment-reference",
    "href": "index.html#acknowledgment-reference",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Acknowledgment & Reference",
    "text": "Acknowledgment & Reference\n\n{CASdatasets}: CRAN\nThis demonstration utilizes the freMTPL2freq dataset from the {CASdatasets} package.\nAITools4Actuaries: Website\nWe would like to acknowledge the project for their foundational work on benchmarking ML models in insurance, which served as a reference, especially for our data preprocessing pipeline."
  },
  {
    "objectID": "notebooks/demo_py.html",
    "href": "notebooks/demo_py.html",
    "title": "Python Demo",
    "section": "",
    "text": "Code\nimport midlearn as mid\nfrom plotnine import *\n\n\nError importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\nTrying to import in ABI mode.\n\n\n\n\nCode\nimport pandas as pd\n\ntrain = pd.read_parquet(\"../data/train.parquet\")\ntest  = pd.read_parquet(\"../data/test.parquet\")\n\nprint(train.head())\n\n\n   Frequency  Exposure  VehPower  VehAge  DrivAge VehBrand   VehGas  \\\n0        0.0      1.00         6     9.0       43       B1   Diesel   \n1        0.0      0.18         4    16.0       30       B2  Regular   \n2        0.0      0.53         8     3.0       48      B12   Diesel   \n3        0.0      0.08         6    18.0       38    Other  Regular   \n4        0.0      1.00         7     5.0       69       B2  Regular   \n\n   LogDensity                       Region  \n0    9.138522                  Rhone-Alpes  \n1    9.138522                  Rhone-Alpes  \n2    6.336826  Provence-Alpes-Cotes-D'Azur  \n3    8.336151                  Rhone-Alpes  \n4    7.242082                        Other"
  }
]