[
  {
    "objectID": "notebooks/demo_r.html",
    "href": "notebooks/demo_r.html",
    "title": "Surrogate Modeling with MID in R",
    "section": "",
    "text": "In modern actuarial science, there is an inherent tension between predictive accuracy and model transparency. While ensemble tree-based models like Gradient Boosting Machines (GBMs) frequently outperform traditional Generalized Linear Models (GLMs), their “black-box” nature presents significant hurdles for model governance, regulatory compliance, and price filing.\nThis notebook demonstrates a solution using Maximum Interpretation Decomposition (MID) via the {midr} and {midnight} packages in R.\n\n\n\n\n\n\nCompatibility Notice\n\n\n\nThis article relies on features introduced in midr (&gt;= 0.5.3) and midnight (&gt;= 0.1.1). Please ensure your packages are up to date. Some visualization arguments are not available in earlier versions.\n\n\n\n\nMID is a functional decomposition framework that acts as a high-fidelity surrogate for complex models. It deconstructs a black-box prediction function \\(f(\\mathbf{X})\\) into several interpretable components: intercept \\(g_\\emptyset\\), main effects \\(g_j(X_j)\\), and interaction effects \\(g_{jk}(X_j, X_k)\\). The prediction is represented as the following additive structure:\n\\[\nf(\\mathbf{X}) = g_\\emptyset + \\sum_{j} g_j(X_{j}) + \\sum_{j &lt; k} g_{jk}(X_{j},\\;X_{k}) + \\dots + g_D(\\mathbf{X})\n\\]\nTo ensure the uniqueness and interpretability of each component, MID imposes the centering constraints:\n\\[\n\\begin{aligned}\n\\mathbf{E}\\left[g_j(X_j)\\right] &= 0 \\\\\n\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_j = x_j\\right] &= 0 \\quad (\\forall x_j) \\\\\n\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_k = x_k\\right] &= 0 \\quad (\\forall x_k)\n\\end{aligned}\n\\]\nBy replicating a black-box model with this structured approach, we can quantify the “uninterpreted” variance (captured by \\(g_D(\\mathbf{X})\\)) and derive a representation that captures the superior predictive power of machine learning without sacrificing actuarial clarity.\n\n\n\nWe begin by setting up the environment and loading the necessary libraries.\n\n\nCode\n# data manipulation\nlibrary(arrow)\nlibrary(dplyr)\n\n# predictive modeling\nlibrary(gam)\nlibrary(lightgbm)\n\n# surrogate modeling\nlibrary(midr)\nlibrary(midnight)\n\n# visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# load training and testing datasets\ntrain &lt;- read_parquet(\"../data/train.parquet\")\ntest  &lt;- read_parquet(\"../data/test.parquet\")\n\n\nA key component of our evaluation is the Weighted Mean Poisson Deviance defined as follows:\n\\[\nL(\\mathbf{y}, \\hat{\\mathbf{y}}, \\mathbf{w}) = \\frac{2 \\sum_{i=1}^n w_i \\left( y_i \\log(y_i/\\hat{y}_i) - (y_i - \\hat{y}_i) \\right)}{\\sum_{i=1}^n w_i}\n\\]\n\n\nCode\n# define loss function\nmean_poisson_deviance &lt;- function(\n    y_true, y_pred, sample_weight = rep(1, length(y))\n  ) {\n  stopifnot(all(y_pred &gt; 0))\n  resid &lt;- ifelse(y_true &gt; 0, y_true * log(y_true / y_pred), 0)\n  resid &lt;- resid - y_true + y_pred\n  2 * sum(resid * sample_weight) / sum(sample_weight)\n}"
  },
  {
    "objectID": "notebooks/demo_r.html#introduction",
    "href": "notebooks/demo_r.html#introduction",
    "title": "Surrogate Modeling with MID in R",
    "section": "",
    "text": "In modern actuarial science, there is an inherent tension between predictive accuracy and model transparency. While ensemble tree-based models like Gradient Boosting Machines (GBMs) frequently outperform traditional Generalized Linear Models (GLMs), their “black-box” nature presents significant hurdles for model governance, regulatory compliance, and price filing.\nThis notebook demonstrates a solution using Maximum Interpretation Decomposition (MID) via the {midr} and {midnight} packages in R.\n\n\n\n\n\n\nCompatibility Notice\n\n\n\nThis article relies on features introduced in midr (&gt;= 0.5.3) and midnight (&gt;= 0.1.1). Please ensure your packages are up to date. Some visualization arguments are not available in earlier versions.\n\n\n\n\nMID is a functional decomposition framework that acts as a high-fidelity surrogate for complex models. It deconstructs a black-box prediction function \\(f(\\mathbf{X})\\) into several interpretable components: intercept \\(g_\\emptyset\\), main effects \\(g_j(X_j)\\), and interaction effects \\(g_{jk}(X_j, X_k)\\). The prediction is represented as the following additive structure:\n\\[\nf(\\mathbf{X}) = g_\\emptyset + \\sum_{j} g_j(X_{j}) + \\sum_{j &lt; k} g_{jk}(X_{j},\\;X_{k}) + \\dots + g_D(\\mathbf{X})\n\\]\nTo ensure the uniqueness and interpretability of each component, MID imposes the centering constraints:\n\\[\n\\begin{aligned}\n\\mathbf{E}\\left[g_j(X_j)\\right] &= 0 \\\\\n\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_j = x_j\\right] &= 0 \\quad (\\forall x_j) \\\\\n\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_k = x_k\\right] &= 0 \\quad (\\forall x_k)\n\\end{aligned}\n\\]\nBy replicating a black-box model with this structured approach, we can quantify the “uninterpreted” variance (captured by \\(g_D(\\mathbf{X})\\)) and derive a representation that captures the superior predictive power of machine learning without sacrificing actuarial clarity.\n\n\n\nWe begin by setting up the environment and loading the necessary libraries.\n\n\nCode\n# data manipulation\nlibrary(arrow)\nlibrary(dplyr)\n\n# predictive modeling\nlibrary(gam)\nlibrary(lightgbm)\n\n# surrogate modeling\nlibrary(midr)\nlibrary(midnight)\n\n# visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# load training and testing datasets\ntrain &lt;- read_parquet(\"../data/train.parquet\")\ntest  &lt;- read_parquet(\"../data/test.parquet\")\n\n\nA key component of our evaluation is the Weighted Mean Poisson Deviance defined as follows:\n\\[\nL(\\mathbf{y}, \\hat{\\mathbf{y}}, \\mathbf{w}) = \\frac{2 \\sum_{i=1}^n w_i \\left( y_i \\log(y_i/\\hat{y}_i) - (y_i - \\hat{y}_i) \\right)}{\\sum_{i=1}^n w_i}\n\\]\n\n\nCode\n# define loss function\nmean_poisson_deviance &lt;- function(\n    y_true, y_pred, sample_weight = rep(1, length(y))\n  ) {\n  stopifnot(all(y_pred &gt; 0))\n  resid &lt;- ifelse(y_true &gt; 0, y_true * log(y_true / y_pred), 0)\n  resid &lt;- resid - y_true + y_pred\n  2 * sum(resid * sample_weight) / sum(sample_weight)\n}"
  },
  {
    "objectID": "notebooks/demo_r.html#the-interpretable-baseline-gam",
    "href": "notebooks/demo_r.html#the-interpretable-baseline-gam",
    "title": "Surrogate Modeling with MID in R",
    "section": "The Interpretable Baseline: GAM",
    "text": "The Interpretable Baseline: GAM\nWe first fit a GAM to establish a transparent benchmark. Since GAMs are additive by design, they provide a “ground truth” model structure to be recovered by the functional decomposition.\n\n\nCode\nfit_gam &lt;- gam(\n  Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + s(LogDensity) +\n              VehBrand + VehGas + Region,\n  data = train,\n  weights = Exposure,\n  family = quasipoisson(link = \"log\")\n)\n\nsummary(fit_gam)\n\n\n\nCall: gam(formula = Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + \n    s(LogDensity) + VehBrand + VehGas + Region, family = quasipoisson(link = \"log\"), \n    data = train, weights = Exposure)\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-0.7606 -0.3400 -0.2556 -0.1401 10.7719 \n\n(Dispersion Parameter for quasipoisson family taken to be 1.7241)\n\n    Null Deviance: 86471.6 on 338994 degrees of freedom\nResidual Deviance: 84868.97 on 338966 degrees of freedom\nAIC: NA \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n                  Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \ns(VehPower)        1     45   45.32  26.2882 2.942e-07 ***\ns(VehAge)          1     35   34.96  20.2764 6.704e-06 ***\ns(DrivAge)         1    320  319.82 185.4987 &lt; 2.2e-16 ***\ns(LogDensity)      1    531  531.36 308.1989 &lt; 2.2e-16 ***\nVehBrand           5     91   18.14  10.5199 4.072e-10 ***\nVehGas             1     56   55.72  32.3196 1.309e-08 ***\nRegion             6     73   12.15   7.0499 1.606e-07 ***\nResiduals     338966 584406    1.72                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n              Npar Df Npar F     Pr(F)    \n(Intercept)                               \ns(VehPower)         3  1.320    0.2659    \ns(VehAge)           3 11.223 2.328e-07 ***\ns(DrivAge)          3 83.378 &lt; 2.2e-16 ***\ns(LogDensity)       3  1.052    0.3684    \nVehBrand                                  \nVehGas                                    \nRegion                                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# evaluate fitted model\npred_fit_gam &lt;- predict(fit_gam, test, type = \"response\")\n\ndeviance &lt;- mean_poisson_deviance(\n  y_true = test$Frequency,\n  y_pred = pred_fit_gam,\n  sample_weight = test$Exposure\n)\ncat(\"Mean Poisson Deviance:\", deviance)\n\n\nMean Poisson Deviance: 0.4679338\n\n\n\nSurrogate Modeling\nWe apply the interpret() function to the GAM. This step serves as a sanity check: if MID is effective, it should perfectly replicate the predictive behavior of the original GAM.\n\n\nCode\nmid_gam &lt;- interpret(\n  Frequency ~ VehPower + VehAge + DrivAge + LogDensity +\n              VehBrand + VehGas + Region,\n  data = train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_gam\n)\n\nsummary(mid_gam)\n\n\n\nCall:\ninterpret(formula = Frequency ~ VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region, data = train, model = fit_gam,\n weights = Exposure, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n   working   response \n3.7577e-05 2.5365e-05 \n\nWorking Residuals:\n        Min          1Q      Median          3Q         Max \n-0.01324336 -0.00069460 -0.00004683  0.00057250  0.01822450 \n\nEncoding:\n           main.effect\nVehPower     linear(9)\nVehAge      linear(18)\nDrivAge     linear(25)\nLogDensity  linear(25)\nVehBrand     factor(6)\nVehGas       factor(2)\nRegion       factor(7)\n\n\n\n\nCode\n# evaluate fitted surrogate\npred_mid_gam = predict(mid_gam, test, type = \"response\")\n\ndeviance &lt;- mean_poisson_deviance(\n  y_true = test$Frequency,\n  y_pred = pred_mid_gam,\n  sample_weight = test$Exposure\n)\ncat(\"Mean Poisson Deviance:\", deviance)\n\n\nMean Poisson Deviance: 0.4679292\n\n\n\n\nModel Fidelity\nTo assess model fidelity, i.e., how closely the surrogate replicates the black-box, we calculate the uninterpreted variation ratio \\(\\mathbf{U}\\).\n\\[\n\\mathbf{U}(f,g) = \\frac{\\sum_{i=1}^n (f(x_i) - g(x_i))^2}{\\sum_{i=1}^n (f(x_i) - \\bar{f})^2}, \\quad \\text{where } \\bar{f} = \\frac{1}{n}\\sum_{i=1}^n f(x_i)\n\\]\nThis metric represents the proportion of the black-box model’s variance that is not captured by the additive components of the MID model. The R-squared score, \\(\\mathbf{R}^2(f,g) = 1 - \\mathbf{U}(f,g)\\), is a standard measure for this purpose. It is important to note that this \\(\\mathbf{R}^2\\) measures the fidelity to the black-box model, not the predictive accuracy relative to the ground truth observations.\nIn the {midr} package, the summary output includes this ratio calculated on the training set. For models with non-linear links (e.g., Poisson regression), the “working” ratio is computed on the scale of the link function (e.g., \\(\\log\\) scale).\nTo rigorously confirm the model fidelity, it is recommended to evaluate these metrics on a separate testing set. This ensures that the surrogate model is not just over-fitting the training predictions but has truly captured the underlying functional structure.\n\n\nCode\n# calculate R-squared on testing dataset\nR2_mid &lt;- weighted.loss(\n  x = log(pred_fit_gam),\n  y = log(pred_mid_gam),\n  w = test$Exposure,\n  method = \"r2\"\n)\n\ncat(sprintf(\"R-squared: %.6f\", R2_mid))\n\n\nR-squared: 0.999963\n\n\nAs shown by the high \\(\\mathbf{R}^2\\) score, the MID surrogate achieves near-perfect fidelity. This level of agreement justifies using the MID components (main effects and interactions) as a reliable lens through which to interpret the original black-box model’s behavior.\n\n\nFeature Effects\nVisualizing the functional behavior of each component allows for a direct comparison between the MID surrogate’s decomposition and the original GAM’s structure.\n\nMID Surrogate\n\n\n\nCode\n# main effects of MID surrogate\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_gam, engine = \"graphics\", ylab = \"Main Effect\")\n\n\n\n\n\n\n\n\n\n\nOriginal GAM\n\n\n\nCode\n# feature effects of GAM\npar.midr(mfrow = c(2, 4))\ntermplot(fit_gam)\n\n\n\n\n\n\n\n\n\nFurthermore, we can visualize the joint effects of feature pairs as 3D prediction surfaces using the S3 method for the persp() function.\n\n\nCode\npar.midr(mar = c(1, 0, 1, 0), mfrow = c(1, 2))\npersp(mid_gam, \"DrivAge:LogDensity\", theta = 45, phi = 40)\npersp(mid_gam, \"LogDensity:Region\", theta = -45, phi = 40)\n\n\n\n\n\n\n\n\n\n\n\nEffect Importance\nBeyond simple plots for feature effects, {midr} provides a suite of diagnostic tools. First, the Effect Importance of a term \\(j\\) is defined as the mean absolute contribution of that term across the population:\n\\[\n\\text{Importance}_j = \\mathbf{E} \\left[ | g_j(X_j) | \\right] \\approx \\frac{1}{n} \\sum_{i=1}^n | g_j(x_{ij}) |\n\\]\n\n\nCode\nimp_gam &lt;- mid.importance(mid_gam, data = train, max.nsamples = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(5, 4),\n  ggmid(imp_gam, fill = \"steelblue\") +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\"),\n  ggmid(imp_gam, type = \"beeswarm\", theme = \"mako@div\") +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n\n\n\n\n\n\n\n\n\nFor interaction terms, the importance is similarly calculated using \\(g_{jk}(X_j, X_k)\\). This metric allows us to rank features by their average influence on the model’s predictions.\n\n\nConditional Expectation\nSecond, we can explore Individual Conditional Expectations (ICE). In the MID framework, the ICE for a feature \\(j\\) and a specific observation \\(i\\) is the expected value of the prediction as \\(X_j\\) varies, while keeping other features fixed at their observed values \\(\\mathbf{x}_{\\setminus j}^{(i)}\\):\n\\[\n\\text{ICE}_j^{(i)}(x) = g_\\emptyset + g_j(x) + \\sum_{k \\neq j} g_{jk}(x, x_{ik})\n\\]\n\n\nCode\nice_gam_link &lt;- mid.conditional(mid_gam, type = \"link\", variable = \"DrivAge\")\nice_gam &lt;- mid.conditional(mid_gam, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_gam_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_gam, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n\n\n\n\n\n\n\n\n\nUnlike standard black-box models, MID’s low-order structure allows us to compute these expectations efficiently and interpret the variation across curves (the “thickness” of the ICE plot) as a direct consequence of specified interaction terms \\(g_{jk}\\).\n\n\nAdditive Attribution\nThird, we perform instance-level explanation through an Additive Breakdown of the prediction. For any single observation \\(\\mathbf{x}\\), the MID surrogate’s prediction \\(g(\\mathbf{x})\\) is decomposed into the exact sum of its functional components:\n\\[\ng(\\mathbf{x}) = \\underbrace{g_\\emptyset}_{\\text{Intercept}} + \\underbrace{\\sum_{j} g_j(x_j)}_{\\text{Main Effects}} + \\underbrace{\\sum_{j &lt; k} g_{jk}(x_j, x_k)}_{\\text{Interactions}}\n\\]\n\n\nCode\nset.seed(42)\nrow_ids &lt;- sort(sample(nrow(train), 4))\nbd_list &lt;- lapply(\n  row_ids,\n  function(x) {\n    res &lt;- mid.breakdown(mid_gam, train, row = x)\n    structure(res, row_id = x)\n  }\n)\nbd_plots &lt;- lapply(\n  bd_list, function(x) {\n    label &lt;- paste0(\"Breakdown for Row \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\", format.args = list(digits = 4)) +\n      labs(x = NULL, subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\ngrid.arrange(grobs = bd_plots)\n\n\n\n\n\n\n\n\n\nBy visualizing these contributions in a waterfall plot, we can identify which specific risk factors or interaction effects drove the prediction for a particular instance, such as a high-risk policyholder."
  },
  {
    "objectID": "notebooks/demo_r.html#the-black-box-lightgbm",
    "href": "notebooks/demo_r.html#the-black-box-lightgbm",
    "title": "Surrogate Modeling with MID in R",
    "section": "The Black-Box: LightGBM",
    "text": "The Black-Box: LightGBM\nWhile GAMs are transparent, GBMs such as LightGBM often yield superior predictive power by capturing high-order interactions. However, this accuracy comes at the cost of being a black box.\n\n\nCode\n# hold out validation dataset\nvalid_idx &lt;- seq_len(floor(nrow(train) * 0.2))\n\n# create datasets for training\ndtrain &lt;- lgb.Dataset(\n  data.matrix(select(train[-valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[-valid_idx],\n  weight = train$Exposure[-valid_idx],\n  categorical_feature = c(\"VehBrand\", \"VehGas\", \"Region\")\n)\n\ndvalid &lt;- lgb.Dataset.create.valid(\n  dtrain,\n  data.matrix(select(train[ valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[ valid_idx],\n  weight = train$Exposure[ valid_idx]\n)\n\n# model parameters\nparams_lgb &lt;- list(\n  objective = \"poisson\",\n  learning_rate = 0.03188002,\n  num_leaves = 30,\n  reg_lambda = 0.004201069,\n  reg_alpha = 0.2523909,\n  colsample_bynode = 0.5552524,\n  subsample = 0.5938199,\n  min_child_samples = 9,\n  min_split_gain = 0.3920509,\n  poisson_max_delta_step = 0.8039541\n)\n\nset.seed(42)\nfit_lgb &lt;- lgb.train(\n  params = params_lgb,\n  data = dtrain,\n  nrounds = 1000L,\n  valids = list(eval = dvalid),\n  early_stopping_round = 50L,\n  verbose = -1L\n)\n\nsummary(fit_lgb)\n\n\nLightGBM Model (402 trees)\nObjective: poisson\nFitted to dataset with 7 columns\n\n\n\n\nCode\n# evaluate fitted model\npred_fit_lgb &lt;- predict(\n  fit_lgb, data.matrix(select(test, -Frequency, -Exposure))\n)\n\ndeviance &lt;- mean_poisson_deviance(\n  y_true = test$Frequency,\n  y_pred = pred_fit_lgb,\n  sample_weight = test$Exposure\n)\ncat(\"Mean Poisson Deviance:\", deviance)\n\n\nMean Poisson Deviance: 0.4655023\n\n\n\nSurrogate Modeling\nWe use {midr} to replicate the LightGBM model. By including interaction terms in the model formula, we allow the surrogate to capture the joint relationships that the GBM has learned. The goal is to approximate the LightGBM function \\(f_{LGB}(\\mathbf{x})\\) with our interpretable structure \\(g(\\mathbf{x})\\):\n\\[\nf_{LGB}(\\mathbf{x}) \\approx g(\\mathbf{x}) = g_\\emptyset + \\sum_{j} g_j(x_j) + \\sum_{j &lt; k} g_{jk}(x_j, x_k)\n\\]\n\n\n\n\n\n\nComputational Considerations\n\n\n\nIncluding all second-order interactions using the (...)^2 syntax results in \\(p(p-1)/2\\) interaction terms. For high-dimensional data, this can be memory-intensive. Users should ensure sufficient RAM is available or consider limiting the formula to the most relevant features, or using a subset of the training set.\n\n\n\n\nCode\nmid_lgb &lt;- interpret(\n  Frequency ~ (VehPower + VehAge + DrivAge + LogDensity +\n               VehBrand + VehGas + Region)^2,\n  data = train,\n  lambda = 0.01,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_lgb,\n  pred.fun = function(model, data) {\n    newdata &lt;- data.matrix(select(data, -Frequency, -Exposure))\n    predict(model, newdata)\n  }\n)\n\nsummary(mid_lgb)\n\n\n\n\n\nCall:\ninterpret(formula = Frequency ~ (VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region)^2, data = train,\n model = fit_lgb, pred.fun = function(model, data) {\n newdata &lt;- data.matrix(select(data, -Frequency, -Exposure))\n predict(model, newdata)\n }, weights = Exposure, lambda = 0.01, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n working response \n0.071642 0.181512 \n\nWorking Residuals:\n     Min       1Q   Median       3Q      Max \n-0.51124 -0.04431 -0.00326  0.03899  3.63211 \n\nEncoding:\n           main.effect interaction\nVehPower     linear(9)   linear(5)\nVehAge      linear(18)   linear(5)\nDrivAge     linear(25)   linear(5)\nLogDensity  linear(25)   linear(5)\nVehBrand     factor(6)   factor(6)\nVehGas       factor(2)   factor(2)\nRegion       factor(7)   factor(7)\n\n\n\n\nCode\n# evaluate fitted surrogate\npred_mid_lgb = predict(mid_lgb, test, type = \"response\")\n\ndeviance &lt;- mean_poisson_deviance(\n  y_true = test$Frequency,\n  y_pred = pred_mid_lgb,\n  sample_weight = test$Exposure\n)\ncat(\"Mean Poisson Deviance:\", deviance)\n\n\nMean Poisson Deviance: 0.4670904\n\n\n\n\nModel Fidelity\nTo measure how successfully our surrogate replicates the LightGBM model, we use the R-squared score on the link scale (\\(\\log\\) scale).\n\n\nCode\n# calculate R-squared on testing dataset\nR2_mid &lt;- weighted.loss(\n  x = log(pred_fit_lgb),\n  y = log(pred_mid_lgb),\n  w = test$Exposure,\n  method = \"r2\"\n)\n\ncat(sprintf(\"R-squared: %.4f\", R2_mid))\n\n\nR-squared: 0.9295\n\n\n\n\nFeature Effects\n\n\nCode\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_lgb, engine = \"graphics\", ylab = \"Main Effect\")\n\n\n\n\n\n\n\n\n\nA key advantage of {midr} is its ability to isolate interaction effects \\(g_{jk}\\) from main effects \\(g_j\\). This is particularly useful to understand the joint impact of two variables (e.g., Region and LogDensity).\n\n\nCode\ngrid.arrange(\n  nrow = 1, widths = c(3, 2),\n  ggmid(mid_lgb, \"LogDensity:Region\", type = \"data\",\n        data = train[1:1e4, ]) +\n    labs(y = NULL, subtitle = \"Interaction Effect\") +\n    theme(legend.position = \"bottom\"),\n  ggmid(mid_lgb, \"LogDensity:Region\", main.effects = TRUE) +\n    labs(y = NULL, subtitle = \"Total Effect\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\npar.midr(mar = c(1, 0, 1, 0), mfrow = c(1, 2))\npersp(mid_lgb, \"DrivAge:LogDensity\", theta = 45, phi = 40)\npersp(mid_lgb, \"LogDensity:Region\", theta = -45, phi = 40)\n\n\n\n\n\n\n\n\n\n\n\nEffect Importance\nTo rank the influence of each component discovered in the LightGBM model, we calculate the Effect Importance, defined as the average absolute contribution.\n\n\nCode\nimp_lgb &lt;- mid.importance(mid_lgb, data = train, max.nsamples = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(4, 3),\n  ggmid(imp_lgb, theme = \"bluescale@qual\", max.nterms = 20) +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\") +\n    theme(legend.position = \"none\"),\n  ggmid(imp_lgb, type = \"beeswarm\", theme = \"mako@div\", max.nterms = 20) +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n\n\n\n\n\n\n\n\n\n\n\nConditional Expectation\nWe further explore the model’s behavior using the ICE plot. In the MID framework, the variation in ICE curves for a feature \\(j\\) is explicitly governed by the interaction terms \\(g_{jk}\\) identified from the LightGBM model.\n\n\nCode\nice_lgb_link &lt;- mid.conditional(mid_lgb, type = \"link\", variable = \"DrivAge\")\nice_lgb &lt;- mid.conditional(mid_lgb, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_lgb_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_lgb, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAdditive Attribution\nFinally, we perform an Additive Breakdown for individual predictions. This provides an exact allocation of the LightGBM’s prediction into the terms of our surrogate model.\n\n\nCode\nset.seed(42)\nrow_ids &lt;- sort(sample(nrow(train), 4))\n\nbd_plots &lt;- lapply(\n  row_ids,\n  function(idx) {\n    res &lt;- mid.breakdown(mid_lgb, train, row = idx)\n    label &lt;- paste0(\"Breakdown for Row \", idx)\n    ggmid(res, theme = \"shap\", max.nterms = 10,\n          format.args = list(digits = 4)) +\n      labs(x = \"Linear Predictor\", subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\n\ngrid.arrange(grobs = bd_plots)"
  },
  {
    "objectID": "notebooks/demo_r.html#conclusion",
    "href": "notebooks/demo_r.html#conclusion",
    "title": "Surrogate Modeling with MID in R",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, we have demonstrated how Maximum Interpretation Decomposition (MID) bridges the gap between predictive performance and model transparency. By using the {midr} package, we successfully transformed a complex LightGBM model into a structured, additive representation.\nWhile the surrogate model fidelity may not always be perfect, the crucial advantage lies in our ability to quantify its limitations. Through the uninterpreted variation ratio, we can directly assess the complexity of the black-box model. If the fidelity is lower than expected, it serves as a diagnostic signal that the original model relies on high-order interactions or structural complexities that extend beyond second-order effects.\nKnowing the extent of this “unexplained” variance is far more valuable than operating in the dark. It allows actuaries to make informed decisions about whether the additional complexity of a black-box model is justified by its performance, or if a more transparent structure is preferable for regulatory and risk management purposes.\nAs machine learning models become increasingly prevalent in insurance pricing and reserving, tools like MID will be essential for ensuring that our “black-boxes” remain accountable, reliable, and fundamentally understood."
  },
  {
    "objectID": "notebooks/dataset.html",
    "href": "notebooks/dataset.html",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#overview",
    "href": "notebooks/dataset.html#overview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#data-preprocessing",
    "href": "notebooks/dataset.html#data-preprocessing",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThe following transformations are applied to handle outliers, improve model stability, and align with actuarial pricing practices:\n\nTarget Variable: We define Frequency as the number of claims per unit of exposure. \\[\\text{Frequency} = \\frac{\\text{ClaimNb}}{\\text{Exposure}} \\]\nCapping: VehAge (vehicle age) is capped at 25 years to mitigate the influence of extreme outliers (vintage cars) which follow different risk profiles.\nLog-Transformation: Density (population density) is log-transformed to stabilize the variance and simplify its relationship with claim frequency.\nCategorical Lumping: For VehBrand and Region, infrequent levels are collapsed into an “Other” category. This prevents overfitting in low-exposure segments.\n\n\n\nCode\n# load dataset from CASdatasets\ndata(freMTPL2freq, package = \"CASdatasets\")\n\n# preprocess dataset\ndf_all &lt;- freMTPL2freq |&gt;\n  dplyr::mutate(\n    Frequency  = ClaimNb / Exposure,\n    VehAge = pmin(VehAge, 25),\n    VehBrand = forcats::fct_lump(VehBrand, 5),\n    LogDensity = log(Density),\n    Region = forcats::fct_lump(Region, 6)\n  ) |&gt;\n  dplyr::select(\n    Frequency, Exposure, VehPower, VehAge,\n    DrivAge, VehBrand, VehGas, LogDensity, Region\n  ) |&gt;\n  dplyr::as_tibble()"
  },
  {
    "objectID": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "href": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Split for Hold-Out Validation",
    "text": "Data Split for Hold-Out Validation\nTo ensure a robust evaluation of our models, we split the dataset into training and testing sets.\nWe store these datasets in Parquet format. Unlike standard CSV files, Parquet preserves schema information (e.g., categorical types) and provides high-performance I/O.\nBy locking the data into a binary format after the initial split, we guarantee that all subsequent modeling steps across different environments, R and Python, remain consistent.\n\n\nCode\n# split dataset\nset.seed(42)\ndf_split &lt;- rsample::initial_split(df_all, prop = 1/2)\ndf_train &lt;- rsample::training(df_split)\ndf_test  &lt;- rsample::testing(df_split)\n\n# write dataset as parquets\narrow::write_parquet(df_train, \"../data/train.parquet\")\narrow::write_parquet(df_test, \"../data/test.parquet\")"
  },
  {
    "objectID": "notebooks/dataset.html#data-preview",
    "href": "notebooks/dataset.html#data-preview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preview",
    "text": "Data Preview\nBelow is a summary of the first 1000 rows of the processed training dataset. The Exposure column will be utilized as an offset in our subsequent modeling to account for varying policy durations."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Overview",
    "text": "Overview\nWelcome to the supplementary repository for our presentation Maximizing the Interpretation of Black-Box Models at CONVENTION A | ASIA.\nThis project demonstrates the application of Maximum Interpretation Decomposition (MID), a novel approach in Interpretable Machine Learning (IML) designed to bridge the gap between high-performance “black-box” models and the transparency requirements of actuarial practice.\n\nKey Topics of the Presentation\n\nTheoretical Foundation: Understanding the decomposition logic of complex models into interpretable parts: intercept, first-order main effects, and second-order interaction effects by MID.\nActuarial Application: Benchmarking MID using the industry-standard freMTPL2freq dataset using open-source software: {midr}, {midnight} (R), and {midlearn} (Python)."
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Project Structure",
    "text": "Project Structure\nThis Quarto website contains the following sections:\n\nDataset: Data cleaning and feature engineering of the French Motor Third-Party Liability dataset.\nR Demo: A demonstration of the {midr} and {midlearn} packages in R.\nPython Demo: A demonstration of the {midlearn} library, providing a seamless interface for Python users."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Software",
    "text": "Software\n\n{midr}: GitHub / CRAN\nR package for Maximum Interpretation Decomposition in R\n{midnight}: GitHub\nR package to integrate {midr} to the {tidymodels} ecosystem\n{midlearn}: GitHub / PyPI\nPython library to integrate {midr} to the {scikit-learn} ecosystem"
  },
  {
    "objectID": "index.html#acknowledgment-reference",
    "href": "index.html#acknowledgment-reference",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Acknowledgment & Reference",
    "text": "Acknowledgment & Reference\n\n{CASdatasets}: CRAN\nThis demonstration utilizes the freMTPL2freq dataset from the {CASdatasets} package.\nAITools4Actuaries: Website\nWe would like to acknowledge the project for their foundational work on benchmarking ML models in insurance, which served as a reference, especially for our data preprocessing pipeline."
  },
  {
    "objectID": "notebooks/demo_py.html",
    "href": "notebooks/demo_py.html",
    "title": "Surrogate Modeling with MID in Python",
    "section": "",
    "text": "In modern actuarial science, there is an inherent tension between predictive accuracy and model transparency. While ensemble tree-based models like Gradient Boosting Machines (GBMs) frequently outperform traditional Generalized Linear Models (GLMs), their “black-box” nature presents significant hurdles for model governance, regulatory compliance, and price filing.\nThis notebook demonstrates a solution using Maximum Interpretation Decomposition (MID) via the {midlearn} library in Python.\n\n\n\n\n\n\nCompatibility Notice\n\n\n\nThis article relies on features introduced in midlearn (&gt;= 0.1.3) and the underlying R package midr (&gt;= 0.5.3). Please ensure your library and package are up to date. Some arguments are not available in earlier versions.\n\n\n\n\nMID is a functional decomposition framework that acts as a high-fidelity surrogate for complex models. It deconstructs a black-box prediction function \\(f(\\mathbf{X})\\) into several interpretable components: intercept \\(g_\\emptyset\\), main effects \\(g_j(X_j)\\), and interaction effects \\(g_{jk}(X_j, X_k)\\). The prediction is represented as the following additive structure:\n\\[\nf(\\mathbf{X}) = g_\\emptyset + \\sum_{j} g_j(X_{j}) + \\sum_{j &lt; k} g_{jk}(X_{j},\\;X_{k}) + \\dots + g_D(\\mathbf{X})\n\\]\nTo ensure the uniqueness and interpretability of each component, MID imposes the centering constraints:\n\\[\n\\begin{aligned}\n\\mathbf{E}\\left[g_j(X_j)\\right] &= 0 \\\\\n\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_j = x_j\\right] &= 0 \\quad (\\forall x_j) \\\\\n\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_k = x_k\\right] &= 0 \\quad (\\forall x_k)\n\\end{aligned}\n\\]\nBy replicating a black-box model with this structured approach, we can quantify the “uninterpreted” variance (captured by \\(g_D(\\mathbf{X})\\)) and derive a representation that captures the superior predictive power of machine learning without sacrificing actuarial clarity.\n\n\n\nWe begin by setting up the environment and loading the necessary libraries.\n\n\nCode\n# utility\nfrom pathlib import Path\n\n# data manipulation\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# predictive modeling\nimport sklearn.linear_model as lm\nimport lightgbm as lgb\n\n# import loss function\nfrom sklearn.metrics import mean_poisson_deviance, r2_score\n\n# surrogate modeling\nimport midlearn as mid\n\n# visualization\nfrom plotnine import *\n\n# load training and testing datasets\nPATH = Path(\"../data\")\ntrain = pd.read_parquet(PATH / \"train.parquet\")\ntest  = pd.read_parquet(PATH / \"test.parquet\")\n\n\nError importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\nTrying to import in ABI mode.\n\n\nIn Python workflows, we typically partition the dataset into feature matrix \\(X\\), target vector \\(y\\), and weight vector \\(w\\).\n\n\nCode\n# training set\nX_train = train.drop(['Frequency', 'Exposure'], axis=1)\ny_train = train['Frequency']\nw_train = train['Exposure']\n# testing set\nX_test  = test.drop(['Frequency', 'Exposure'], axis=1)\ny_test  = test['Frequency']\nw_test  = test['Exposure']\n\n\nA key component of our evaluation is the Weighted Mean Poisson Deviance defined as follows:\n\\[\nL(\\mathbf{y}, \\hat{\\mathbf{y}}, \\mathbf{w}) = \\frac{2 \\sum_{i=1}^n w_i \\left( y_i \\log(y_i/\\hat{y}_i) - (y_i - \\hat{y}_i) \\right)}{\\sum_{i=1}^n w_i}\n\\]"
  },
  {
    "objectID": "notebooks/demo_py.html#introduction",
    "href": "notebooks/demo_py.html#introduction",
    "title": "Surrogate Modeling with MID in Python",
    "section": "",
    "text": "In modern actuarial science, there is an inherent tension between predictive accuracy and model transparency. While ensemble tree-based models like Gradient Boosting Machines (GBMs) frequently outperform traditional Generalized Linear Models (GLMs), their “black-box” nature presents significant hurdles for model governance, regulatory compliance, and price filing.\nThis notebook demonstrates a solution using Maximum Interpretation Decomposition (MID) via the {midlearn} library in Python.\n\n\n\n\n\n\nCompatibility Notice\n\n\n\nThis article relies on features introduced in midlearn (&gt;= 0.1.3) and the underlying R package midr (&gt;= 0.5.3). Please ensure your library and package are up to date. Some arguments are not available in earlier versions.\n\n\n\n\nMID is a functional decomposition framework that acts as a high-fidelity surrogate for complex models. It deconstructs a black-box prediction function \\(f(\\mathbf{X})\\) into several interpretable components: intercept \\(g_\\emptyset\\), main effects \\(g_j(X_j)\\), and interaction effects \\(g_{jk}(X_j, X_k)\\). The prediction is represented as the following additive structure:\n\\[\nf(\\mathbf{X}) = g_\\emptyset + \\sum_{j} g_j(X_{j}) + \\sum_{j &lt; k} g_{jk}(X_{j},\\;X_{k}) + \\dots + g_D(\\mathbf{X})\n\\]\nTo ensure the uniqueness and interpretability of each component, MID imposes the centering constraints:\n\\[\n\\begin{aligned}\n\\mathbf{E}\\left[g_j(X_j)\\right] &= 0 \\\\\n\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_j = x_j\\right] &= 0 \\quad (\\forall x_j) \\\\\n\\mathbf{E}\\left[g_{jk}(X_j, X_k) \\mid X_k = x_k\\right] &= 0 \\quad (\\forall x_k)\n\\end{aligned}\n\\]\nBy replicating a black-box model with this structured approach, we can quantify the “uninterpreted” variance (captured by \\(g_D(\\mathbf{X})\\)) and derive a representation that captures the superior predictive power of machine learning without sacrificing actuarial clarity.\n\n\n\nWe begin by setting up the environment and loading the necessary libraries.\n\n\nCode\n# utility\nfrom pathlib import Path\n\n# data manipulation\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# predictive modeling\nimport sklearn.linear_model as lm\nimport lightgbm as lgb\n\n# import loss function\nfrom sklearn.metrics import mean_poisson_deviance, r2_score\n\n# surrogate modeling\nimport midlearn as mid\n\n# visualization\nfrom plotnine import *\n\n# load training and testing datasets\nPATH = Path(\"../data\")\ntrain = pd.read_parquet(PATH / \"train.parquet\")\ntest  = pd.read_parquet(PATH / \"test.parquet\")\n\n\nError importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\nTrying to import in ABI mode.\n\n\nIn Python workflows, we typically partition the dataset into feature matrix \\(X\\), target vector \\(y\\), and weight vector \\(w\\).\n\n\nCode\n# training set\nX_train = train.drop(['Frequency', 'Exposure'], axis=1)\ny_train = train['Frequency']\nw_train = train['Exposure']\n# testing set\nX_test  = test.drop(['Frequency', 'Exposure'], axis=1)\ny_test  = test['Frequency']\nw_test  = test['Exposure']\n\n\nA key component of our evaluation is the Weighted Mean Poisson Deviance defined as follows:\n\\[\nL(\\mathbf{y}, \\hat{\\mathbf{y}}, \\mathbf{w}) = \\frac{2 \\sum_{i=1}^n w_i \\left( y_i \\log(y_i/\\hat{y}_i) - (y_i - \\hat{y}_i) \\right)}{\\sum_{i=1}^n w_i}\n\\]"
  },
  {
    "objectID": "notebooks/demo_py.html#the-interpretable-baseline-gam",
    "href": "notebooks/demo_py.html#the-interpretable-baseline-gam",
    "title": "Surrogate Modeling with MID in Python",
    "section": "The Interpretable Baseline: GAM",
    "text": "The Interpretable Baseline: GAM\nWe first fit a GAM to establish a transparent benchmark. Since GAMs are additive by design, they provide a “ground truth” model structure to be recovered by the functional decomposition.\n\n\nCode\n# define variable encoder for PoissonRegressor\ndef one_hot_encode(X):\n    cats = X.select_dtypes(include=['object', 'category']).columns.tolist()\n    nums = X.select_dtypes(exclude=['object', 'category']).columns.tolist()\n    ct = ColumnTransformer(\n        transformers=[\n            ('cat', OneHotEncoder(drop='first', sparse_output=False), cats),\n            ('num', 'passthrough', nums)\n        ],\n        verbose_feature_names_out=False\n    )\n    ct.set_output(transform=\"pandas\")\n    return ct.fit_transform(X)\n\n# build poisson regressor\nfit_glm = lm.PoissonRegressor(alpha=0, max_iter=300)\nfit_glm.fit(one_hot_encode(X_train), y_train, sample_weight=w_train)\n\n\nPoissonRegressor(alpha=0, max_iter=300)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PoissonRegressor?Documentation for PoissonRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nalpha \n0\n\n\n\nfit_intercept \nTrue\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n300\n\n\n\ntol \n0.0001\n\n\n\nwarm_start \nFalse\n\n\n\nverbose \n0\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\n# evaluate fitted model\ny_pred = fit_glm.predict(one_hot_encode(X_test))\ndeviance = mean_poisson_deviance(\n  y_true=y_test,\n  y_pred=y_pred,\n  sample_weight=w_test\n)\nprint(\"Mean Poisson Deviance:\", round(deviance, 6))\n\n\nMean Poisson Deviance: 0.4548\n\n\n\nSurrogate Modeling\nWe apply the interpret() function to the GAM. This step serves as a sanity check: if MID is effective, it should perfectly replicate the predictive behavior of the original GAM.\n\n\nCode\nmid_glm = mid.MIDExplainer(\n  estimator = fit_glm,\n  link = \"log\"\n)\nmid_glm.fit(\n  X_train,\n  y = fit_glm.predict(one_hot_encode(X_train))\n)\n\n\nMIDExplainer(estimator=PoissonRegressor(alpha=0, max_iter=300), link='log')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MIDExplaineriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nPoissonRegres... max_iter=300)\n\n\n\ntarget_classes \nNone\n\n\n\nparams_main \nNone\n\n\n\nparams_inter \nNone\n\n\n\npenalty \n0\n\n\n\nlink \n'log'\n\n\n\nkernel_type \n1\n\n\n\nencoding_frames \n{}\n\n\n\nmodel_terms \nNone\n\n\n\nsingular_ok \nFalse\n\n\n\nmode \n1\n\n\n\nmethod \nNone\n\n\n\ncentering_penalty \n1000000.0\n\n\n\nna_action \n'na.omit'\n\n\n\nverbosity \n1\n\n\n\nencoding_digits \n3\n\n\n\nuse_catchall \nFalse\n\n\n\ncatchall \n'(others)'\n\n\n\nmax_nelements \n1000000000.0\n\n\n\nnil \n1e-07\n\n\n\ntol \n1e-07\n\n\n\n\n            \n        \n    estimator: PoissonRegressorPoissonRegressor(alpha=0, max_iter=300)PoissonRegressor?Documentation for PoissonRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nalpha \n0\n\n\n\nfit_intercept \nTrue\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n300\n\n\n\ntol \n0.0001\n\n\n\nwarm_start \nFalse\n\n\n\nverbose \n0\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\n# evaluate fitted surrogate\nprint(\n    \"Uninterpreted Variation Ratio:\",\n    round(mid_glm.ratio['working'], 6)\n)\ny_pred = mid_glm.predict(X_test)\ndeviance = mean_poisson_deviance(\n  y_true=y_test,\n  y_pred=y_pred,\n  sample_weight=w_test\n)\nprint(\"Mean Poisson Deviance:\", round(deviance, 6))\n\n\nUninterpreted Variation Ratio: 0.0\nMean Poisson Deviance: 0.4548\n\n\n\n\nModel Fidelity\nTo assess model fidelity, i.e., how closely the surrogate replicates the black-box, we calculate the uninterpreted variation ratio \\(\\mathbf{U}\\).\n\\[\n\\mathbf{U}(f,g) = \\frac{\\sum_{i=1}^n (f(x_i) - g(x_i))^2}{\\sum_{i=1}^n (f(x_i) - \\bar{f})^2}, \\quad \\text{where } \\bar{f} = \\frac{1}{n}\\sum_{i=1}^n f(x_i)\n\\]\nThis metric represents the proportion of the black-box model’s variance that is not captured by the additive components of the MID model. The R-squared score, \\(\\mathbf{R}^2(f,g) = 1 - \\mathbf{U}(f,g)\\), is a standard measure for this purpose. It is important to note that this \\(\\mathbf{R}^2\\) measures the fidelity to the black-box model, not the predictive accuracy relative to the ground truth observations.\nIn the {midr} package, the summary output includes this ratio calculated on the training set. For models with non-linear links (e.g., Poisson regression), the “working” ratio is computed on the scale of the link function (e.g., \\(\\log\\) scale).\nTo rigorously confirm the model fidelity, it is recommended to evaluate these metrics on a separate testing set. This ensures that the surrogate model is not just over-fitting the training predictions but has truly captured the underlying functional structure.\n\n\nCode\n# calculate R-squared on testing dataset\nr2_mid = mid_glm.fidelity_score(\n    X=X_test,\n    y=fit_glm.predict(one_hot_encode(X_test)),\n    sample_weight=w_test\n)\nprint(\"R squared:\", round(r2_mid, 10))\n\n\nR squared: 0.9999999986\n\n\nAs shown by the high \\(\\mathbf{R}^2\\) score, the MID surrogate achieves near-perfect fidelity. This level of agreement justifies using the MID components (main effects and interactions) as a reliable lens through which to interpret the original black-box model’s behavior.\n\n\nFeature Effects\nVisualizing the functional behavior of each component allows for a direct comparison between the MID surrogate’s decomposition and the original GAM’s structure.\n\nMID Surrogate\n\n\n\nCode\n# main effects of MID surrogate\nplots = []\nfor feature in X_train.columns:\n    p = (\n        mid_glm.plot(feature) +\n        lims(y=[-0.8, 0.8]) +\n        labs(y=\"Main Effect\")\n    )\n    plots.append(p)\n\ndisplay(\n    (plots[0] | plots[1] | plots[2] | plots[5]) /\n    (plots[3] | plots[4] | plots[6])\n)\n\n\n\n\n\n\n\n\n\n\nOriginal GAM\n\n\n\nCode\n# feature effects of GAM\n\n\nFurthermore, we can visualize the joint effects of feature pairs as 3D prediction surfaces using the S3 method for the persp() function.\n\n\nEffect Importance\nBeyond simple plots for feature effects, {midr} provides a suite of diagnostic tools. First, the Effect Importance of a term \\(j\\) is defined as the mean absolute contribution of that term across the population:\n\\[\n\\text{Importance}_j = \\mathbf{E} \\left[ | g_j(X_j) | \\right] \\approx \\frac{1}{n} \\sum_{i=1}^n | g_j(x_{ij}) |\n\\]\n\n\nCode\nimp_glm = mid_glm.importance(max_nsamples = 2000)\np1 = (\n    imp_glm.plot(fill = \"steelblue\") +\n    labs(title = \"Mean Absolute Effect\",\n        subtitle = \"Bar Plot\")\n)\np2 = (\n    imp_glm.plot(style = \"sinaplot\", theme = \"mako@div\") +\n    labs(subtitle = \"Distribution of Effects\") +\n    theme(legend_position = \"none\",\n          axis_text_y = element_blank())\n)\ndisplay(p1 | p2)\n\n\n\n\n\n\n\n\n\nFor interaction terms, the importance is similarly calculated using \\(g_{jk}(X_j, X_k)\\). This metric allows us to rank features by their average influence on the model’s predictions.\n\n\nConditional Expectation\nSecond, we can explore Individual Conditional Expectations (ICE). In the MID framework, the ICE for a feature \\(j\\) and a specific observation \\(i\\) is the expected value of the prediction as \\(X_j\\) varies, while keeping other features fixed at their observed values \\(\\mathbf{x}_{\\setminus j}^{(i)}\\):\n\\[\n\\text{ICE}_j^{(i)}(x) = g_\\emptyset + g_j(x) + \\sum_{k \\neq j} g_{jk}(x, x_{ik})\n\\]\n\n\nCode\nice_glm_link = mid_glm.conditional(\n    type=\"link\", variable=\"DrivAge\", data=X_train.sample(200)\n)\nice_glm = mid_glm.conditional(\n    variable=\"DrivAge\", data=X_train.sample(200)\n)\np1 = (\n    ice_glm_link.plot(var_color=\"LogDensity\", theme=\"bluescale\") +\n    theme(legend_position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\")\n    )\np2 = (\n    ice_glm.plot(var_color=\"LogDensity\", theme=\"bluescale\") +\n    theme(legend_position = \"bottom\") +\n    labs(y = \"Linear Predictor\", title = \"\",\n         subtitle = \"Change in original scale\")\n    )\ndisplay(p1 | p2)\n\n\n\n\n\n\n\n\n\nUnlike standard black-box models, MID’s low-order structure allows us to compute these expectations efficiently and interpret the variation across curves (the “thickness” of the ICE plot) as a direct consequence of specified interaction terms \\(g_{jk}\\).\n\n\nAdditive Attribution\nThird, we perform instance-level explanation through an Additive Breakdown of the prediction. For any single observation \\(\\mathbf{x}\\), the MID surrogate’s prediction \\(g(\\mathbf{x})\\) is decomposed into the exact sum of its functional components:\n\\[\ng(\\mathbf{x}) = \\underbrace{g_\\emptyset}_{\\text{Intercept}} + \\underbrace{\\sum_{j} g_j(x_j)}_{\\text{Main Effects}} + \\underbrace{\\sum_{j &lt; k} g_{jk}(x_j, x_k)}_{\\text{Interactions}}\n\\]\n\n\nCode\nnp.random.seed(42)\nrow_ids = sorted(\n  np.random.randint(1, train.shape[0], 4).tolist()\n)\nbd_plots = []\nfor i in row_ids:\n    bd = mid_glm.breakdown(row = i - 1)\n    label = \"Breakdown of Row \" + str(i)\n    p = (\n        bd.plot(theme = \"shap\", format_args = {'digits': 2}) +\n        labs(x = \"\", subtitle = label) +\n        theme(legend_position = \"none\")\n    )\n    bd_plots.append(p)\n\ndisplay(\n    (bd_plots[0] | bd_plots[1]) /\n    (bd_plots[2] | bd_plots[3]) \n)\n\n\n\n\n\n\n\n\n\nBy visualizing these contributions in a waterfall plot, we can identify which specific risk factors or interaction effects drove the prediction for a particular instance, such as a high-risk policyholder."
  },
  {
    "objectID": "notebooks/demo_py.html#the-black-box-lightgbm",
    "href": "notebooks/demo_py.html#the-black-box-lightgbm",
    "title": "Surrogate Modeling with MID in Python",
    "section": "The Black-Box: LightGBM",
    "text": "The Black-Box: LightGBM\nWhile GLMs are transparent, GBMs such as LightGBM often yield superior predictive power by capturing high-order interactions. However, this accuracy comes at the cost of being a black box.\n\n\nCode\n# data preprocessing for LightGBM\ndef str_to_cat(X):\n  cats = X.select_dtypes(include=['object', 'category']).columns.tolist()\n  X = X.copy()\n  X[cats] = X[cats].astype('category')\n  return X\n\n# model parameters (mirroring the R version)\nparams_lgb = {\n  'objective': \"poisson\",\n  'n_estimators': 551,\n  'learning_rate': 0.01672663973358928,\n  'num_leaves': 61,\n  'max_depth': 19,\n  'min_child_samples': 10,\n  'subsample': 0.8123000876841823,\n  'colsample_bytree': 0.6507848978461632,\n  'reg_alpha': 4.234603347091384,\n  'reg_lambda': 8.790496879009705e-07,\n  'random_state': 42,\n  'n_jobs': -1,\n  'verbosity': -1,\n  'importance_type': 'gain'\n}\n\n# split datasets for validation\nvalid_n = int(X_train.shape[0] * 0.2)\ntrain_X, valid_X = X_train.iloc[valid_n:], X_train.iloc[:valid_n]\ntrain_y, valid_y = y_train[valid_n:], y_train[:valid_n]\ntrain_w, valid_w = w_train[valid_n:], w_train[:valid_n]\n\n# initialize and train a LightGBM\nfit_lgb = lgb.LGBMRegressor(**params_lgb)\n\nfit_lgb.fit(\n  X=str_to_cat(train_X),\n  y=train_y,\n  sample_weight=train_w,\n  eval_set=[(valid_X, valid_y)],\n  eval_sample_weight=[valid_w],\n  callbacks=[lgb.early_stopping(stopping_rounds=50)]\n)\n\n\nTraining until validation scores don't improve for 50 rounds\nEarly stopping, best iteration is:\n[458]   valid_0's poisson: 0.263843\n\n\nLGBMRegressor(colsample_bytree=0.6507848978461632, importance_type='gain',\n              learning_rate=0.01672663973358928, max_depth=19,\n              min_child_samples=10, n_estimators=551, n_jobs=-1, num_leaves=61,\n              objective='poisson', random_state=42, reg_alpha=4.234603347091384,\n              reg_lambda=8.790496879009705e-07, subsample=0.8123000876841823,\n              verbosity=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nboosting_type \n'gbdt'\n\n\n\nnum_leaves \n61\n\n\n\nmax_depth \n19\n\n\n\nlearning_rate \n0.01672663973358928\n\n\n\nn_estimators \n551\n\n\n\nsubsample_for_bin \n200000\n\n\n\nobjective \n'poisson'\n\n\n\nclass_weight \nNone\n\n\n\nmin_split_gain \n0.0\n\n\n\nmin_child_weight \n0.001\n\n\n\nmin_child_samples \n10\n\n\n\nsubsample \n0.8123000876841823\n\n\n\nsubsample_freq \n0\n\n\n\ncolsample_bytree \n0.6507848978461632\n\n\n\nreg_alpha \n4.234603347091384\n\n\n\nreg_lambda \n8.790496879009705e-07\n\n\n\nrandom_state \n42\n\n\n\nn_jobs \n-1\n\n\n\nimportance_type \n'gain'\n\n\n\nverbosity \n-1\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\n# evaluate fitted model\ny_pred_fit_lgb = fit_lgb.predict(str_to_cat(X_test))\n\ndeviance = mean_poisson_deviance(\n  y_true=y_test,\n  y_pred=y_pred_fit_lgb,\n  sample_weight=w_test\n)\nprint(\"Mean Poisson Deviance:\", round(deviance, 6))\n\n\nMean Poisson Deviance: 0.465148\n\n\n\nSurrogate Modeling\nWe use {midr} to replicate the LightGBM model. By including interaction terms in the model formula, we allow the surrogate to capture the joint relationships that the GBM has learned. The goal is to approximate the LightGBM function \\(f_{LGB}(\\mathbf{x})\\) with our interpretable structure \\(g(\\mathbf{x})\\):\n\\[\nf_{LGB}(\\mathbf{x}) \\approx g(\\mathbf{x}) = g_\\emptyset + \\sum_{j} g_j(x_j) + \\sum_{j &lt; k} g_{jk}(x_j, x_k)\n\\]\n\n\n\n\n\n\nComputational Considerations\n\n\n\nIncluding all second-order interactions using the (...)^2 syntax results in \\(p(p-1)/2\\) interaction terms. For high-dimensional data, this can be memory-intensive. Users should ensure sufficient RAM is available or consider limiting the formula to the most relevant features, or using a subset of the training set.\n\n\n\n\nCode\n# build a surrogate model\nmid_lgb = mid.MIDExplainer(\n  estimator=fit_lgb,\n  interactions=True,\n  link=\"log\",\n  penalty=0.01\n)\n\nmid_lgb.fit(\n  X_train, \n  y=fit_lgb.predict(X_train),\n  sample_weight=w_train\n)\n\n\n\n\nMIDExplainer(estimator=LGBMRegressor(colsample_bytree=0.6507848978461632,\n                                     importance_type='gain',\n                                     learning_rate=0.01672663973358928,\n                                     max_depth=19, min_child_samples=10,\n                                     n_estimators=551, n_jobs=-1, num_leaves=61,\n                                     objective='poisson', random_state=42,\n                                     reg_alpha=4.234603347091384,\n                                     reg_lambda=8.790496879009705e-07,\n                                     subsample=0.8123000876841823,\n                                     verbosity=-1),\n             link='log', penalty=0.01)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MIDExplaineriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nLGBMRegressor... verbosity=-1)\n\n\n\ntarget_classes \nNone\n\n\n\nparams_main \nNone\n\n\n\nparams_inter \nNone\n\n\n\npenalty \n0.01\n\n\n\nlink \n'log'\n\n\n\nkernel_type \n1\n\n\n\nencoding_frames \n{}\n\n\n\nmodel_terms \nNone\n\n\n\nsingular_ok \nFalse\n\n\n\nmode \n1\n\n\n\nmethod \nNone\n\n\n\ncentering_penalty \n1000000.0\n\n\n\nna_action \n'na.omit'\n\n\n\nverbosity \n1\n\n\n\nencoding_digits \n3\n\n\n\nuse_catchall \nFalse\n\n\n\ncatchall \n'(others)'\n\n\n\nmax_nelements \n1000000000.0\n\n\n\nnil \n1e-07\n\n\n\ntol \n1e-07\n\n\n\n\n            \n        \n    estimator: LGBMRegressorLGBMRegressor(colsample_bytree=0.6507848978461632, importance_type='gain',\n              learning_rate=0.01672663973358928, max_depth=19,\n              min_child_samples=10, n_estimators=551, n_jobs=-1, num_leaves=61,\n              objective='poisson', random_state=42, reg_alpha=4.234603347091384,\n              reg_lambda=8.790496879009705e-07, subsample=0.8123000876841823,\n              verbosity=-1)LGBMRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nboosting_type \n'gbdt'\n\n\n\nnum_leaves \n61\n\n\n\nmax_depth \n19\n\n\n\nlearning_rate \n0.01672663973358928\n\n\n\nn_estimators \n551\n\n\n\nsubsample_for_bin \n200000\n\n\n\nobjective \n'poisson'\n\n\n\nclass_weight \nNone\n\n\n\nmin_split_gain \n0.0\n\n\n\nmin_child_weight \n0.001\n\n\n\nmin_child_samples \n10\n\n\n\nsubsample \n0.8123000876841823\n\n\n\nsubsample_freq \n0\n\n\n\ncolsample_bytree \n0.6507848978461632\n\n\n\nreg_alpha \n4.234603347091384\n\n\n\nreg_lambda \n8.790496879009705e-07\n\n\n\nrandom_state \n42\n\n\n\nn_jobs \n-1\n\n\n\nimportance_type \n'gain'\n\n\n\nverbosity \n-1\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\n# evaluate fitted surrogate\nprint(\n  \"Uninterpreted Variation Ratio:\",\n  round(mid_lgb.ratio['working'], 6)\n)\n\ny_pred_mid_lgb = mid_lgb.predict(X_test)\n\ndeviance = mean_poisson_deviance(\n  y_true=y_test,\n  y_pred=y_pred_mid_lgb,\n  sample_weight=w_test\n)\nprint(\"Mean Poisson Deviance:\", round(deviance, 6))\n\n\nUninterpreted Variation Ratio: 0.09313\nMean Poisson Deviance: 0.467089\n\n\n\n\nModel Fidelity\nTo measure how successfully our surrogate replicates the LightGBM model, we use the R-squared score on the link scale (\\(\\log\\) scale).\n\n\nCode\n# calculate R-squared on testing dataset\nr2_mid = r2_score(\n  y_true=np.log(fit_lgb.predict(str_to_cat(X_test))),\n  y_pred=np.log(mid_lgb.predict(X_test)),\n  sample_weight=w_test\n)\nprint(\"R squared:\", round(r2_mid, 10))\n\n\nR squared: 0.9050963659\n\n\n\n\nFeature Effects\n\n\nCode\n# main effects of MID surrogate\nplots = []\nfor feature in X_train.columns:\n  p = (\n  mid_lgb.plot(feature) +\n    lims(y=[-1.0, 1.0]) +\n    labs(y=\"Main Effect\")\n  )\n  plots.append(p)\n\ndisplay(\n  (plots[0] | plots[1] | plots[2] | plots[5]) /\n  (plots[3] | plots[4] | plots[6])\n)\n\n\n\n\n\n\n\n\n\nA key advantage of {midr} is its ability to isolate interaction effects \\(g_{jk}\\) from main effects \\(g_j\\). This is particularly useful to understand the joint impact of two variables (e.g., Region and LogDensity).\n\n\nCode\np1 = (\n  mid_lgb.plot(\"LogDensity:Region\", style=\"data\",\n               data=train.sample(2000), theme=\"bluescale\") +\n  labs(subtitle=\"Interaction Effect\", y=\"\") +\n  theme(legend_position=\"bottom\")\n)\np2 = (\n  mid_lgb.plot(\"LogDensity:Region\", main_effects=True) +\n  labs(subtitle=\"Total Effect\", y=\"\") +\n  theme(axis_text_y=element_blank(),\n        legend_position=\"bottom\")\n)\ndisplay(p1 | p2)\n\n\n\n\n\n\n\n\n\n\n\nEffect Importance\nTo rank the influence of each component discovered in the LightGBM model, we calculate the Effect Importance, defined as the average absolute contribution.\n\n\nCode\nimp_lgb = mid_lgb.importance(max_nsamples=2000)\n\np1 = (\n  imp_lgb.plot(theme=\"bluescale@qual\", max_nterms=20) +\n  labs(title=\"Effect Importance\", subtitle=\"Average absolute effect\") +\n  theme(legend_position=\"none\")\n)\np2 = (\n  imp_lgb.plot(style=\"sinaplot\", theme=\"mako@div\", max_nterms=20) +\n  labs(title=\"\", subtitle=\"Distribution of effects\") +\n  theme(axis_text_y=element_blank(), legend_position=\"none\")\n)\ndisplay(p1 | p2)\n\n\n\n\n\n\n\n\n\n\n\nConditional Expectation\nWe further explore the model’s behavior using the ICE plot. In the MID framework, the variation in ICE curves for a feature \\(j\\) is explicitly governed by the interaction terms \\(g_{jk}\\) identified from the LightGBM model.\n\n\nCode\nice_lgb_link = mid_lgb.conditional(\n  type=\"link\", variable=\"DrivAge\", data=X_train.sample(200)\n)\nice_lgb = mid_lgb.conditional(\n  variable=\"DrivAge\", data=X_train.sample(200)\n)\n\np1 = (\n  ice_lgb_link.plot(theme=\"bluescale\", var_color=\"LogDensity\") +\n  theme(legend_position=\"bottom\") +\n  labs(y=\"Linear Predictor\",\n       title=\"Conditional Expectation\",\n       subtitle=\"Change in linear predictor\")\n  )\np2 = (\n  ice_lgb.plot(style=\"centered\", theme=\"bluescale\",\n               var_color=\"LogDensity\") +\n  theme(legend_position = \"bottom\") +\n  labs(y = \"Prediction\", title = \"\",\n     subtitle = \"Change in original scale\")\n  )\ndisplay(p1 | p2)\n\n\n\n\n\n\n\n\n\n\n\nAdditive Attribution\nFinally, we perform an Additive Breakdown for individual predictions. This provides an exact allocation of the LightGBM’s prediction into the terms of our surrogate model.\n\n\nCode\nnp.random.seed(42)\nrow_ids = sorted(\n  np.random.randint(0, train.shape[0], 4).tolist()\n)\nbd_plots = []\nfor idx in row_ids:\n  bd = mid_lgb.breakdown(row=idx)\n  label = \"Breakdown for Row \" + str(idx)\n  p = (\n    bd.plot(theme=\"shap\", format_args={'digits': 4},\n            max_nterms=10) +\n    labs(x=\"\", subtitle=label) +\n    theme(legend_position=\"none\")\n  )\n  bd_plots.append(p)\n\ndisplay(\n  (bd_plots[0] | bd_plots[1]) /\n  (bd_plots[2] | bd_plots[3]) \n)"
  },
  {
    "objectID": "notebooks/demo_py.html#conclusion",
    "href": "notebooks/demo_py.html#conclusion",
    "title": "Surrogate Modeling with MID in Python",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, we have demonstrated how Maximum Interpretation Decomposition (MID) bridges the gap between predictive performance and model transparency. By using the {midlearn} library, we successfully transformed a complex LightGBM model into a structured, additive representation.\nWhile the surrogate model fidelity may not always be perfect, the crucial advantage lies in our ability to quantify its limitations. Through the uninterpreted variation ratio, we can directly assess the complexity of the black-box model. If the fidelity is lower than expected, it serves as a diagnostic signal that the original model relies on high-order interactions or structural complexities that extend beyond second-order effects.\nKnowing the extent of this “unexplained” variance is far more valuable than operating in the dark. It allows actuaries to make informed decisions about whether the additional complexity of a black-box model is justified by its performance, or if a more transparent structure is preferable for regulatory and risk management purposes.\nAs machine learning models become increasingly prevalent in insurance pricing and reserving, tools like MID will be essential for ensuring that our “black-boxes” remain accountable, reliable, and fundamentally understood."
  },
  {
    "objectID": "notebooks/demo_py.html#the-interpretable-baseline-glm",
    "href": "notebooks/demo_py.html#the-interpretable-baseline-glm",
    "title": "Surrogate Modeling with MID in Python",
    "section": "The Interpretable Baseline: GLM",
    "text": "The Interpretable Baseline: GLM\nWe first fit a GLM to establish a transparent benchmark. Since GLMs are strictly additive on the link scale, they provide a “ground truth” structure. This allows us to verify whether the MID framework can accurately recover the original coefficients and linear effects before moving to more complex black-box models.\n\n\nCode\n# define variable encoder for PoissonRegressor\ndef one_hot_encode(X):\n  cats = X.select_dtypes(include=['object', 'category']).columns.tolist()\n  nums = X.select_dtypes(exclude=['object', 'category']).columns.tolist()\n  ct = ColumnTransformer(\n    transformers=[\n      ('cat', OneHotEncoder(drop='first', sparse_output=False), cats),\n      ('num', 'passthrough', nums)\n    ],\n    verbose_feature_names_out=False\n  )\n  ct.set_output(transform=\"pandas\")\n  return ct.fit_transform(X)\n\n# initialize and train a Poisson GLM\nfit_glm = lm.PoissonRegressor(alpha=0, max_iter=300)\nfit_glm.fit(one_hot_encode(X_train), y_train, sample_weight=w_train)\n\n\nPoissonRegressor(alpha=0, max_iter=300)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PoissonRegressor?Documentation for PoissonRegressoriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nalpha \n0\n\n\n\nfit_intercept \nTrue\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n300\n\n\n\ntol \n0.0001\n\n\n\nwarm_start \nFalse\n\n\n\nverbose \n0\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\n# evaluate fitted model\ny_pred_fit_glm = fit_glm.predict(one_hot_encode(X_test))\n\ndeviance = mean_poisson_deviance(\n  y_true=y_test,\n  y_pred=y_pred_fit_glm,\n  sample_weight=w_test\n)\nprint(\"Mean Poisson Deviance:\", round(deviance, 6))\n\n\nMean Poisson Deviance: 0.470569\n\n\n\nSurrogate Modeling\nWe apply the interpret() function to the GLM. This step serves as a sanity check: if MID is effective, it should perfectly replicate the predictive behavior of the original GLM.\n\n\nCode\n# build a surrogate model\nmid_glm = mid.MIDExplainer(\n  estimator=fit_glm,\n  link=\"log\"\n)\n\nmid_glm.fit(\n  X_train,\n  y=fit_glm.predict(one_hot_encode(X_train))\n)\n\n\nMIDExplainer(estimator=PoissonRegressor(alpha=0, max_iter=300), link='log')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MIDExplaineriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nPoissonRegres... max_iter=300)\n\n\n\ntarget_classes \nNone\n\n\n\nparams_main \nNone\n\n\n\nparams_inter \nNone\n\n\n\npenalty \n0\n\n\n\nlink \n'log'\n\n\n\nkernel_type \n1\n\n\n\nencoding_frames \n{}\n\n\n\nmodel_terms \nNone\n\n\n\nsingular_ok \nFalse\n\n\n\nmode \n1\n\n\n\nmethod \nNone\n\n\n\ncentering_penalty \n1000000.0\n\n\n\nna_action \n'na.omit'\n\n\n\nverbosity \n1\n\n\n\nencoding_digits \n3\n\n\n\nuse_catchall \nFalse\n\n\n\ncatchall \n'(others)'\n\n\n\nmax_nelements \n1000000000.0\n\n\n\nnil \n1e-07\n\n\n\ntol \n1e-07\n\n\n\n\n            \n        \n    estimator: PoissonRegressorPoissonRegressor(alpha=0, max_iter=300)PoissonRegressor?Documentation for PoissonRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nalpha \n0\n\n\n\nfit_intercept \nTrue\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n300\n\n\n\ntol \n0.0001\n\n\n\nwarm_start \nFalse\n\n\n\nverbose \n0\n\n\n\n\n            \n        \n    \n\n\n\n\nCode\n# evaluate fitted surrogate\nprint(\n  \"Uninterpreted Variation Ratio:\",\n  round(mid_glm.ratio['working'], 6)\n)\n\ny_pred_mid_glm = mid_glm.predict(X_test)\n\ndeviance = mean_poisson_deviance(\n  y_true=y_test,\n  y_pred=y_pred_mid_glm,\n  sample_weight=w_test\n)\nprint(\"Mean Poisson Deviance:\", round(deviance, 6))\n\n\nUninterpreted Variation Ratio: 0.0\nMean Poisson Deviance: 0.470569\n\n\n\n\nModel Fidelity\nTo assess model fidelity, i.e., how closely the surrogate replicates the black-box, we calculate the uninterpreted variation ratio \\(\\mathbf{U}\\).\n\\[\n\\mathbf{U}(f,g) = \\frac{\\sum_{i=1}^n (f(x_i) - g(x_i))^2}{\\sum_{i=1}^n (f(x_i) - \\bar{f})^2}, \\quad \\text{where } \\bar{f} = \\frac{1}{n}\\sum_{i=1}^n f(x_i)\n\\]\nThis metric represents the proportion of the black-box model’s variance that is not captured by the additive components of the MID model. The R-squared score, \\(\\mathbf{R}^2(f,g) = 1 - \\mathbf{U}(f,g)\\), is a standard measure for this purpose. It is important to note that this \\(\\mathbf{R}^2\\) measures the fidelity to the black-box model, not the predictive accuracy relative to the ground truth observations.\nIn the {midr} package, the summary output includes this ratio calculated on the training set. For models with non-linear links (e.g., Poisson regression), the “working” ratio is computed on the scale of the link function (e.g., \\(\\log\\) scale).\nTo rigorously confirm the model fidelity, it is recommended to evaluate these metrics on a separate testing set. This ensures that the surrogate model is not just over-fitting the training predictions but has truly captured the underlying functional structure.\n\n\nCode\n# calculate R-squared on testing dataset\nr2_mid = r2_score(\n  y_true=np.log(fit_glm.predict(one_hot_encode(X_test))),\n  y_pred=np.log(mid_glm.predict(X_test)),\n  sample_weight=w_test\n)\nprint(\"R squared:\", round(r2_mid, 10))\n\n\nR squared: 0.9999999934\n\n\nAs shown by the high \\(\\mathbf{R}^2\\) score, the MID surrogate achieves near-perfect fidelity. This level of agreement justifies using the MID components (main effects and interactions) as a reliable lens through which to interpret the original black-box model’s behavior.\n\n\nFeature Effects\nWhile the coefficients of a GLM are directly interpretable, visualizing their functional behavior across the feature space provides a more intuitive grasp of the model’s structure. In the Python ecosystem, standard libraries for GLMs often lack built-in “term plots” (common in R) to visualize partial effects on the link scale.\nHere, the MID surrogate serves as a visualization tool: by appropriately replicating the PoissonRegressor, we can directly plot the main effects \\(g_j(X_j)\\) to verify that the linear relationships on the log scale are correctly captured.\n\n\nCode\n# main effects of MID surrogate\nplots = []\nfor feature in X_train.columns:\n  p = (\n  mid_glm.plot(feature) +\n    lims(y=[-0.8, 0.8]) +\n    labs(y=\"Main Effect\")\n  )\n  plots.append(p)\n\ndisplay(\n  (plots[0] | plots[1] | plots[2] | plots[5]) /\n  (plots[3] | plots[4] | plots[6])\n)\n\n\n\n\n\n\n\n\n\n\n\nEffect Importance\nBeyond simple plots for feature effects, {midr} provides a suite of diagnostic tools. First, the Effect Importance of a term \\(j\\) is defined as the mean absolute contribution of that term across the population:\n\\[\n\\text{Importance}_j = \\mathbf{E} \\left[ | g_j(X_j) | \\right] \\approx \\frac{1}{n} \\sum_{i=1}^n | g_j(x_{ij}) |\n\\]\n\n\nCode\nimp_glm = mid_glm.importance(max_nsamples=2000)\n\np1 = (\n  imp_glm.plot(fill=\"steelblue\") +\n  labs(title=\"Mean Absolute Effect\", subtitle=\"Bar Plot\")\n)\np2 = (\n  imp_glm.plot(style=\"sinaplot\", theme=\"mako@div\") +\n  labs(subtitle=\"Distribution of Effects\") +\n  theme(legend_position=\"none\", axis_text_y=element_blank())\n)\ndisplay(p1 | p2)\n\n\n\n\n\n\n\n\n\nFor interaction terms, the importance is similarly calculated using \\(g_{jk}(X_j, X_k)\\). This metric allows us to rank features by their average influence on the model’s predictions.\n\n\nConditional Expectation\nSecond, we can explore Individual Conditional Expectations (ICE). In the MID framework, the ICE for a feature \\(j\\) and a specific observation \\(i\\) is the expected value of the prediction as \\(X_j\\) varies, while keeping other features fixed at their observed values \\(\\mathbf{x}_{\\setminus j}^{(i)}\\):\n\\[\n\\text{ICE}_j^{(i)}(x) = g_\\emptyset + g_j(x) + \\sum_{k \\neq j} g_{jk}(x, x_{ik})\n\\]\n\n\nCode\nice_glm_link = mid_glm.conditional(\n  type=\"link\", variable=\"DrivAge\", data=X_train.sample(200)\n)\nice_glm = mid_glm.conditional(\n  variable=\"DrivAge\", data=X_train.sample(200)\n)\n\np1 = (\n  ice_glm_link.plot(theme=\"bluescale\", var_color=\"LogDensity\") +\n  theme(legend_position=\"bottom\") +\n  labs(y=\"Linear Predictor\",\n       title=\"Conditional Expectation\",\n       subtitle=\"Change in linear predictor\")\n  )\np2 = (\n  ice_glm.plot(style=\"centered\", theme=\"bluescale\",\n               var_color=\"LogDensity\") +\n  theme(legend_position = \"bottom\") +\n  labs(y=\"Prediction\", title=\"\",\n       subtitle=\"Change in original scale\")\n  )\ndisplay(p1 | p2)\n\n\n\n\n\n\n\n\n\nUnlike standard black-box models, MID’s low-order structure allows us to compute these expectations efficiently and interpret the variation across curves (the “thickness” of the ICE plot) as a direct consequence of specified interaction terms \\(g_{jk}\\).\n\n\nAdditive Attribution\nThird, we perform instance-level explanation through an Additive Breakdown of the prediction. For any single observation \\(\\mathbf{x}\\), the MID surrogate’s prediction \\(g(\\mathbf{x})\\) is decomposed into the exact sum of its functional components:\n\\[\ng(\\mathbf{x}) = \\underbrace{g_\\emptyset}_{\\text{Intercept}} + \\underbrace{\\sum_{j} g_j(x_j)}_{\\text{Main Effects}} + \\underbrace{\\sum_{j &lt; k} g_{jk}(x_j, x_k)}_{\\text{Interactions}}\n\\]\n\n\nCode\nnp.random.seed(42)\nrow_ids = sorted(np.random.randint(0, train.shape[0], 4).tolist())\n\nbd_plots = []\nfor idx in row_ids:\n  bd = mid_glm.breakdown(row=idx)\n  label = \"Breakdown for Row \" + str(idx)\n  p = (\n    bd.plot(theme=\"shap\", format_args={'digits': 2}) +\n    labs(x=\"\", subtitle=label) +\n    theme(legend_position=\"none\")\n  )\n  bd_plots.append(p)\n\ndisplay(\n  (bd_plots[0] | bd_plots[1]) /\n  (bd_plots[2] | bd_plots[3]) \n)\n\n\n\n\n\n\n\n\n\nBy visualizing these contributions in a waterfall plot, we can identify which specific risk factors or interaction effects drove the prediction for a particular instance, such as a high-risk policyholder."
  }
]