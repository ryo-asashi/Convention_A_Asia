[
  {
    "objectID": "notebooks/demo_r.html",
    "href": "notebooks/demo_r.html",
    "title": "Interpretation by Global Surrogate Modeling with {midr}",
    "section": "",
    "text": "Code\nlibrary(arrow)\n\n\nWarning: package 'arrow' was built under R version 4.5.2\n\n\n\nAttaching package: 'arrow'\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n\n\nCode\nlibrary(midr)\n\n\nWarning in lapply(X = .schemes, FUN = function(x) {: strings not representable\nin native encoding will be translated to UTF-8\n\n\nCode\nlibrary(midnight)\n\n\nLoading required package: parsnip\n\n\nCode\nlibrary(ggplot2)\n\n\n\n\nCode\ntrain &lt;- read_parquet(\"../data/train.parquet\")\ntest  &lt;- read_parquet(\"../data/test.parquet\")\n\n\n\n\nCode\nlibrary(splines)\n\nfit_glm &lt;- glm(\n  Frequency ~ VehPower + VehAge + ns(DrivAge, df = 5) + VehBrand + VehGas + LogDensity + Region,\n  data = train,\n  weights = Exposure,\n  family = quasipoisson()\n)\n\nsummary(fit_glm)\n\n\n\nCall:\nglm(formula = Frequency ~ VehPower + VehAge + ns(DrivAge, df = 5) + \n    VehBrand + VehGas + LogDensity + Region, family = quasipoisson(), \n    data = train, weights = Exposure)\n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       -1.922742   0.098497 -19.521  &lt; 2e-16 ***\nVehPower                           0.030288   0.005801   5.221 1.78e-07 ***\nVehAge                            -0.012916   0.002398  -5.386 7.20e-08 ***\nns(DrivAge, df = 5)1              -1.306706   0.067780 -19.279  &lt; 2e-16 ***\nns(DrivAge, df = 5)2              -1.309012   0.080730 -16.215  &lt; 2e-16 ***\nns(DrivAge, df = 5)3              -1.048974   0.088655 -11.832  &lt; 2e-16 ***\nns(DrivAge, df = 5)4              -2.933995   0.176914 -16.584  &lt; 2e-16 ***\nns(DrivAge, df = 5)5              -0.224127   0.187145  -1.198  0.23107    \nVehBrandB12                       -0.235715   0.040323  -5.846 5.05e-09 ***\nVehBrandB2                         0.011060   0.031912   0.347  0.72890    \nVehBrandB3                         0.032384   0.044682   0.725  0.46860    \nVehBrandB5                         0.156921   0.050076   3.134  0.00173 ** \nVehBrandOther                      0.038337   0.036485   1.051  0.29338    \nVehGasRegular                     -0.152496   0.023653  -6.447 1.14e-10 ***\nLogDensity                         0.102270   0.007322  13.968  &lt; 2e-16 ***\nRegionCentre                      -0.012085   0.049034  -0.246  0.80533    \nRegionIle-de-France                0.095651   0.059853   1.598  0.11002    \nRegionPays-de-la-Loire             0.041869   0.063439   0.660  0.50927    \nRegionProvence-Alpes-Cotes-D'Azur  0.146424   0.055538   2.636  0.00838 ** \nRegionRhone-Alpes                  0.226987   0.052063   4.360 1.30e-05 ***\nRegionOther                        0.063823   0.048680   1.311  0.18983    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 1.740256)\n\n    Null deviance: 86472  on 338994  degrees of freedom\nResidual deviance: 84838  on 338974  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nCode\nmid_glm &lt;- interpret(\n  Frequency ~\n    DrivAge + LogDensity + VehAge + VehPower +\n    Region + VehBrand + VehGas,\n  train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_glm,\n  verbosity = 3\n)\n\n\n[info] model fitting started (2026-01-10 02:51:45)\n\n\n- [debug] 338995 predictions obtained from 'model': 0.13265512, 0.09079538, 0.08235036, ...\n\n\n- [debug] model frame with 338995 observations created\n\n\n- [debug] 'y' values are transformed by 'link': -2.020003, -2.399147, -2.496772, ...\n\n\n- [debug] 'terms' include 7 main effects\n\n\n- [debug] 'k' is set to 25 for main effects\n\n\n[info] least squares estimation initiated with 'mode' 1 and 'method' 0\n\n\n- [debug] 92 parameters, 338995 observations, 7 centering constraints\n\n\n[info] least squares estimation completed\n\n\n- [debug] uninterpreted variation ratio: 0.000107208\n\n\n- [debug] uninterpreted variation ratio (response): 0.0001247579\n\n\n[info] model fitting successfully finished (2026-01-10 02:51:49)\n\n\n\n\nCode\nsummary(mid_glm)\n\n\n\nCall:\ninterpret(formula = Frequency ~ DrivAge + LogDensity + VehAge +\n VehPower + Region + VehBrand + VehGas, data = train, model = fit_glm,\n weights = Exposure, verbosity = 3, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n   working   response \n0.00010721 0.00012476 \n\nWorking Residuals:\n       Min         1Q     Median         3Q        Max \n-0.0103322 -0.0008741 -0.0001187  0.0012507  0.0945788 \n\n\n\n\n\n\n\n\n\n\nEncoding:\n           main.effect\nDrivAge     linear(25)\nLogDensity  linear(25)\nVehAge      linear(18)\nVehPower     linear(9)\nRegion       factor(7)\nVehBrand     factor(6)\nVehGas       factor(2)\n\n\n\n\nCode\npal &lt;- \"#004080\"\ngridExtra::grid.arrange(\n  ggmid(mid_glm, \"DrivAge\", linewidth = 1, color = pal),\n  ggmid(mid_glm, \"LogDensity\", linewidth = 1, color = pal),\n  ggmid(mid_glm, \"VehAge\", linewidth = 1, color = pal),\n  ggmid(mid_glm, \"VehPower\", linewidth = 1, color = pal)\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\npar.midr(mar = c(1,1,1,1))\npersp(mid_glm, \"DrivAge:LogDensity\", theta = 135, phi = 30, col = \"#00408080\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nimp_glm &lt;- mid.importance(mid_glm, max.nrow = 2000)\n\n\nnumber of observations exceeds 'max.nrow': a sample of 2000 observations from 'data' is stored\n\n\nCode\nggmid(imp_glm, \"bee\", theme = \"viridis_r\") + theme_midr(\"y\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n### GBM\nX_train &lt;- data.matrix(\n  train |&gt; dplyr::select(-Frequency, -Exposure)\n)\n\nX_test  &lt;- data.matrix(\n  test  |&gt; dplyr::select(-Frequency, -Exposure)\n)\n\nlibrary(lightgbm)\n\n\nWarning: package 'lightgbm' was built under R version 4.5.2\n\n\nCode\nparams_lgb &lt;- list(\n  objective = \"poisson\",\n  learning_rate = 0.02,\n  num_leaves = 31,\n  reg_lambda = 0,\n  reg_alpha = 2,\n  colsample_bynode = 0.8,\n  subsample = 0.8,\n  min_child_samples = 20,\n  min_split_gain = 0.1,\n  poisson_max_delta_step = 0.1\n)\n\nfit_lgb &lt;- lightgbm(\n  data = X_train,\n  label = train$Frequency,\n  weights = train$Exposure,\n  params = params_lgb,\n  nrounds = 200L,\n  verbose = 0L,\n  categorical_feature = c(\"VehBrand\", \"VehGas\", \"Region\")\n)\n\n\nWarning in .get_default_num_threads(): Optional package 'RhpcBLASctl' not\nfound. Detection of CPU cores might not be accurate.\n\n\nCode\nsummary(fit_lgb)\n\n\nLightGBM Model (200 trees)\nObjective: poisson\nFitted to dataset with 7 columns\n\n\nCode\nfitted_values_lgb &lt;- get.yhat(fit_lgb, X_train)\n\n\n\n\nCode\nmid_lgb &lt;- interpret(\n  Frequency ~ (. - Exposure)^2,\n  data = train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_lgb,\n  pred.fun = function(model, newdata) fitted_values_lgb,\n  verbosity = 3,\n  lambda = .05\n)\n\n\n[info] model fitting started (2026-01-10 02:52:10)\n\n\n- [debug] 338995 predictions obtained from 'model': 0.11716635, 0.08585971, 0.07847172, ...\n\n\n- [debug] model frame with 338995 observations created\n\n\n- [debug] 'y' values are transformed by 'link': -2.144161, -2.455041, -2.545017, ...\n\n\n- [debug] 'terms' include 7 main effects and 21 interactions\n\n\n- [debug] 'k' is set to 25 for main effects and 5 for interactions\n\n\n[info] least squares estimation initiated with 'mode' 1 and 'method' 0\n\n\n- [debug] 610 parameters, 338995 observations, 217 centering constraints, 677 smoothing constraints\n\n\n[info] least squares estimation completed\n\n\n- [debug] uninterpreted variation ratio: 0.06028539\n\n\n- [debug] uninterpreted variation ratio (response): 0.2397887\n\n\n[info] model fitting successfully finished (2026-01-10 02:53:44)\n\n\nCode\n# required 10 GiB\n\n\n\n\nCode\nsummary(mid_lgb)\n\n\n\nCall:\ninterpret(formula = Frequency ~ (. - Exposure)^2, data = train,\n model = fit_lgb, pred.fun = function(model, newdata) fitted_values_lgb,\n weights = Exposure, verbosity = 3, link = \"log\", lambda = 0.05)\n\nLink: log\n\nUninterpreted Variation Ratio:\n working response \n0.060285 0.239789 \n\nWorking Residuals:\n     Min       1Q   Median       3Q      Max \n-0.53975 -0.04373 -0.00305  0.03957  4.21130 \n\n\n\n\n\n\n\n\n\n\nEncoding:\n           main.effect interaction\nVehPower     linear(9)   linear(5)\nVehAge      linear(18)   linear(5)\nDrivAge     linear(25)   linear(5)\nVehBrand     factor(6)   factor(6)\nVehGas       factor(2)   factor(2)\nLogDensity  linear(25)   linear(5)\nRegion       factor(7)   factor(7)\n\n\n\n\nCode\ngridExtra::grid.arrange(\n  ggmid(mid_lgb, \"DrivAge\", linewidth = 1, color = pal),\n  ggmid(mid_lgb, \"LogDensity\", linewidth = 1, color = pal),\n  ggmid(mid_lgb, \"VehAge\", linewidth = 1, color = pal),\n  ggmid(mid_lgb, \"VehPower\", linewidth = 1, color = pal)\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nrotate &lt;- theme(axis.text.x = element_text(angle = 90))\ngridExtra::grid.arrange(\n  nrow = 1,\n  ggmid(mid_lgb, \"Region\") + rotate,\n  ggmid(mid_lgb, \"VehBrand\") + rotate\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nimp_lgb &lt;- mid.importance(mid_lgb, data = train, max.nrow = 2000)\n\n\nnumber of observations exceeds 'max.nrow': a sample of 2000 observations from 'data' is stored\n\n\nCode\nggmid(imp_lgb, \"heatmap\")\n\n\n\n\n\n\n\n\n\nCode\nggmid(imp_lgb, \"bee\", max.nterms = 15, theme = \"viridis_r\")\n\n\n\n\n\n\n\n\n\n```"
  },
  {
    "objectID": "notebooks/dataset.html",
    "href": "notebooks/dataset.html",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#overview",
    "href": "notebooks/dataset.html#overview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#data-preprocessing",
    "href": "notebooks/dataset.html#data-preprocessing",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThe following transformations are applied to handle outliers, improve model stability, and align with actuarial pricing practices:\n\nTarget Variable: We define Frequency as the number of claims per unit of exposure. \\[\\text{Frequency} = \\frac{\\text{ClaimNb}}{\\text{Exposure}} \\]\nCapping: VehAge (vehicle age) is capped at 25 years to mitigate the influence of extreme outliers (vintage cars) which follow different risk profiles.\nLog-Transformation: Density (population density) is log-transformed to stabilize the variance and simplify its relationship with claim frequency.\nCategorical Lumping: For VehBrand and Region, infrequent levels are collapsed into an “Other” category. This prevents overfitting in low-exposure segments.\n\n\n\nCode\n# load dataset from CASdatasets\ndata(freMTPL2freq, package = \"CASdatasets\")\n\n# preprocess dataset\ndf_all &lt;- freMTPL2freq |&gt;\n  dplyr::mutate(\n    Frequency  = ClaimNb / Exposure,\n    VehAge = pmin(VehAge, 25),\n    VehBrand = forcats::fct_lump(VehBrand, 5),\n    LogDensity = log(Density),\n    Region = forcats::fct_lump(Region, 6)\n  ) |&gt;\n  dplyr::select(\n    Frequency, Exposure, VehPower, VehAge,\n    DrivAge, VehBrand, VehGas, LogDensity, Region\n  ) |&gt;\n  dplyr::as_tibble()"
  },
  {
    "objectID": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "href": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Split for Hold-Out Validation",
    "text": "Data Split for Hold-Out Validation\nTo ensure a robust evaluation of our models, we split the dataset into training and testing sets.\nWe store these datasets in Parquet format. Unlike standard CSV files, Parquet preserves schema information (e.g., categorical types) and provides high-performance I/O.\nBy locking the data into a binary format after the initial split, we guarantee that all subsequent modeling steps across different environments, R and Python, remain consistent.\n\n\nCode\n# split dataset\nset.seed(42)\ndf_split &lt;- rsample::initial_split(df_all, prop = 1/2)\ndf_train &lt;- rsample::training(df_split)\ndf_test  &lt;- rsample::testing(df_split)\n\n# write dataset as parquets\narrow::write_parquet(df_train, \"../data/train.parquet\")\narrow::write_parquet(df_test, \"../data/test.parquet\")"
  },
  {
    "objectID": "notebooks/dataset.html#data-preview",
    "href": "notebooks/dataset.html#data-preview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preview",
    "text": "Data Preview\nBelow is a summary of the first 1000 rows of the processed training dataset. The Exposure column will be utilized as an offset in our subsequent modeling to account for varying policy durations."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Overview",
    "text": "Overview\nWelcome to the supplementary repository for our presentation Maximizing the Interpretation of Black-Box Models at CONVENTION A | ASIA.\nThis project demonstrates the application of Maximum Interpretation Decomposition (MID), a novel approach in Interpretable Machine Learning (IML) designed to bridge the gap between high-performance “black-box” models and the transparency requirements of actuarial practice.\n\nKey Topics of the Presentation\n\nTheoretical Foundation: Understanding the decomposition logic of complex models into interpretable parts: intercept, first-order main effects, and second-order interaction effects by MID.\nActuarial Application: Benchmarking MID using the industry-standard freMTPL2freq dataset using open-source software: midr, midnight (R), and midlearn (Python)."
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Project Structure",
    "text": "Project Structure\nThis Quarto website contains the following sections:\n\nDataset: Data cleaning and feature engineering of the French Motor Third-Party Liability dataset.\nR Demo: A demonstration of the midr and midlearn packages in R.\nPython Demo: A demonstration of the midlearn library, providing a seamless interface for Python users."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Software",
    "text": "Software\n\nmidr: GitHub / CRAN\nR package for Maximum Interpretation Decomposition in R\nmidnight: GitHub\nR package to integrate midr to the tidymodels ecosystem\nmidlearn: GitHub / PyPI\nPython library to integrate midr to the scikit-learn ecosystem"
  },
  {
    "objectID": "index.html#acknowledgment-reference",
    "href": "index.html#acknowledgment-reference",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Acknowledgment & Reference",
    "text": "Acknowledgment & Reference\n\nCASDatasets: CRAN\nThis demonstration utilizes the freMTPL2freq dataset from the CASdatasets package.\nAITools4Actuaries: Website\nWe would like to acknowledge the project for their foundational work on benchmarking ML models in insurance, which served as a reference, especially for our data preprocessing pipeline."
  },
  {
    "objectID": "notebooks/demo_py.html",
    "href": "notebooks/demo_py.html",
    "title": "Python Demo",
    "section": "",
    "text": "Code\nimport midlearn as mid\nfrom plotnine import *\n\n\nError importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\nTrying to import in ABI mode.\n\n\n\n\nCode\nimport pandas as pd\n\ntrain = pd.read_parquet(\"../data/train.parquet\")\ntest  = pd.read_parquet(\"../data/test.parquet\")\n\nprint(train.head())\n\n\n   Frequency  Exposure  VehPower  VehAge  DrivAge VehBrand   VehGas  \\\n0        0.0      1.00         6     9.0       43       B1   Diesel   \n1        0.0      0.18         4    16.0       30       B2  Regular   \n2        0.0      0.53         8     3.0       48      B12   Diesel   \n3        0.0      0.08         6    18.0       38    Other  Regular   \n4        0.0      1.00         7     5.0       69       B2  Regular   \n\n   LogDensity                       Region  \n0    9.138522                  Rhone-Alpes  \n1    9.138522                  Rhone-Alpes  \n2    6.336826  Provence-Alpes-Cotes-D'Azur  \n3    8.336151                  Rhone-Alpes  \n4    7.242082                        Other"
  }
]