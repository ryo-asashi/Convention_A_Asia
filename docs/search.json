[
  {
    "objectID": "notebooks/demo_r.html",
    "href": "notebooks/demo_r.html",
    "title": "Surrogate Modeling with MID in R",
    "section": "",
    "text": "In actuarial practice, balancing predictive performance with model interpretability is a constant challenge. While Generalized Additive Models (GAMs) offer inherent transparency, complex black-box models like Gradient Boosting Models (GBMs) often provide superior accuracy at the cost of clarity.\nThis notebook demonstrates Maximum Interpretation Decomposition (MID) using the {midr} package in R. MID serves as a surrogate modeling framework that decomposes complex predictions into interpretable components—main effects and interactions—allowing us to open the black box without sacrificing the predictive power of it.\n\n\nCode\n# data manipuration\nlibrary(arrow)\nlibrary(dplyr)\n\n# predictive modeling\nlibrary(gam)\nlibrary(lightgbm)\n\n# surrogate modeling\nlibrary(midr)\nlibrary(midnight)\n\n# visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# load training and testin datasets\ntrain &lt;- read_parquet(\"../data/train.parquet\")\ntest  &lt;- read_parquet(\"../data/test.parquet\")\n\n# cold execution\nCOLD_RUN &lt;- TRUE\nif (COLD_RUN) {\n  set.seed(42)\n  train &lt;- train[sample(nrow(train), 50000), ]\n  test  &lt;- test[sample(nrow(test), 50000), ]\n}\n\n\n\n\nCode\n# define loss function\nmean_poisson_deviance &lt;- function(\n    y_true, y_pred, sample_weight = rep(1, length(y))\n  ) {\n  stopifnot(all(y_pred &gt; 0))\n  resid &lt;- ifelse(y_true &gt; 0, y_true * log(y_true / y_pred), 0)\n  resid &lt;- resid - y_true + y_pred\n  2 * sum(resid * sample_weight) / sum(sample_weight)\n}"
  },
  {
    "objectID": "notebooks/demo_r.html#introduction",
    "href": "notebooks/demo_r.html#introduction",
    "title": "Surrogate Modeling with MID in R",
    "section": "",
    "text": "In actuarial practice, balancing predictive performance with model interpretability is a constant challenge. While Generalized Additive Models (GAMs) offer inherent transparency, complex black-box models like Gradient Boosting Models (GBMs) often provide superior accuracy at the cost of clarity.\nThis notebook demonstrates Maximum Interpretation Decomposition (MID) using the {midr} package in R. MID serves as a surrogate modeling framework that decomposes complex predictions into interpretable components—main effects and interactions—allowing us to open the black box without sacrificing the predictive power of it.\n\n\nCode\n# data manipuration\nlibrary(arrow)\nlibrary(dplyr)\n\n# predictive modeling\nlibrary(gam)\nlibrary(lightgbm)\n\n# surrogate modeling\nlibrary(midr)\nlibrary(midnight)\n\n# visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# load training and testin datasets\ntrain &lt;- read_parquet(\"../data/train.parquet\")\ntest  &lt;- read_parquet(\"../data/test.parquet\")\n\n# cold execution\nCOLD_RUN &lt;- TRUE\nif (COLD_RUN) {\n  set.seed(42)\n  train &lt;- train[sample(nrow(train), 50000), ]\n  test  &lt;- test[sample(nrow(test), 50000), ]\n}\n\n\n\n\nCode\n# define loss function\nmean_poisson_deviance &lt;- function(\n    y_true, y_pred, sample_weight = rep(1, length(y))\n  ) {\n  stopifnot(all(y_pred &gt; 0))\n  resid &lt;- ifelse(y_true &gt; 0, y_true * log(y_true / y_pred), 0)\n  resid &lt;- resid - y_true + y_pred\n  2 * sum(resid * sample_weight) / sum(sample_weight)\n}"
  },
  {
    "objectID": "notebooks/demo_r.html#interpretable-gam",
    "href": "notebooks/demo_r.html#interpretable-gam",
    "title": "Surrogate Modeling with MID in R",
    "section": "Interpretable GAM",
    "text": "Interpretable GAM\nWe begin by fitting a standard GAM to the motor insurance frequency dataset. Since a GAM is already composed of additive smooth functions, it serves as a perfect sanity check for our surrogate modeling approach.\n\nPredictive Modeling\n\n\nCode\nfit_gam &lt;- gam(\n  Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + s(LogDensity) +\n              VehBrand + VehGas + Region,\n  data = train,\n  weights = Exposure,\n  family = quasipoisson(link = \"log\")\n)\nsummary(fit_gam)\n\n\n\nCall: gam(formula = Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + \n    s(LogDensity) + VehBrand + VehGas + Region, family = quasipoisson(link = \"log\"), \n    data = train, weights = Exposure)\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-0.7346 -0.3492 -0.2640 -0.1436 10.7132 \n\n(Dispersion Parameter for quasipoisson family taken to be 1.7132)\n\n    Null Deviance: 13216.45 on 49999 degrees of freedom\nResidual Deviance: 12963.4 on 49971 degrees of freedom\nAIC: NA \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n                 Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ns(VehPower)       1      9   9.385  5.4783   0.01926 *  \ns(VehAge)         1      2   2.002  1.1683   0.27975    \ns(DrivAge)        1     34  33.966 19.8260 8.500e-06 ***\ns(LogDensity)     1    110 109.592 63.9691 1.291e-15 ***\nVehBrand          5     11   2.142  1.2505   0.28245    \nVehGas            1      1   1.274  0.7438   0.38844    \nRegion            6     12   2.037  1.1889   0.30869    \nResiduals     49971  85610   1.713                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n              Npar Df  Npar F     Pr(F)    \n(Intercept)                                \ns(VehPower)         3  0.5258    0.6645    \ns(VehAge)           3  0.7300    0.5339    \ns(DrivAge)          3 15.5538 4.146e-10 ***\ns(LogDensity)       3  0.3274    0.8055    \nVehBrand                                   \nVehGas                                     \nRegion                                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# evaluate model\npred_fit_gam &lt;- predict(fit_gam, test, type = \"response\")\ncat(\n  \"Mean Poisson Deviance:\",\n  mean_poisson_deviance(test$Frequency, pred_fit_gam, test$Exposure)\n)\n\n\nMean Poisson Deviance: 0.4895171\n\n\n\n\nSurrogate Modeling\n\n\nCode\nmid_gam &lt;- interpret(\n  Frequency ~ VehPower + VehAge + DrivAge + LogDensity +\n              VehBrand + VehGas + Region,\n  train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_gam\n)\nsummary(mid_gam)\n\n\n\nCall:\ninterpret(formula = Frequency ~ VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region, data = train, model = fit_gam,\n weights = Exposure, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n   working   response \n0.00017046 0.00011598 \n\nWorking Residuals:\n       Min         1Q     Median         3Q        Max \n-0.0338466 -0.0008416 -0.0000489  0.0007905  0.0327146 \n\nEncoding:\n           main.effect\nVehPower     linear(9)\nVehAge      linear(18)\nDrivAge     linear(25)\nLogDensity  linear(25)\nVehBrand     factor(6)\nVehGas       factor(2)\nRegion       factor(7)\n\n\n\n\nModel Fidelity\nAfter wrapping the GAM with the interpret() function, we evaluate the Model Fidelity. In the MID framework, the (Working) Uninterpreted Variation Ratio (\\(1 - R^s\\)) quantifies how much of the original model’s logic remains “unexplained.” As shown in the scatter plots, the MID surrogate almost perfectly replicates the GAM’s predictions on the test set, both on the linear predictor and the original response scale.\n\n\nCode\ntheme_set(theme_midr(\"xy\"))\npred_mid_gam = predict(mid_gam, test, type = \"response\")\n\nset.seed(42)\nplot_idx &lt;- sample(nrow(test), 2000L)\n\np &lt;- ggplot(\n  data.frame(fit = pred_fit_gam[plot_idx],\n             mid = pred_mid_gam[plot_idx])\n)\ngrid.arrange(\n  nrow = 1,\n  p +\n    geom_point(aes(log(fit), log(mid))) +\n    labs(x = \"GAM\", y = \"MID Surrogate\",\n         title = \"Model Fidelity\", subtitle = \"Linear predictor\"),\n  p +\n    geom_point(aes(fit, mid)) +\n    labs(x = \"GAM\", y = \"MID Surrogate\",\n         title = \"\", subtitle = \"Prediction in the original scale\")\n)\n\n\n\n\n\n\n\n\n\n\n\nMain Effect\nA critical step in validating a surrogate model is ensuring that the extracted functional forms (main effects) align with the underlying model’s logic. In the case of a GAM, where effects are explicitly modeled as splines, the MID surrogate should ideally recover these shapes with high fidelity.\n\n\nCode\npar.midr(mfrow = c(2, 4))\ntermplot(fit_gam)\n\n\n\n\n\n\n\n\n\n\n\nCode\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_gam, engine = \"graphics\", ylab = \"Partial Effect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\npar.midr(mar = c(1,1,1,1))\npersp(mid_gam, \"LogDensity:DrivAge\", theta = 225, phi = 40, shade = .5)\n\n\n\n\n\n\n\n\n\n\n\nEffect Importance\n\n\nCode\nimp_gam &lt;- mid.importance(mid_gam, data = train, max.nrow = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(5, 4),\n  ggmid(imp_gam, fill = \"steelblue\") +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\"),\n  ggmid(imp_gam, type = \"beeswarm\", theme = \"mako@div\") +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n\n\n\n\n\n\n\n\n\n\n\nConditional Expectation\n\n\nCode\nice_gam_link &lt;- mid.conditional(mid_gam, type = \"link\", variable = \"DrivAge\")\nice_gam &lt;- mid.conditional(mid_gam, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_gam_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_gam, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAdditive Attribution\n\n\nCode\nset.seed(42)\nrow_ids &lt;- sort(sample(nrow(train), 4))\nbd_list &lt;- lapply(\n  row_ids,\n  function(x) {\n    res &lt;- mid.breakdown(mid_gam, train, row = x, format = \"%.6s\")\n    structure(res, row_id = x)\n  }\n)\nbd_plots &lt;- lapply(\n  bd_list, function(x) {\n    label &lt;- paste0(\"Breakdown of Row \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\") +\n      labs(x = NULL, subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\ngrid.arrange(grobs = bd_plots)"
  },
  {
    "objectID": "notebooks/demo_r.html#black-box-gbm",
    "href": "notebooks/demo_r.html#black-box-gbm",
    "title": "Surrogate Modeling with MID in R",
    "section": "Black-Box GBM",
    "text": "Black-Box GBM\nNext, we train a LightGBM model. While more powerful, its tree-based structure makes it difficult to explain the relationship between inputs and outputs directly.\nWe apply MID to the LightGBM model, specifically allowing for second-order interactions (^ 2) in the formula. This allows the surrogate to capture joint effects that a simple additive model would miss.\n\n\nCode\nvalid_idx &lt;- seq_len(floor(nrow(train) * 0.2))\ndtrain &lt;- lgb.Dataset(\n  data.matrix(select(train[-valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[-valid_idx],\n  weight = train$Exposure[-valid_idx],\n  categorical_feature = c(\"VehBrand\", \"VehGas\", \"Region\")\n)\ndvalid &lt;- lgb.Dataset.create.valid(\n  dtrain,\n  data.matrix(select(train[ valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[ valid_idx],\n  weight = train$Exposure[ valid_idx]\n)\nparams_lgb &lt;- list(\n  objective = \"poisson\",\n  learning_rate = 0.005,\n  num_leaves = 10,\n  reg_lambda = 1,\n  reg_alpha = 1,\n  colsample_bynode = 0.8,\n  subsample = 0.8,\n  min_child_samples = 20,\n  min_split_gain = 0.1,\n  poisson_max_delta_step = 0.1\n)\nfit_lgb &lt;- lgb.train(\n  params = params_lgb,\n  data = dtrain,\n  nrounds = 1000L,\n  valids = list(eval = dvalid),\n  early_stopping_round = 20L,\n  verbose = 0L\n)\nsummary(fit_lgb)\n\n\nLightGBM Model (432 trees)\nObjective: poisson\nFitted to dataset with 7 columns\n\n\n\n\nCode\n# evaluate model\npred_fit_lgb &lt;- predict(\n  fit_lgb, data.matrix(select(test, -Frequency, -Exposure))\n)\ncat(\n  \"Mean Poisson Deviance:\",\n  mean_poisson_deviance(test$Frequency, pred_fit_lgb, test$Exposure)\n)\n\n\nMean Poisson Deviance: 0.4891941\n\n\n\nSurrogate Modeling\n\n\nCode\nmid_lgb &lt;- interpret(\n  Frequency ~ (VehPower + VehAge + DrivAge + LogDensity +\n               VehBrand + VehGas + Region) ^ 2,\n  train,\n  lambda = 0.5,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_lgb,\n  pred.fun = function(model, data) {\n    newdata &lt;- data.matrix(select(data, -Frequency, -Exposure))\n    predict(model, newdata)\n  }\n)\nsummary(mid_lgb)\n\n\n\nCall:\ninterpret(formula = Frequency ~ (VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region)^2, data = train,\n model = fit_lgb, pred.fun = function(model, data) {\n newdata &lt;- data.matrix(select(data, -Frequency, -Exposure))\n predict(model, newdata)\n }, weights = Exposure, lambda = 0.5, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n working response \n0.091418 0.128929 \n\nWorking Residuals:\n      Min        1Q    Median        3Q       Max \n-0.442868 -0.039610 -0.001806  0.036518  0.778933 \n\nEncoding:\n           main.effect interaction\nVehPower     linear(9)   linear(5)\nVehAge      linear(18)   linear(5)\nDrivAge     linear(25)   linear(5)\nLogDensity  linear(25)   linear(5)\nVehBrand     factor(6)   factor(6)\nVehGas       factor(2)   factor(2)\nRegion       factor(7)   factor(7)\n\n\n\n\nModel Fidelity\n\n\nCode\ntheme_set(theme_midr(\"xy\"))\npred_mid_lgb = predict(mid_lgb, test, type = \"response\")\n\n# set.seed(42)\n# plot_idx &lt;- sample(nrow(test), 2000L)\n\np &lt;- ggplot(\n  data.frame(fit = pred_fit_lgb[plot_idx],\n             mid = pred_mid_lgb[plot_idx])\n)\ngrid.arrange(\n  nrow = 1,\n  p +\n    geom_point(aes(log(fit), log(mid))) +\n    labs(x = \"LightGBM\", y = \"MID Surrogate\",\n         title = \"Model Fidelity\", subtitle = \"Linear predictor\"),\n  p +\n    geom_point(aes(fit, mid)) +\n    labs(x = \"LightGBM\", y = \"MID Surrogate\",\n         title = \"\", subtitle = \"Prediction in the original scale\")\n)\n\n\n\n\n\n\n\n\n\n\n\nMain Effect\n\n\nCode\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_lgb, engine = \"graphics\", ylab = \"Partial Effect\")\n\n\n\n\n\n\n\n\n\n\n\nInteraction Effect\nA key strength of {midr} is its ability to isolate and visualize interaction effects:\n\nInteraction Plots: We can visualize specific interactions, such as LogDensity:Region, using ggmid(). This helps identify whether certain geographical regions react differently to population density.\nPerspective Plots: The persp() function provides a 3D view of these interactions, offering a more intuitive understanding of non-linear surfaces. This S3 method is implemented in {midnight}.\n\n\n\nCode\ngrid.arrange(\n  nrow = 1, widths = c(3, 2),\n  ggmid(mid_lgb, \"LogDensity:Region\", type = \"data\",\n        data = train[1:1e4, ]) +\n    labs(y = NULL, subtitle = \"Interaction Effect\") +\n    theme(legend.position = \"bottom\"),\n  ggmid(mid_lgb, \"LogDensity:Region\", main.effects = TRUE) +\n    labs(y = NULL, subtitle = \"Total Effect\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\npar.midr(mar = c(1,1,1,1))\npersp(mid_lgb, \"LogDensity:Region\", theta = 225, phi = 40, shade = .5)\n\n\n\n\n\n\n\n\n\n\n\nEffect Importance\n\n\nCode\nimp_lgb &lt;- mid.importance(mid_lgb, data = train, max.nrow = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(4, 3),\n  ggmid(imp_lgb, theme = \"bluescale@qual\") +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\") +\n    theme(legend.position = \"none\"),\n  ggmid(imp_lgb, type = \"beeswarm\", theme = \"mako@div\") +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n\n\n\n\n\n\n\n\n\n\n\nConditional Expectation\n\n\nCode\nice_lgb_link &lt;- mid.conditional(mid_lgb, type = \"link\", variable = \"DrivAge\")\nice_lgb &lt;- mid.conditional(mid_lgb, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_lgb_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_lgb, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAdditive Attribution\n\n\nCode\nset.seed(42)\nrow_ids &lt;- sort(sample(nrow(train), 4))\nbd_list &lt;- lapply(\n  row_ids,\n  function(x) {\n    res &lt;- mid.breakdown(mid_lgb, train, row = x,\n                         format = c(\"%.6s\", \"%.3s, %.3s\"))\n    structure(res, row_id = x)\n  }\n)\nbd_plots &lt;- lapply(\n  bd_list, function(x) {\n    label &lt;- paste0(\"Breakdown of Row \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\", max.nterms = 10) +\n      labs(x = \"Linear Predictor\", subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\ngrid.arrange(grobs = bd_plots)"
  },
  {
    "objectID": "notebooks/demo_r.html#conclusion",
    "href": "notebooks/demo_r.html#conclusion",
    "title": "Surrogate Modeling with MID in R",
    "section": "Conclusion",
    "text": "Conclusion\nBy using {midr}, we have transformed a complex LightGBM model into a set of interpretable charts and attributions."
  },
  {
    "objectID": "notebooks/dataset.html",
    "href": "notebooks/dataset.html",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#overview",
    "href": "notebooks/dataset.html#overview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#data-preprocessing",
    "href": "notebooks/dataset.html#data-preprocessing",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThe following transformations are applied to handle outliers, improve model stability, and align with actuarial pricing practices:\n\nTarget Variable: We define Frequency as the number of claims per unit of exposure. \\[\\text{Frequency} = \\frac{\\text{ClaimNb}}{\\text{Exposure}} \\]\nCapping: VehAge (vehicle age) is capped at 25 years to mitigate the influence of extreme outliers (vintage cars) which follow different risk profiles.\nLog-Transformation: Density (population density) is log-transformed to stabilize the variance and simplify its relationship with claim frequency.\nCategorical Lumping: For VehBrand and Region, infrequent levels are collapsed into an “Other” category. This prevents overfitting in low-exposure segments.\n\n\n\nCode\n# load dataset from CASdatasets\ndata(freMTPL2freq, package = \"CASdatasets\")\n\n# preprocess dataset\ndf_all &lt;- freMTPL2freq |&gt;\n  dplyr::mutate(\n    Frequency  = ClaimNb / Exposure,\n    VehAge = pmin(VehAge, 25),\n    VehBrand = forcats::fct_lump(VehBrand, 5),\n    LogDensity = log(Density),\n    Region = forcats::fct_lump(Region, 6)\n  ) |&gt;\n  dplyr::select(\n    Frequency, Exposure, VehPower, VehAge,\n    DrivAge, VehBrand, VehGas, LogDensity, Region\n  ) |&gt;\n  dplyr::as_tibble()"
  },
  {
    "objectID": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "href": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Split for Hold-Out Validation",
    "text": "Data Split for Hold-Out Validation\nTo ensure a robust evaluation of our models, we split the dataset into training and testing sets.\nWe store these datasets in Parquet format. Unlike standard CSV files, Parquet preserves schema information (e.g., categorical types) and provides high-performance I/O.\nBy locking the data into a binary format after the initial split, we guarantee that all subsequent modeling steps across different environments, R and Python, remain consistent.\n\n\nCode\n# split dataset\nset.seed(42)\ndf_split &lt;- rsample::initial_split(df_all, prop = 1/2)\ndf_train &lt;- rsample::training(df_split)\ndf_test  &lt;- rsample::testing(df_split)\n\n# write dataset as parquets\narrow::write_parquet(df_train, \"../data/train.parquet\")\narrow::write_parquet(df_test, \"../data/test.parquet\")"
  },
  {
    "objectID": "notebooks/dataset.html#data-preview",
    "href": "notebooks/dataset.html#data-preview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preview",
    "text": "Data Preview\nBelow is a summary of the first 1000 rows of the processed training dataset. The Exposure column will be utilized as an offset in our subsequent modeling to account for varying policy durations."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Overview",
    "text": "Overview\nWelcome to the supplementary repository for our presentation Maximizing the Interpretation of Black-Box Models at CONVENTION A | ASIA.\nThis project demonstrates the application of Maximum Interpretation Decomposition (MID), a novel approach in Interpretable Machine Learning (IML) designed to bridge the gap between high-performance “black-box” models and the transparency requirements of actuarial practice.\n\nKey Topics of the Presentation\n\nTheoretical Foundation: Understanding the decomposition logic of complex models into interpretable parts: intercept, first-order main effects, and second-order interaction effects by MID.\nActuarial Application: Benchmarking MID using the industry-standard freMTPL2freq dataset using open-source software: {midr}, {midnight} (R), and {midlearn} (Python)."
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Project Structure",
    "text": "Project Structure\nThis Quarto website contains the following sections:\n\nDataset: Data cleaning and feature engineering of the French Motor Third-Party Liability dataset.\nR Demo: A demonstration of the {midr} and {midlearn} packages in R.\nPython Demo: A demonstration of the {midlearn} library, providing a seamless interface for Python users."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Software",
    "text": "Software\n\n{midr}: GitHub / CRAN\nR package for Maximum Interpretation Decomposition in R\n{midnight}: GitHub\nR package to integrate {midr} to the {tidymodels} ecosystem\n{midlearn}: GitHub / PyPI\nPython library to integrate {midr} to the {scikit-learn} ecosystem"
  },
  {
    "objectID": "index.html#acknowledgment-reference",
    "href": "index.html#acknowledgment-reference",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Acknowledgment & Reference",
    "text": "Acknowledgment & Reference\n\n{CASdatasets}: CRAN\nThis demonstration utilizes the freMTPL2freq dataset from the {CASdatasets} package.\nAITools4Actuaries: Website\nWe would like to acknowledge the project for their foundational work on benchmarking ML models in insurance, which served as a reference, especially for our data preprocessing pipeline."
  },
  {
    "objectID": "notebooks/demo_py.html",
    "href": "notebooks/demo_py.html",
    "title": "Python Demo",
    "section": "",
    "text": "Code\nimport midlearn as mid\nfrom plotnine import *\n\n\nError importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\nTrying to import in ABI mode.\n\n\n\n\nCode\nimport pandas as pd\n\ntrain = pd.read_parquet(\"../data/train.parquet\")\ntest  = pd.read_parquet(\"../data/test.parquet\")\n\nprint(train.head())\n\n\n   Frequency  Exposure  VehPower  VehAge  DrivAge VehBrand   VehGas  \\\n0        0.0      1.00         6     9.0       43       B1   Diesel   \n1        0.0      0.18         4    16.0       30       B2  Regular   \n2        0.0      0.53         8     3.0       48      B12   Diesel   \n3        0.0      0.08         6    18.0       38    Other  Regular   \n4        0.0      1.00         7     5.0       69       B2  Regular   \n\n   LogDensity                       Region  \n0    9.138522                  Rhone-Alpes  \n1    9.138522                  Rhone-Alpes  \n2    6.336826  Provence-Alpes-Cotes-D'Azur  \n3    8.336151                  Rhone-Alpes  \n4    7.242082                        Other"
  }
]