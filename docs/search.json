[
  {
    "objectID": "notebooks/preprocessing.html",
    "href": "notebooks/preprocessing.html",
    "title": "Preprocessing French MTPL Dataset",
    "section": "",
    "text": "Code\n# load dataset\ndata(freMTPL2freq, package = \"CASdatasets\")\n\n# preprocess dataset\ndf_all &lt;- freMTPL2freq |&gt;\n  dplyr::mutate(\n    Frequency  = ClaimNb / Exposure,\n    VehAge = pmin(VehAge, 25),\n    VehBrand = forcats::fct_lump(VehBrand, 5),\n    LogDensity = log(Density),\n    Region = forcats::fct_lump(Region, 6)\n  ) |&gt;\n  dplyr::select(\n    Frequency, Exposure, VehPower, VehAge,\n    DrivAge, VehBrand, VehGas, LogDensity, Region\n  ) |&gt;\n  dplyr::as_tibble()\n\n# split dataset\nset.seed(42)\ndf_split &lt;- rsample::initial_split(df_all, prop = 1/2)\ndf_train &lt;- rsample::training(df_split)\ndf_test  &lt;- rsample::testing(df_split)\n\n# write dataset as parquets\narrow::write_parquet(df_train, \"../data/train.parquet\")\narrow::write_parquet(df_test, \"../data/test.parquet\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IML and MID: Learning from Black-Box Models",
    "section": "",
    "text": "Welcome to the supplementary repository for our presentation at CONVENTION A | ASIA.\nThis project demonstrates the application of Maximum Interpretation Decomposition (MID), a novel approach in Interpretable Machine Learning (IML) designed to bridge the gap between high-performance “black-box” models and the transparency requirements of actuarial practice.\n\n\n\nTheoretical Foundation: Understanding the decomposition of complex models into interpretable main effects and interaction components.\nActuarial Application: Benchmarking MID using the industry-standard freMTPL2freq dataset.\nMulti-Language Software: Hands-on demonstrations using midr (R) and midlearn (Python)."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "IML and MID: Learning from Black-Box Models",
    "section": "",
    "text": "Welcome to the supplementary repository for our presentation at CONVENTION A | ASIA.\nThis project demonstrates the application of Maximum Interpretation Decomposition (MID), a novel approach in Interpretable Machine Learning (IML) designed to bridge the gap between high-performance “black-box” models and the transparency requirements of actuarial practice.\n\n\n\nTheoretical Foundation: Understanding the decomposition of complex models into interpretable main effects and interaction components.\nActuarial Application: Benchmarking MID using the industry-standard freMTPL2freq dataset.\nMulti-Language Software: Hands-on demonstrations using midr (R) and midlearn (Python)."
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "IML and MID: Learning from Black-Box Models",
    "section": "2 Project Structure",
    "text": "2 Project Structure\nThis Quarto website contains the following sections:\n\nPreprocessing: Data cleaning and feature engineering of the French Motor Third-Party Liability dataset.\nR Demo (midr): A demonstration of the midr package."
  },
  {
    "objectID": "index.html#methodology-what-is-mid",
    "href": "index.html#methodology-what-is-mid",
    "title": "IML and MID: Learning from Black-Box Models",
    "section": "3 Methodology: What is MID?",
    "text": "3 Methodology: What is MID?\nMaximum Interpretation Decomposition (MID) allows actuaries to extract “smooth” and interpretable rating factors from complex models like Gradient Boosting Machines (GBM).\nUnlike traditional GLMs, MID captures non-linearities and interactions automatically while maintaining the clarity of traditional actuarial pricing tables."
  },
  {
    "objectID": "index.html#software-references",
    "href": "index.html#software-references",
    "title": "IML and MID: Learning from Black-Box Models",
    "section": "4 Software & References",
    "text": "4 Software & References\n\nmidr: GitHub Repository (R package)\nmidlearn: GitHub Repository (Python library)\nPaper: “midr: Learning from Black-Box Models by Maximum Interpretation Decomposition” (arXiv:2506.08338)"
  },
  {
    "objectID": "index.html#acknowledgment",
    "href": "index.html#acknowledgment",
    "title": "IML and MID: Learning from Black-Box Models",
    "section": "5 Acknowledgment",
    "text": "5 Acknowledgment\nThis demonstration utilizes the freMTPL2freq dataset from the CASdatasets package.\nWe would like to acknowledge the AITools4Actuaries project for their foundational work on benchmarking ML models in insurance, which served as a reference for our data preprocessing pipeline."
  },
  {
    "objectID": "notebooks/demo_midr.html",
    "href": "notebooks/demo_midr.html",
    "title": "Interpretation by Global Surrogate Modeling with {midr}",
    "section": "",
    "text": "Code\nlibrary(arrow)\n\n\nWarning: package 'arrow' was built under R version 4.5.2\n\n\n\nAttaching package: 'arrow'\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n\n\nCode\nlibrary(midr)\nlibrary(midnight)\n\n\nLoading required package: parsnip\n\n\nCode\nlibrary(ggplot2)\n\n\n\n\nCode\ntrain &lt;- read_parquet(\"../data/train.parquet\")\ntest  &lt;- read_parquet(\"../data/test.parquet\")\n\n\n\n\nCode\nlibrary(splines)\n\nfit_glm &lt;- glm(\n  Frequency ~ VehPower + VehAge + ns(DrivAge, df = 5) + VehBrand + VehGas + LogDensity + Region,\n  data = train,\n  weights = Exposure,\n  family = quasipoisson()\n)\n\nsummary(fit_glm)\n\n\n\nCall:\nglm(formula = Frequency ~ VehPower + VehAge + ns(DrivAge, df = 5) + \n    VehBrand + VehGas + LogDensity + Region, family = quasipoisson(), \n    data = train, weights = Exposure)\n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                       -1.922742   0.098497 -19.521  &lt; 2e-16 ***\nVehPower                           0.030288   0.005801   5.221 1.78e-07 ***\nVehAge                            -0.012916   0.002398  -5.386 7.20e-08 ***\nns(DrivAge, df = 5)1              -1.306706   0.067780 -19.279  &lt; 2e-16 ***\nns(DrivAge, df = 5)2              -1.309012   0.080730 -16.215  &lt; 2e-16 ***\nns(DrivAge, df = 5)3              -1.048974   0.088655 -11.832  &lt; 2e-16 ***\nns(DrivAge, df = 5)4              -2.933995   0.176914 -16.584  &lt; 2e-16 ***\nns(DrivAge, df = 5)5              -0.224127   0.187145  -1.198  0.23107    \nVehBrandB12                       -0.235715   0.040323  -5.846 5.05e-09 ***\nVehBrandB2                         0.011060   0.031912   0.347  0.72890    \nVehBrandB3                         0.032384   0.044682   0.725  0.46860    \nVehBrandB5                         0.156921   0.050076   3.134  0.00173 ** \nVehBrandOther                      0.038337   0.036485   1.051  0.29338    \nVehGasRegular                     -0.152496   0.023653  -6.447 1.14e-10 ***\nLogDensity                         0.102270   0.007322  13.968  &lt; 2e-16 ***\nRegionCentre                      -0.012085   0.049034  -0.246  0.80533    \nRegionIle-de-France                0.095651   0.059853   1.598  0.11002    \nRegionPays-de-la-Loire             0.041869   0.063439   0.660  0.50927    \nRegionProvence-Alpes-Cotes-D'Azur  0.146424   0.055538   2.636  0.00838 ** \nRegionRhone-Alpes                  0.226987   0.052063   4.360 1.30e-05 ***\nRegionOther                        0.063823   0.048680   1.311  0.18983    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 1.740256)\n\n    Null deviance: 86472  on 338994  degrees of freedom\nResidual deviance: 84838  on 338974  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nCode\nmid_glm &lt;- interpret(\n  Frequency ~\n    DrivAge + LogDensity + VehAge + VehPower +\n    Region + VehBrand + VehGas,\n  train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_glm,\n  verbosity = 3\n)\n\n\n[info] model fitting started (2026-01-09 17:43:19)\n\n\n- [debug] 338995 predictions obtained from 'model': 0.13265512, 0.09079538, 0.08235036, ...\n\n\n- [debug] model frame with 338995 observations created\n\n\n- [debug] 'y' values are transformed by 'link': -2.020003, -2.399147, -2.496772, ...\n\n\n- [debug] 'terms' include 7 main effects\n\n\n- [debug] 'k' is set to 25 for main effects\n\n\n[info] least squares estimation initiated with 'mode' 1 and 'method' 0\n\n\n- [debug] 92 parameters, 338995 observations, 7 centering constraints\n\n\n[info] least squares estimation completed\n\n\n- [debug] uninterpreted variation ratio: 0.000107208\n\n\n- [debug] uninterpreted variation ratio (response): 0.0001247579\n\n\n[info] model fitting successfully finished (2026-01-09 17:43:23)\n\n\n\n\nCode\nsummary(mid_glm)\n\n\n\nCall:\ninterpret(formula = Frequency ~ DrivAge + LogDensity + VehAge +\n VehPower + Region + VehBrand + VehGas, data = train, model = fit_glm,\n weights = Exposure, verbosity = 3, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n   working   response \n0.00010721 0.00012476 \n\nWorking Residuals:\n       Min         1Q     Median         3Q        Max \n-0.0103322 -0.0008741 -0.0001187  0.0012507  0.0945788 \n\n\n\n\n\n\n\n\n\n\nEncoding:\n           main.effect\nDrivAge     linear(25)\nLogDensity  linear(25)\nVehAge      linear(18)\nVehPower     linear(9)\nRegion       factor(7)\nVehBrand     factor(6)\nVehGas       factor(2)\n\n\n\n\nCode\npal &lt;- \"#004080\"\ngridExtra::grid.arrange(\n  ggmid(mid_glm, \"DrivAge\", linewidth = 1, color = pal),\n  ggmid(mid_glm, \"LogDensity\", linewidth = 1, color = pal),\n  ggmid(mid_glm, \"VehAge\", linewidth = 1, color = pal),\n  ggmid(mid_glm, \"VehPower\", linewidth = 1, color = pal)\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\npar.midr(mar = c(1,1,1,1))\npersp(mid_glm, \"DrivAge:LogDensity\", theta = 135, phi = 30, col = \"#00408080\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nimp_glm &lt;- mid.importance(mid_glm, max.nrow = 2000)\n\n\nnumber of observations exceeds 'max.nrow': a sample of 2000 observations from 'data' is stored\n\n\nCode\nggmid(imp_glm, \"bee\", theme = \"viridis_r\") + theme_midr(\"y\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n### GBM\nX_train &lt;- data.matrix(\n  train |&gt; dplyr::select(-Frequency, -Exposure)\n)\n\nX_test  &lt;- data.matrix(\n  test  |&gt; dplyr::select(-Frequency, -Exposure)\n)\n\nlibrary(lightgbm)\n\n\nWarning: package 'lightgbm' was built under R version 4.5.2\n\n\nCode\nparams_lgb &lt;- list(\n  objective = \"poisson\",\n  learning_rate = 0.02,\n  num_leaves = 31,\n  reg_lambda = 0,\n  reg_alpha = 2,\n  colsample_bynode = 0.8,\n  subsample = 0.8,\n  min_child_samples = 20,\n  min_split_gain = 0.1,\n  poisson_max_delta_step = 0.1\n)\n\nfit_lgb &lt;- lightgbm(\n  data = X_train,\n  label = train$Frequency,\n  weights = train$Exposure,\n  params = params_lgb,\n  nrounds = 200L,\n  verbose = 0L,\n  categorical_feature = c(\"VehBrand\", \"VehGas\", \"Region\")\n)\n\n\nWarning in .get_default_num_threads(): Optional package 'RhpcBLASctl' not\nfound. Detection of CPU cores might not be accurate.\n\n\nCode\nsummary(fit_lgb)\n\n\nLightGBM Model (200 trees)\nObjective: poisson\nFitted to dataset with 7 columns\n\n\nCode\nfitted_values_lgb &lt;- get.yhat(fit_lgb, X_train)\n\n\n\n\nCode\nmid_lgb &lt;- interpret(\n  Frequency ~ (. - Exposure)^2,\n  data = train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_lgb,\n  pred.fun = function(model, newdata) fitted_values_lgb,\n  verbosity = 3,\n  lambda = .05\n)\n\n\n[info] model fitting started (2026-01-09 17:43:45)\n\n\n- [debug] 338995 predictions obtained from 'model': 0.11716635, 0.08585971, 0.07847172, ...\n\n\n- [debug] model frame with 338995 observations created\n\n\n- [debug] 'y' values are transformed by 'link': -2.144161, -2.455041, -2.545017, ...\n\n\n- [debug] 'terms' include 7 main effects and 21 interactions\n\n\n- [debug] 'k' is set to 25 for main effects and 5 for interactions\n\n\n[info] least squares estimation initiated with 'mode' 1 and 'method' 0\n\n\n- [debug] 610 parameters, 338995 observations, 217 centering constraints, 677 smoothing constraints\n\n\n[info] least squares estimation completed\n\n\n- [debug] uninterpreted variation ratio: 0.06028539\n\n\n- [debug] uninterpreted variation ratio (response): 0.2397887\n\n\n[info] model fitting successfully finished (2026-01-09 17:45:20)\n\n\nCode\n# required 10 GiB\n\n\n\n\nCode\nsummary(mid_lgb)\n\n\n\nCall:\ninterpret(formula = Frequency ~ (. - Exposure)^2, data = train,\n model = fit_lgb, pred.fun = function(model, newdata) fitted_values_lgb,\n weights = Exposure, verbosity = 3, link = \"log\", lambda = 0.05)\n\nLink: log\n\nUninterpreted Variation Ratio:\n working response \n0.060285 0.239789 \n\nWorking Residuals:\n     Min       1Q   Median       3Q      Max \n-0.53975 -0.04373 -0.00305  0.03957  4.21130 \n\n\n\n\n\n\n\n\n\n\nEncoding:\n           main.effect interaction\nVehPower     linear(9)   linear(5)\nVehAge      linear(18)   linear(5)\nDrivAge     linear(25)   linear(5)\nVehBrand     factor(6)   factor(6)\nVehGas       factor(2)   factor(2)\nLogDensity  linear(25)   linear(5)\nRegion       factor(7)   factor(7)\n\n\n\n\nCode\ngridExtra::grid.arrange(\n  ggmid(mid_lgb, \"DrivAge\", linewidth = 1, color = pal),\n  ggmid(mid_lgb, \"LogDensity\", linewidth = 1, color = pal),\n  ggmid(mid_lgb, \"VehAge\", linewidth = 1, color = pal),\n  ggmid(mid_lgb, \"VehPower\", linewidth = 1, color = pal)\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nrotate &lt;- theme(axis.text.x = element_text(angle = 90))\ngridExtra::grid.arrange(\n  nrow = 1,\n  ggmid(mid_lgb, \"Region\") + rotate,\n  ggmid(mid_lgb, \"VehBrand\") + rotate\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nimp_lgb &lt;- mid.importance(mid_lgb, data = train, max.nrow = 2000)\n\n\nnumber of observations exceeds 'max.nrow': a sample of 2000 observations from 'data' is stored\n\n\nCode\nggmid(imp_lgb, \"heatmap\")\n\n\n\n\n\n\n\n\n\nCode\nggmid(imp_lgb, \"bee\", max.nterms = 15, theme = \"viridis_r\")\n\n\n\n\n\n\n\n\n\n```"
  }
]