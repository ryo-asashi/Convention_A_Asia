[
  {
    "objectID": "notebooks/demo_r.html",
    "href": "notebooks/demo_r.html",
    "title": "Building Surrogate Models with {midr}",
    "section": "",
    "text": "Code\n# data manipuration\nlibrary(arrow)\n\n# predictive modeling\nlibrary(gam)\nlibrary(lightgbm)\n\n# surrogate modeling\nlibrary(midr)\nlibrary(midnight)\n\n# visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# load training and testin datasets\ntrain &lt;- read_parquet(\"../data/train.parquet\")\ntest  &lt;- read_parquet(\"../data/test.parquet\")\ntrain &lt;- train[sample(nrow(train), 50000), ]"
  },
  {
    "objectID": "notebooks/demo_r.html#gam-an-interpretable-model",
    "href": "notebooks/demo_r.html#gam-an-interpretable-model",
    "title": "Building Surrogate Models with {midr}",
    "section": "GAM: An Interpretable Model",
    "text": "GAM: An Interpretable Model\n\n\nCode\nfit_gam &lt;- gam(\n  Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + s(LogDensity) +\n              VehBrand + VehGas + Region,\n  data = train,\n  weights = Exposure,\n  family = quasipoisson(link = \"log\")\n)\nsummary(fit_gam)\n\n\n\nCall: gam(formula = Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + \n    s(LogDensity) + VehBrand + VehGas + Region, family = quasipoisson(link = \"log\"), \n    data = train, weights = Exposure)\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-0.7767 -0.3375 -0.2522 -0.1401 10.8162 \n\n(Dispersion Parameter for quasipoisson family taken to be 2.0443)\n\n    Null Deviance: 12885.06 on 49999 degrees of freedom\nResidual Deviance: 12550.42 on 49971 degrees of freedom\nAIC: NA \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n                 Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ns(VehPower)       1     18  17.535  8.5774  0.003405 ** \ns(VehAge)         1      8   7.891  3.8598  0.049462 *  \ns(DrivAge)        1     88  87.779 42.9388 5.703e-11 ***\ns(LogDensity)     1     81  80.666 39.4593 3.377e-10 ***\nVehBrand          5     26   5.297  2.5913  0.023804 *  \nVehGas            1      7   6.568  3.2129  0.073066 .  \nRegion            6     12   1.976  0.9667  0.445957    \nResiduals     49971 102155   2.044                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n              Npar Df  Npar F     Pr(F)    \n(Intercept)                                \ns(VehPower)         3  0.6279   0.59689    \ns(VehAge)           3  2.6376   0.04786 *  \ns(DrivAge)          3 11.5076 1.545e-07 ***\ns(LogDensity)       3  1.6039   0.18612    \nVehBrand                                   \nVehGas                                     \nRegion                                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nSurrogate Modeling for GAM\n\n\nCode\nmid_gam &lt;- interpret(\n  Frequency ~ VehPower + VehAge + DrivAge + LogDensity +\n              VehBrand + VehGas + Region,\n  train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_gam\n)\nsummary(mid_gam)\n\n\n\nCall:\ninterpret(formula = Frequency ~ VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region, data = train, model = fit_gam,\n weights = Exposure, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n   working   response \n0.00027850 0.00043825 \n\nWorking Residuals:\n       Min         1Q     Median         3Q        Max \n-0.0620208 -0.0012274 -0.0000731  0.0010569  0.0493120 \n\nEncoding:\n           main.effect\nVehPower     linear(9)\nVehAge      linear(18)\nDrivAge     linear(25)\nLogDensity  linear(25)\nVehBrand     factor(6)\nVehGas       factor(2)\nRegion       factor(7)\n\n\n\n\nEvaluation of Surrogate Model Fidelity\nTypically, the R-squared metrics is used to measure the fidelity of surrogate models.\nIn the summary of the fitted MID model, \\(1 - R^s\\) is referred as (Working) Uninterpreted Variation Ratio.\nWe can confirm how the predictions of the MID surrogate replicates the predictions from the original GAM on the testing dataset.\n\n\nCode\ntheme_set(theme_midr(\"xy\"))\npreds_df &lt;- data.frame(\n  fit_gam = predict(fit_gam, test, type = \"response\"),\n  mid_gam = predict(mid_gam, test, type = \"response\")\n) \nsample_id &lt;- sample(nrow(preds_df), 2000)\np &lt;- ggplot(preds_df[sample_id, ])\ngrid.arrange(\n  nrow = 1,\n  p +\n    geom_point(aes(log(fit_gam), log(mid_gam))) +\n    labs(x = \"GAM\", y = \"MID Surrogate\",\n         title = \"Model Fidelity\", subtitle = \"Linear predictor\"),\n  p +\n    geom_point(aes(log(fit_gam), log(mid_gam))) +\n    labs(x = \"GAM\", y = \"MID Surrogate\",\n         title = \"\", subtitle = \"Prediction in the original scale\")\n)\n\n\n\n\n\n\n\n\n\n\n\nVisualization of Effects\n\n\nCode\npar.midr(mfrow = c(2, 4))\ntermplot(fit_gam)\n\n\n\n\n\n\n\n\n\n\n\nCode\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_gam, engine = \"graphics\", ylab = \"Partial Effect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\npar.midr(mar = c(1,1,1,1))\npersp(mid_gam, \"LogDensity:DrivAge\", theta = 225, phi = 40, shade = .5)\n\n\n\n\n\n\n\n\n\n\n\nFeature Importance\n\n\nCode\nimp_gam &lt;- mid.importance(mid_gam, data = train, max.nrow = 2000)\ngrid.arrange(\n  nrow = 1,\n  ggmid(imp_gam, fill = \"steelblue\") +\n    labs(title = \"Feature Importance\",\n         subtitle = \"Average absolute effect per feature\"),\n  ggmid(imp_gam, type = \"beeswarm\", theme = \"mako@div\") +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    theme(legend.position = \"none\")\n)\n\n\n\n\n\n\n\n\n\n\n\nConditional Expectation\n\n\nCode\nice_gam_link &lt;- mid.conditional(mid_gam, type = \"link\", variable = \"DrivAge\")\nice_gam &lt;- mid.conditional(mid_gam, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_gam_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_gam, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAdditive Feature Attribution\n\n\nCode\nset.seed(42)\nrow_ids &lt;- sample(nrow(train), 4)\nbd_list &lt;- lapply(\n  row_ids,\n  function(x) {\n    res &lt;- mid.breakdown(mid_gam, train, row = x, format = \"%.6s\")\n    structure(res, row_id = x)\n  }\n)\nbd_plots &lt;- lapply(\n  bd_list, function(x) {\n    label &lt;- paste0(\"Row ID: \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\") +\n      labs(x = \"Linear Predictor\", subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\ngrid.arrange(grobs = bd_plots)"
  },
  {
    "objectID": "notebooks/demo_r.html#lightgbm-a-black-box-model-with-high-performance",
    "href": "notebooks/demo_r.html#lightgbm-a-black-box-model-with-high-performance",
    "title": "Building Surrogate Models with {midr}",
    "section": "LightGBM: A Black-Box Model with High Performance",
    "text": "LightGBM: A Black-Box Model with High Performance"
  },
  {
    "objectID": "notebooks/dataset.html",
    "href": "notebooks/dataset.html",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#overview",
    "href": "notebooks/dataset.html#overview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "",
    "text": "In this section, we prepare the French Motor Third-Party Liability (freMTPL2freq) dataset, a standard benchmark in actuarial science. Our goal is to transform the raw data into a clean, structured format suitable for both R and Python environments."
  },
  {
    "objectID": "notebooks/dataset.html#data-preprocessing",
    "href": "notebooks/dataset.html#data-preprocessing",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThe following transformations are applied to handle outliers, improve model stability, and align with actuarial pricing practices:\n\nTarget Variable: We define Frequency as the number of claims per unit of exposure. \\[\\text{Frequency} = \\frac{\\text{ClaimNb}}{\\text{Exposure}} \\]\nCapping: VehAge (vehicle age) is capped at 25 years to mitigate the influence of extreme outliers (vintage cars) which follow different risk profiles.\nLog-Transformation: Density (population density) is log-transformed to stabilize the variance and simplify its relationship with claim frequency.\nCategorical Lumping: For VehBrand and Region, infrequent levels are collapsed into an “Other” category. This prevents overfitting in low-exposure segments.\n\n\n\nCode\n# load dataset from CASdatasets\ndata(freMTPL2freq, package = \"CASdatasets\")\n\n# preprocess dataset\ndf_all &lt;- freMTPL2freq |&gt;\n  dplyr::mutate(\n    Frequency  = ClaimNb / Exposure,\n    VehAge = pmin(VehAge, 25),\n    VehBrand = forcats::fct_lump(VehBrand, 5),\n    LogDensity = log(Density),\n    Region = forcats::fct_lump(Region, 6)\n  ) |&gt;\n  dplyr::select(\n    Frequency, Exposure, VehPower, VehAge,\n    DrivAge, VehBrand, VehGas, LogDensity, Region\n  ) |&gt;\n  dplyr::as_tibble()"
  },
  {
    "objectID": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "href": "notebooks/dataset.html#data-split-for-hold-out-validation",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Split for Hold-Out Validation",
    "text": "Data Split for Hold-Out Validation\nTo ensure a robust evaluation of our models, we split the dataset into training and testing sets.\nWe store these datasets in Parquet format. Unlike standard CSV files, Parquet preserves schema information (e.g., categorical types) and provides high-performance I/O.\nBy locking the data into a binary format after the initial split, we guarantee that all subsequent modeling steps across different environments, R and Python, remain consistent.\n\n\nCode\n# split dataset\nset.seed(42)\ndf_split &lt;- rsample::initial_split(df_all, prop = 1/2)\ndf_train &lt;- rsample::training(df_split)\ndf_test  &lt;- rsample::testing(df_split)\n\n# write dataset as parquets\narrow::write_parquet(df_train, \"../data/train.parquet\")\narrow::write_parquet(df_test, \"../data/test.parquet\")"
  },
  {
    "objectID": "notebooks/dataset.html#data-preview",
    "href": "notebooks/dataset.html#data-preview",
    "title": "French Motor Third-Party Liability Dataset",
    "section": "Data Preview",
    "text": "Data Preview\nBelow is a summary of the first 1000 rows of the processed training dataset. The Exposure column will be utilized as an offset in our subsequent modeling to account for varying policy durations."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Overview",
    "text": "Overview\nWelcome to the supplementary repository for our presentation Maximizing the Interpretation of Black-Box Models at CONVENTION A | ASIA.\nThis project demonstrates the application of Maximum Interpretation Decomposition (MID), a novel approach in Interpretable Machine Learning (IML) designed to bridge the gap between high-performance “black-box” models and the transparency requirements of actuarial practice.\n\nKey Topics of the Presentation\n\nTheoretical Foundation: Understanding the decomposition logic of complex models into interpretable parts: intercept, first-order main effects, and second-order interaction effects by MID.\nActuarial Application: Benchmarking MID using the industry-standard freMTPL2freq dataset using open-source software: {midr}, {midnight} (R), and {midlearn} (Python)."
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Project Structure",
    "text": "Project Structure\nThis Quarto website contains the following sections:\n\nDataset: Data cleaning and feature engineering of the French Motor Third-Party Liability dataset.\nR Demo: A demonstration of the {midr} and {midlearn} packages in R.\nPython Demo: A demonstration of the {midlearn} library, providing a seamless interface for Python users."
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Software",
    "text": "Software\n\n{midr}: GitHub / CRAN\nR package for Maximum Interpretation Decomposition in R\n{midnight}: GitHub\nR package to integrate {midr} to the {tidymodels} ecosystem\n{midlearn}: GitHub / PyPI\nPython library to integrate {midr} to the {scikit-learn} ecosystem"
  },
  {
    "objectID": "index.html#acknowledgment-reference",
    "href": "index.html#acknowledgment-reference",
    "title": "Maximizing the Interpretation of Black-Box Models",
    "section": "Acknowledgment & Reference",
    "text": "Acknowledgment & Reference\n\n{CASdatasets}: CRAN\nThis demonstration utilizes the freMTPL2freq dataset from the {CASdatasets} package.\nAITools4Actuaries: Website\nWe would like to acknowledge the project for their foundational work on benchmarking ML models in insurance, which served as a reference, especially for our data preprocessing pipeline."
  },
  {
    "objectID": "notebooks/demo_py.html",
    "href": "notebooks/demo_py.html",
    "title": "Python Demo",
    "section": "",
    "text": "Code\nimport midlearn as mid\nfrom plotnine import *\n\n\nError importing in API mode: ImportError('On Windows, cffi mode \"ANY\" is only \"ABI\".')\nTrying to import in ABI mode.\n\n\n\n\nCode\nimport pandas as pd\n\ntrain = pd.read_parquet(\"../data/train.parquet\")\ntest  = pd.read_parquet(\"../data/test.parquet\")\n\nprint(train.head())\n\n\n   Frequency  Exposure  VehPower  VehAge  DrivAge VehBrand   VehGas  \\\n0        0.0      1.00         6     9.0       43       B1   Diesel   \n1        0.0      0.18         4    16.0       30       B2  Regular   \n2        0.0      0.53         8     3.0       48      B12   Diesel   \n3        0.0      0.08         6    18.0       38    Other  Regular   \n4        0.0      1.00         7     5.0       69       B2  Regular   \n\n   LogDensity                       Region  \n0    9.138522                  Rhone-Alpes  \n1    9.138522                  Rhone-Alpes  \n2    6.336826  Provence-Alpes-Cotes-D'Azur  \n3    8.336151                  Rhone-Alpes  \n4    7.242082                        Other"
  }
]