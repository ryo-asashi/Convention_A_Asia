{
  "hash": "e9be3d575ed11a05be0692149015bdf9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Surrogate Modeling with MID in R\"\n---\n\n## Introduction\n\nIn modern actuarial science, there is often a tension between predictive accuracy and model transparency.\nWhile ensemble tree-based models like **Gradient Boosting Machines (GBMs)** frequently outperform traditional **Generalized Linear Models (GLMs)**, their black-box nature presents significant hurdles for model governance, regulatory filing, and price filing.\n\nThis notebook demonstrates a solution using **Maximum Interpretation Decomposition (MID)** via the `{midr}` and `{midnight}` packages in R.\nMID is a functional decomposition framework that acts as a high-fidelity surrogate for complex models.\nIt decomposes black-box predictions into **main effects**, the isolated impact of each individual feature on the response, and **interaction effects**, the joint impact of feature pairs.\n\nBy replicating a black-box model with this structured additive approach, we can quantify the \"unexplained\" variance and derive an interpretable model that captures the superior predictive power of machine learning without sacrificing clarity.\n\nWe begin by setting up the environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data manipulation\nlibrary(arrow)\nlibrary(dplyr)\n\n# predictive modeling\nlibrary(gam)\nlibrary(lightgbm)\n\n# surrogate modeling\nlibrary(midr)\nlibrary(midnight)\n\n# visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# load training and testin datasets\ntrain <- read_parquet(\"../data/train.parquet\")\ntest  <- read_parquet(\"../data/test.parquet\")\n```\n:::\n\n\n\n\nA key component of our evaluation is the Weighted Poisson Deviance defined as follows.\n\n$$\nL(\\mathbf{y}, \\hat{\\mathbf{y}}, \\mathbf{w}) = \\frac{2 \\sum_{i=1}^n w_i \\left( y_i \\log(y_i/\\hat{y}_i) - (y_i - \\hat{y}_i) \\right)}{\\sum_{i=1}^n w_i}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define loss function\nmean_poisson_deviance <- function(\n    y_true, y_pred, sample_weight = rep(1, length(y))\n  ) {\n  stopifnot(all(y_pred > 0))\n  resid <- ifelse(y_true > 0, y_true * log(y_true / y_pred), 0)\n  resid <- resid - y_true + y_pred\n  2 * sum(resid * sample_weight) / sum(sample_weight)\n}\n```\n:::\n\n\n## The Interpretable Baseline (GAM)\n\nWe first fit a GAM to establish a transparent benchmark.\nSince GAMs are additive by design, they provide a \"ground truth\" model structure to be recovered by the functional decomposition.\n\n### Predictive Modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_gam <- gam(\n  Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + s(LogDensity) +\n              VehBrand + VehGas + Region,\n  data = train,\n  weights = Exposure,\n  family = quasipoisson(link = \"log\")\n)\n\nsummary(fit_gam)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall: gam(formula = Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + \n    s(LogDensity) + VehBrand + VehGas + Region, family = quasipoisson(link = \"log\"), \n    data = train, weights = Exposure)\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-0.7606 -0.3400 -0.2556 -0.1401 10.7719 \n\n(Dispersion Parameter for quasipoisson family taken to be 1.7241)\n\n    Null Deviance: 86471.6 on 338994 degrees of freedom\nResidual Deviance: 84868.97 on 338966 degrees of freedom\nAIC: NA \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n                  Df Sum Sq Mean Sq  F value    Pr(>F)    \ns(VehPower)        1     45   45.32  26.2882 2.942e-07 ***\ns(VehAge)          1     35   34.96  20.2764 6.704e-06 ***\ns(DrivAge)         1    320  319.82 185.4987 < 2.2e-16 ***\ns(LogDensity)      1    531  531.36 308.1989 < 2.2e-16 ***\nVehBrand           5     91   18.14  10.5199 4.072e-10 ***\nVehGas             1     56   55.72  32.3196 1.309e-08 ***\nRegion             6     73   12.15   7.0499 1.606e-07 ***\nResiduals     338966 584406    1.72                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n              Npar Df Npar F     Pr(F)    \n(Intercept)                               \ns(VehPower)         3  1.320    0.2659    \ns(VehAge)           3 11.223 2.328e-07 ***\ns(DrivAge)          3 83.378 < 2.2e-16 ***\ns(LogDensity)       3  1.052    0.3684    \nVehBrand                                  \nVehGas                                    \nRegion                                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# evaluate fitted model\npred_fit_gam <- predict(fit_gam, test, type = \"response\")\n\ndeviance <- mean_poisson_deviance(\n  y_true = test$Frequency,\n  y_pred = pred_fit_gam,\n  sample_weight = test$Exposure\n)\ncat(\"Mean Poisson Deviance of GAM:\", deviance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean Poisson Deviance of GAM: 0.4679338\n```\n\n\n:::\n:::\n\n\n### Surrogate Modeling\n\nWe apply the `interpret()` function to the GAM.\nThis step serves as a sanity check: if MID is effective, it should perfectly replicate the predictive behavior of the original GAM.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmid_gam <- interpret(\n  Frequency ~ VehPower + VehAge + DrivAge + LogDensity +\n              VehBrand + VehGas + Region,\n  data = train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_gam\n)\n\nsummary(mid_gam)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ninterpret(formula = Frequency ~ VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region, data = train, model = fit_gam,\n weights = Exposure, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n   working   response \n3.7577e-05 2.5365e-05 \n\nWorking Residuals:\n        Min          1Q      Median          3Q         Max \n-0.01324336 -0.00069460 -0.00004683  0.00057250  0.01822450 \n\nEncoding:\n           main.effect\nVehPower     linear(9)\nVehAge      linear(18)\nDrivAge     linear(25)\nLogDensity  linear(25)\nVehBrand     factor(6)\nVehGas       factor(2)\nRegion       factor(7)\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# evaluate fitted surrogate\npred_mid_gam = predict(mid_gam, test, type = \"response\")\n\ndeviance <- mean_poisson_deviance(\n  y_true = test$Frequency,\n  y_pred = pred_mid_gam,\n  sample_weight = test$Exposure\n)\ncat(\"Mean Poisson Deviance of Surrogate(GAM):\", deviance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean Poisson Deviance of Surrogate(GAM): 0.4679292\n```\n\n\n:::\n:::\n\n\n### Model Fidelity\n\nOne way to measure the surrogate model **fidelity**, i.e., how closely the surrogate model replicates the original model, is to calculate the $R^2$ measure between the original model's predictions $\\mathbf{y}$ and the surrogate's predictions $\\hat{\\mathbf{y}}$.\n\n$$\nR^2(\\mathbf{y},\\hat{\\mathbf{y}}) = 1 - \\frac{\\sum_{i=1}^n ({y_i}-\\hat{y}_i)^2} {\\sum_{i=1}^n (y_i-\\bar{y}_i)^2}\n$$\n\nThe **Uninterpreted Variation Ratio**\n\nAs shown by the $R^2$ score on the linear predictor (log) scale, the MID surrogate achieves near-perfect fidelity here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\n# calculate R-squared on testing dataset\nR2_mid <- weighted.loss(\n  x = log(pred_fit_gam),\n  y = log(pred_mid_gam),\n  w = test$Exposure,\n  method = \"r2\"\n)\n\ncat(sprintf(\"R-squared: %.6f\", R2_mid))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.999963\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# feature effects of GAM\npar.midr(mfrow = c(2, 4))\ntermplot(fit_gam)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/effects_fit_gam-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# main effects of MID surrogate\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_gam, engine = \"graphics\", ylab = \"Main Effect\")\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/effects_mid_gam-1.png){width=672}\n:::\n:::\n\n\nWe can also visualize prediction surface with the S3 method of the `persp()` function for MID models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar.midr(mar = c(1, 0, 1, 0), mfrow = c(1, 2))\npersp(mid_gam, \"LogDensity:DrivAge\", theta = 225, phi = 40, shade = .5)\npersp(mid_gam, \"LogDensity:Region\", theta = 225, phi = 40, shade = .5)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/persp_mid_gam-1.png){width=672}\n:::\n:::\n\n\n### Effect Importance\n\nBeyond simple plots for feature effects, `{midr}` provides a suite of diagnostic tools.\nFirst, we can quantify feature importance measured as follows:\n\n$$\n\\text{Importance}_j=\\mathbf{E}\\left[\\left| f_j(\\mathbf{X}) \\right|\\right]\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_gam <- mid.importance(mid_gam, data = train, max.nrow = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(5, 4),\n  ggmid(imp_gam, fill = \"steelblue\") +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\"),\n  ggmid(imp_gam, type = \"beeswarm\", theme = \"mako@div\") +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/imp_gam-1.png){width=672}\n:::\n:::\n\n\n### Conditional Expectation\n\nSecond, we can explore individual conditional expectations (ICE) of the fitted MID model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nice_gam_link <- mid.conditional(mid_gam, type = \"link\", variable = \"DrivAge\")\nice_gam <- mid.conditional(mid_gam, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_gam_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_gam, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/ice_gam-1.png){width=672}\n:::\n:::\n\n\n### Additive Attribution\n\nThird, we can perform instance-level attribution through additive breakdown plots, which are directly derived from the functional decomposition.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nrow_ids <- sort(sample(nrow(train), 4))\nbd_list <- lapply(\n  row_ids,\n  function(x) {\n    res <- mid.breakdown(mid_gam, train, row = x, format = \"%.6s\")\n    structure(res, row_id = x)\n  }\n)\nbd_plots <- lapply(\n  bd_list, function(x) {\n    label <- paste0(\"Breakdown of Row \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\") +\n      labs(x = NULL, subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\ngrid.arrange(grobs = bd_plots)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Black-Box GBM\n\nWhile GAMs are transparent, GBMs such as **LightGBM** often yields superior predictive power by capturing high-order interactions.\nHowever, this accuracy comes at the cost of being a black box. \nHere, we train a LightGBM model using optimized hyperparameters to achieve high predictive performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# hold out validation dataset\nvalid_idx <- seq_len(floor(nrow(train) * 0.2))\n\n# create datasets for training\ndtrain <- lgb.Dataset(\n  data.matrix(select(train[-valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[-valid_idx],\n  weight = train$Exposure[-valid_idx],\n  categorical_feature = c(\"VehBrand\", \"VehGas\", \"Region\")\n)\n\ndvalid <- lgb.Dataset.create.valid(\n  dtrain,\n  data.matrix(select(train[ valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[ valid_idx],\n  weight = train$Exposure[ valid_idx]\n)\n\n# model parameters\nparams_lgb <- list(\n  objective = \"poisson\",\n  learning_rate = 0.03188002,\n  num_leaves = 30,\n  reg_lambda = 0.004201069,\n  reg_alpha = 0.2523909,\n  colsample_bynode = 0.5552524,\n  subsample = 0.5938199,\n  min_child_samples = 9,\n  min_split_gain = 0.3920509,\n  poisson_max_delta_step = 0.8039541\n)\n\nset.seed(42)\nfit_lgb <- lgb.train(\n  params = params_lgb,\n  data = dtrain,\n  nrounds = 1000L,\n  valids = list(eval = dvalid),\n  early_stopping_round = 50L,\n  verbose = 0L\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit_lgb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLightGBM Model (402 trees)\nObjective: poisson\nFitted to dataset with 7 columns\n```\n\n\n:::\n:::\n\n\nThe weighted mean Poisson deviance of the fitted LightGBM is around 0.4655023, which is better than the GAM's performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# evaluate model\npred_fit_lgb <- predict(\n  fit_lgb, data.matrix(select(test, -Frequency, -Exposure))\n)\n\ncat(\"Mean Poisson Deviance:\",\n    mean_poisson_deviance(test$Frequency, pred_fit_lgb, test$Exposure))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean Poisson Deviance: 0.4655023\n```\n\n\n:::\n:::\n\n\n### Surrogate Modeling\n\nWe now use `{midr}` to replicate the LightGBM model.\nBy including interaction terms in the model formula, we allow the surrogate to capture two-way joint relationships of features that the GBM might have learned from the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmid_lgb <- interpret(\n  Frequency ~ (VehPower + VehAge + DrivAge + LogDensity +\n               VehBrand + VehGas + Region)^2,\n  data = train,\n  lambda = 0.01,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_lgb,\n  pred.fun = function(model, data) {\n    newdata <- data.matrix(select(data, -Frequency, -Exposure))\n    predict(model, newdata)\n  }\n)\n\nsummary(mid_lgb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ninterpret(formula = Frequency ~ (VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region)^2, data = train,\n model = fit_lgb, pred.fun = function(model, data) {\n newdata <- data.matrix(select(data, -Frequency, -Exposure))\n predict(model, newdata)\n }, weights = Exposure, lambda = 0.01, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n working response \n0.071642 0.181512 \n\nWorking Residuals:\n     Min       1Q   Median       3Q      Max \n-0.51124 -0.04431 -0.00326  0.03899  3.63211 \n\nEncoding:\n           main.effect interaction\nVehPower     linear(9)   linear(5)\nVehAge      linear(18)   linear(5)\nDrivAge     linear(25)   linear(5)\nLogDensity  linear(25)   linear(5)\nVehBrand     factor(6)   factor(6)\nVehGas       factor(2)   factor(2)\nRegion       factor(7)   factor(7)\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_mid_lgb = predict(mid_lgb, test, type = \"response\")\n\ncat(\"Mean Poisson Deviance:\",\n    mean_poisson_deviance(test$Frequency, pred_mid_lgb, test$Exposure))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean Poisson Deviance: 0.4670904\n```\n\n\n:::\n:::\n\n\n### Model Fidelity\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_midr(\"xy\"))\n\n# calculate R-squared on testing dataset\nR2_mid <- weighted.loss(\n  x = log(pred_fit_lgb),\n  y = log(pred_mid_lgb),\n  w = test$Exposure,\n  method = \"r2\"\n)\n\ncat(sprintf(\"R-squared: %.4f\", R2_mid))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared: 0.9295\n```\n\n\n:::\n:::\n\n\n### Main Effect\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_lgb, engine = \"graphics\", ylab = \"Partial Effect\")\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n### Interaction Effect\n\nOne of the important features of `{midr}` is its ability to isolate interaction effects.\nIn this section, we visualize how geographical `Region` interacts with `LogDensity`.\nThese insights are often hidden in GBMs but are critical in actuarial practice.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid.arrange(\n  nrow = 1, widths = c(3, 2),\n  ggmid(mid_lgb, \"LogDensity:Region\", type = \"data\",\n        data = train[1:1e4, ]) +\n    labs(y = NULL, subtitle = \"Interaction Effect\") +\n    theme(legend.position = \"bottom\"),\n  ggmid(mid_lgb, \"LogDensity:Region\", main.effects = TRUE) +\n    labs(y = NULL, subtitle = \"Total Effect\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"bottom\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar.midr(mar = c(1, 0, 1, 0), mfrow = c(1, 2))\npersp(mid_lgb, \"LogDensity:DrivAge\", theta = 225, phi = 40, shade = .5)\npersp(mid_lgb, \"LogDensity:Region\", theta = 225, phi = 40, shade = .5)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### Effect Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_lgb <- mid.importance(mid_lgb, data = train, max.nrow = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(4, 3),\n  ggmid(imp_lgb, theme = \"bluescale@qual\", max.nterms = 20) +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\") +\n    theme(legend.position = \"none\"),\n  ggmid(imp_lgb, type = \"beeswarm\", theme = \"mako@div\", max.nterms = 20) +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Conditional Expectation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nice_lgb_link <- mid.conditional(mid_lgb, type = \"link\", variable = \"DrivAge\")\nice_lgb <- mid.conditional(mid_lgb, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_lgb_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_lgb, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n### Additive Attribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nrow_ids <- sort(sample(nrow(train), 4))\n\nbd_list <- lapply(\n  row_ids,\n  function(x) {\n    res <- mid.breakdown(mid_lgb, train, row = x,\n                         format = c(\"%.6s\", \"%.3s, %.3s\"))\n    structure(res, row_id = x)\n  }\n)\n\nbd_plots <- lapply(\n  bd_list, function(x) {\n    label <- paste0(\"Breakdown of Row \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\", max.nterms = 10) +\n      labs(x = \"Linear Predictor\", subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\n\ngrid.arrange(grobs = bd_plots)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Conclusion\n\nBy using `{midr}`, we have transformed a complex LightGBM model into a set of interpretable charts and attributions.\n",
    "supporting": [
      "demo_r_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}