{
  "hash": "e95216fcb3be7a90209101b9d46cad2e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Surrogate Modeling with MID in R\"\n---\n\n## Introduction\n\nIn actuarial practice, balancing predictive performance with model interpretability is a constant challenge.\nWhile **Generalized Additive Models (GAMs)** offer inherent transparency, complex black-box models like **Gradient Boosting Models (GBMs)** often provide superior accuracy at the cost of clarity.\n\nThis notebook demonstrates **Maximum Interpretation Decomposition (MID)** using the `{midr}` package in R.\nMID serves as a surrogate modeling framework that decomposes complex predictions into interpretable components—main effects and interactions—allowing us to open the black box without sacrificing the predictive power of it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data manipuration\nlibrary(arrow)\nlibrary(dplyr)\n\n# predictive modeling\nlibrary(gam)\nlibrary(lightgbm)\n\n# surrogate modeling\nlibrary(midr)\nlibrary(midnight)\n\n# visualization\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# load training and testin datasets\ntrain <- read_parquet(\"../data/train.parquet\")\ntest  <- read_parquet(\"../data/test.parquet\")\n\n# cold execution\nCOLD_RUN <- TRUE\nif (COLD_RUN) {\n  set.seed(42)\n  train <- train[sample(nrow(train), 50000), ]\n  test  <- test[sample(nrow(test), 50000), ]\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define loss function\nmean_poisson_deviance <- function(\n    y_true, y_pred, sample_weight = rep(1, length(y))\n  ) {\n  stopifnot(all(y_pred > 0))\n  resid <- ifelse(y_true > 0, y_true * log(y_true / y_pred), 0)\n  resid <- resid - y_true + y_pred\n  2 * sum(resid * sample_weight) / sum(sample_weight)\n}\n```\n:::\n\n\n## Interpretable GAM\n\nWe begin by fitting a standard GAM to the motor insurance frequency dataset.\nSince a GAM is already composed of additive smooth functions, it serves as a perfect sanity check for our surrogate modeling approach.\n\n### Predictive Modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_gam <- gam(\n  Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + s(LogDensity) +\n              VehBrand + VehGas + Region,\n  data = train,\n  weights = Exposure,\n  family = quasipoisson(link = \"log\")\n)\nsummary(fit_gam)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall: gam(formula = Frequency ~ s(VehPower) + s(VehAge) + s(DrivAge) + \n    s(LogDensity) + VehBrand + VehGas + Region, family = quasipoisson(link = \"log\"), \n    data = train, weights = Exposure)\nDeviance Residuals:\n    Min      1Q  Median      3Q     Max \n-0.7346 -0.3492 -0.2640 -0.1436 10.7132 \n\n(Dispersion Parameter for quasipoisson family taken to be 1.7132)\n\n    Null Deviance: 13216.45 on 49999 degrees of freedom\nResidual Deviance: 12963.4 on 49971 degrees of freedom\nAIC: NA \n\nNumber of Local Scoring Iterations: NA \n\nAnova for Parametric Effects\n                 Df Sum Sq Mean Sq F value    Pr(>F)    \ns(VehPower)       1      9   9.385  5.4783   0.01926 *  \ns(VehAge)         1      2   2.002  1.1683   0.27975    \ns(DrivAge)        1     34  33.966 19.8260 8.500e-06 ***\ns(LogDensity)     1    110 109.592 63.9691 1.291e-15 ***\nVehBrand          5     11   2.142  1.2505   0.28245    \nVehGas            1      1   1.274  0.7438   0.38844    \nRegion            6     12   2.037  1.1889   0.30869    \nResiduals     49971  85610   1.713                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova for Nonparametric Effects\n              Npar Df  Npar F     Pr(F)    \n(Intercept)                                \ns(VehPower)         3  0.5258    0.6645    \ns(VehAge)           3  0.7300    0.5339    \ns(DrivAge)          3 15.5538 4.146e-10 ***\ns(LogDensity)       3  0.3274    0.8055    \nVehBrand                                   \nVehGas                                     \nRegion                                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# evaluate model\npred_fit_gam <- predict(fit_gam, test, type = \"response\")\ncat(\n  \"Mean Poisson Deviance:\",\n  mean_poisson_deviance(test$Frequency, pred_fit_gam, test$Exposure)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean Poisson Deviance: 0.4895171\n```\n\n\n:::\n:::\n\n\n### Surrogate Modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmid_gam <- interpret(\n  Frequency ~ VehPower + VehAge + DrivAge + LogDensity +\n              VehBrand + VehGas + Region,\n  train,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_gam\n)\nsummary(mid_gam)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ninterpret(formula = Frequency ~ VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region, data = train, model = fit_gam,\n weights = Exposure, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n   working   response \n0.00017046 0.00011598 \n\nWorking Residuals:\n       Min         1Q     Median         3Q        Max \n-0.0338466 -0.0008416 -0.0000489  0.0007905  0.0327146 \n\nEncoding:\n           main.effect\nVehPower     linear(9)\nVehAge      linear(18)\nDrivAge     linear(25)\nLogDensity  linear(25)\nVehBrand     factor(6)\nVehGas       factor(2)\nRegion       factor(7)\n```\n\n\n:::\n:::\n\n\n### Model Fidelity\n\nAfter wrapping the GAM with the `interpret()` function, we evaluate the Model Fidelity.\nIn the MID framework, the (Working) Uninterpreted Variation Ratio ($1 - R^s$) quantifies how much of the original model's logic remains \"unexplained.\"\nAs shown in the scatter plots, the MID surrogate almost perfectly replicates the GAM's predictions on the test set, both on the linear predictor and the original response scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_midr(\"xy\"))\npred_mid_gam = predict(mid_gam, test, type = \"response\")\n\nset.seed(42)\nplot_idx <- sample(nrow(test), 2000L)\n\np <- ggplot(\n  data.frame(fit = pred_fit_gam[plot_idx],\n             mid = pred_mid_gam[plot_idx])\n)\ngrid.arrange(\n  nrow = 1,\n  p +\n    geom_point(aes(log(fit), log(mid))) +\n    labs(x = \"GAM\", y = \"MID Surrogate\",\n         title = \"Model Fidelity\", subtitle = \"Linear predictor\"),\n  p +\n    geom_point(aes(fit, mid)) +\n    labs(x = \"GAM\", y = \"MID Surrogate\",\n         title = \"\", subtitle = \"Prediction in the original scale\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### Main Effect\n\nA critical step in validating a surrogate model is ensuring that the extracted functional forms (main effects) align with the underlying model's logic.\nIn the case of a GAM, where effects are explicitly modeled as splines, the MID surrogate should ideally recover these shapes with high fidelity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar.midr(mfrow = c(2, 4))\ntermplot(fit_gam)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_gam, engine = \"graphics\", ylab = \"Partial Effect\")\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar.midr(mar = c(1,1,1,1))\npersp(mid_gam, \"LogDensity:DrivAge\", theta = 225, phi = 40, shade = .5)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### Effect Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_gam <- mid.importance(mid_gam, data = train, max.nrow = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(5, 4),\n  ggmid(imp_gam, fill = \"steelblue\") +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\"),\n  ggmid(imp_gam, type = \"beeswarm\", theme = \"mako@div\") +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### Conditional Expectation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nice_gam_link <- mid.conditional(mid_gam, type = \"link\", variable = \"DrivAge\")\nice_gam <- mid.conditional(mid_gam, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_gam_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_gam, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### Additive Attribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nrow_ids <- sort(sample(nrow(train), 4))\nbd_list <- lapply(\n  row_ids,\n  function(x) {\n    res <- mid.breakdown(mid_gam, train, row = x, format = \"%.6s\")\n    structure(res, row_id = x)\n  }\n)\nbd_plots <- lapply(\n  bd_list, function(x) {\n    label <- paste0(\"Breakdown of Row \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\") +\n      labs(x = NULL, subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\ngrid.arrange(grobs = bd_plots)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Black-Box GBM\n\nNext, we train a LightGBM model.\nWhile more powerful, its tree-based structure makes it difficult to explain the relationship between inputs and outputs directly.\n\nWe apply MID to the LightGBM model, specifically allowing for second-order interactions (`^ 2`) in the formula.\nThis allows the surrogate to capture joint effects that a simple additive model would miss.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvalid_idx <- seq_len(floor(nrow(train) * 0.2))\ndtrain <- lgb.Dataset(\n  data.matrix(select(train[-valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[-valid_idx],\n  weight = train$Exposure[-valid_idx],\n  categorical_feature = c(\"VehBrand\", \"VehGas\", \"Region\")\n)\ndvalid <- lgb.Dataset.create.valid(\n  dtrain,\n  data.matrix(select(train[ valid_idx, ], -Frequency, -Exposure)),\n  label = train$Frequency[ valid_idx],\n  weight = train$Exposure[ valid_idx]\n)\nparams_lgb <- list(\n  objective = \"poisson\",\n  learning_rate = 0.005,\n  num_leaves = 10,\n  reg_lambda = 1,\n  reg_alpha = 1,\n  colsample_bynode = 0.8,\n  subsample = 0.8,\n  min_child_samples = 20,\n  min_split_gain = 0.1,\n  poisson_max_delta_step = 0.1\n)\nfit_lgb <- lgb.train(\n  params = params_lgb,\n  data = dtrain,\n  nrounds = 1000L,\n  valids = list(eval = dvalid),\n  early_stopping_round = 20L,\n  verbose = 0L\n)\nsummary(fit_lgb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLightGBM Model (432 trees)\nObjective: poisson\nFitted to dataset with 7 columns\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# evaluate model\npred_fit_lgb <- predict(\n  fit_lgb, data.matrix(select(test, -Frequency, -Exposure))\n)\ncat(\n  \"Mean Poisson Deviance:\",\n  mean_poisson_deviance(test$Frequency, pred_fit_lgb, test$Exposure)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean Poisson Deviance: 0.4891941\n```\n\n\n:::\n:::\n\n\n### Surrogate Modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmid_lgb <- interpret(\n  Frequency ~ (VehPower + VehAge + DrivAge + LogDensity +\n               VehBrand + VehGas + Region) ^ 2,\n  train,\n  lambda = 0.5,\n  weights = Exposure,\n  link = \"log\",\n  model = fit_lgb,\n  pred.fun = function(model, data) {\n    newdata <- data.matrix(select(data, -Frequency, -Exposure))\n    predict(model, newdata)\n  }\n)\nsummary(mid_lgb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ninterpret(formula = Frequency ~ (VehPower + VehAge + DrivAge +\n LogDensity + VehBrand + VehGas + Region)^2, data = train,\n model = fit_lgb, pred.fun = function(model, data) {\n newdata <- data.matrix(select(data, -Frequency, -Exposure))\n predict(model, newdata)\n }, weights = Exposure, lambda = 0.5, link = \"log\")\n\nLink: log\n\nUninterpreted Variation Ratio:\n working response \n0.091418 0.128929 \n\nWorking Residuals:\n      Min        1Q    Median        3Q       Max \n-0.442868 -0.039610 -0.001806  0.036518  0.778933 \n\nEncoding:\n           main.effect interaction\nVehPower     linear(9)   linear(5)\nVehAge      linear(18)   linear(5)\nDrivAge     linear(25)   linear(5)\nLogDensity  linear(25)   linear(5)\nVehBrand     factor(6)   factor(6)\nVehGas       factor(2)   factor(2)\nRegion       factor(7)   factor(7)\n```\n\n\n:::\n:::\n\n\n### Model Fidelity\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_midr(\"xy\"))\npred_mid_lgb = predict(mid_lgb, test, type = \"response\")\n\n# set.seed(42)\n# plot_idx <- sample(nrow(test), 2000L)\n\np <- ggplot(\n  data.frame(fit = pred_fit_lgb[plot_idx],\n             mid = pred_mid_lgb[plot_idx])\n)\ngrid.arrange(\n  nrow = 1,\n  p +\n    geom_point(aes(log(fit), log(mid))) +\n    labs(x = \"LightGBM\", y = \"MID Surrogate\",\n         title = \"Model Fidelity\", subtitle = \"Linear predictor\"),\n  p +\n    geom_point(aes(fit, mid)) +\n    labs(x = \"LightGBM\", y = \"MID Surrogate\",\n         title = \"\", subtitle = \"Prediction in the original scale\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n### Main Effect\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar.midr(mfrow = c(2, 4))\nmid.plots(mid_lgb, engine = \"graphics\", ylab = \"Partial Effect\")\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n### Interaction Effect\n\nA key strength of `{midr}` is its ability to isolate and visualize interaction effects:\n\n- Interaction Plots: We can visualize specific interactions, such as `LogDensity:Region`, using `ggmid()`.\nThis helps identify whether certain geographical regions react differently to population density.\n\n- Perspective Plots: The `persp()` function provides a 3D view of these interactions, offering a more intuitive understanding of non-linear surfaces. This S3 method is implemented in `{midnight}`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid.arrange(\n  nrow = 1, widths = c(3, 2),\n  ggmid(mid_lgb, \"LogDensity:Region\", type = \"data\",\n        data = train[1:1e4, ]) +\n    labs(y = NULL, subtitle = \"Interaction Effect\") +\n    theme(legend.position = \"bottom\"),\n  ggmid(mid_lgb, \"LogDensity:Region\", main.effects = TRUE) +\n    labs(y = NULL, subtitle = \"Total Effect\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"bottom\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar.midr(mar = c(1,1,1,1))\npersp(mid_lgb, \"LogDensity:Region\", theta = 225, phi = 40, shade = .5)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n### Effect Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimp_lgb <- mid.importance(mid_lgb, data = train, max.nrow = 2000)\ngrid.arrange(\n  nrow = 1, widths = c(4, 3),\n  ggmid(imp_lgb, theme = \"bluescale@qual\") +\n    labs(title = \"Effect Importance\",\n         subtitle = \"Average absolute effect per feature\") +\n    theme(legend.position = \"none\"),\n  ggmid(imp_lgb, type = \"beeswarm\", theme = \"mako@div\") +\n    labs(title = \"\",\n         subtitle = \"Distribution of effect per feature\") +\n    scale_y_discrete(labels = NULL) +\n    theme(legend.position = \"none\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n### Conditional Expectation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nice_lgb_link <- mid.conditional(mid_lgb, type = \"link\", variable = \"DrivAge\")\nice_lgb <- mid.conditional(mid_lgb, variable = \"DrivAge\")\ngrid.arrange(\n  nrow = 1,\n  ggmid(ice_lgb_link, var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Linear Predictor\",\n         title = \"Conditional Expectation\",\n         subtitle = \"Change in linear predictor\"),\n  ggmid(ice_lgb, type = \"centered\", var.color = LogDensity) +\n    theme(legend.position = \"bottom\") +\n    labs(y = \"Prediction\", title = \"\",\n         subtitle = \"Centered change in original scale\")\n)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n### Additive Attribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nrow_ids <- sort(sample(nrow(train), 4))\nbd_list <- lapply(\n  row_ids,\n  function(x) {\n    res <- mid.breakdown(mid_lgb, train, row = x,\n                         format = c(\"%.6s\", \"%.3s, %.3s\"))\n    structure(res, row_id = x)\n  }\n)\nbd_plots <- lapply(\n  bd_list, function(x) {\n    label <- paste0(\"Breakdown of Row \", attr(x, \"row_id\"))\n    ggmid(x, theme = \"shap\", max.nterms = 10) +\n      labs(x = \"Linear Predictor\", subtitle = label) +\n      theme(legend.position = \"none\")\n  }\n)\ngrid.arrange(grobs = bd_plots)\n```\n\n::: {.cell-output-display}\n![](demo_r_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n## Conclusion\n\nBy using `{midr}`, we have transformed a complex LightGBM model into a set of interpretable charts and attributions.\n",
    "supporting": [
      "demo_r_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}